"""
ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ - Firestore â†” BigQuery åŒæœŸ

@description Firestoreãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã¨ BigQueryåˆ†æãƒ‡ãƒ¼ã‚¿ã®çµ±åˆç®¡ç†
ETLå‡¦ç†ã€ãƒ‡ãƒ¼ã‚¿åŒæœŸã€åˆ†æãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’æä¾›

@author InfuMatch Development Team
@version 1.0.0
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone, timedelta
import json

from core.database import get_firestore_client, DatabaseCollections, DatabaseHelper
from core.bigquery_client import get_bigquery_client, BigQueryTables, get_bigquery_analytics
from core.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class DataIntegrationService:
    """
    Firestore ã¨ BigQuery ã®çµ±åˆç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹
    
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿(Firestore)ã¨åˆ†æãƒ‡ãƒ¼ã‚¿(BigQuery)ã®åŒæœŸã‚’ç®¡ç†
    """
    
    def __init__(self):
        self.firestore_client = get_firestore_client()
        self.bigquery_client = get_bigquery_client()
        self.db_helper = DatabaseHelper()
        self.analytics = get_bigquery_analytics()
    
    async def sync_influencers_to_bigquery(self, batch_size: int = 100) -> Dict[str, Any]:
        """
        ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’Firestoreã‹ã‚‰BigQueryã«åŒæœŸ
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            
        Returns:
            Dict: åŒæœŸçµæœ
        """
        logger.info("ğŸ”„ Starting influencer data sync to BigQuery")
        
        try:
            # Firestoreã‹ã‚‰æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            influencers = await self.db_helper.get_all_documents(
                collection=DatabaseCollections.INFLUENCERS,
                limit=1000  # ä¸€åº¦ã«æœ€å¤§1000ä»¶
            )
            
            if not influencers:
                logger.info("ğŸ“­ No influencers found in Firestore")
                return {'synced_count': 0, 'error': None}
            
            logger.info(f"ğŸ“Š Found {len(influencers)} influencers to sync")
            
            # BigQueryç”¨ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›
            bigquery_rows = []
            for influencer in influencers:
                row = self._convert_influencer_to_bigquery_format(influencer)
                if row:
                    bigquery_rows.append(row)
            
            # ãƒãƒƒãƒã§BigQueryã«æŒ¿å…¥
            synced_count = 0
            failed_count = 0
            
            for i in range(0, len(bigquery_rows), batch_size):
                batch = bigquery_rows[i:i + batch_size]
                
                success = self.bigquery_client.insert_rows(
                    table_name=BigQueryTables.INFLUENCERS,
                    rows=batch
                )
                
                if success:
                    synced_count += len(batch)
                    logger.info(f"âœ… Synced batch {i//batch_size + 1}: {len(batch)} records")
                else:
                    failed_count += len(batch)
                    logger.error(f"âŒ Failed to sync batch {i//batch_size + 1}")
                
                # å°‘ã—å¾…æ©Ÿã—ã¦APIåˆ¶é™ã‚’é¿ã‘ã‚‹
                await asyncio.sleep(0.5)
            
            return {
                'synced_count': synced_count,
                'failed_count': failed_count,
                'total_processed': len(bigquery_rows),
                'error': None
            }
            
        except Exception as e:
            logger.error(f"âŒ Influencer sync failed: {str(e)}")
            return {
                'synced_count': 0,
                'failed_count': 0,
                'total_processed': 0,
                'error': str(e)
            }
    
    def _convert_influencer_to_bigquery_format(self, influencer: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        try:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if not influencer.get('channel_id'):
                return None
            
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«å¤‰æ›
            created_at = self._parse_timestamp(influencer.get('created_at'))
            updated_at = self._parse_timestamp(influencer.get('updated_at', influencer.get('created_at')))
            
            return {
                'influencer_id': influencer.get('channel_id'),
                'channel_id': influencer.get('channel_id'),
                'channel_title': influencer.get('channel_title', ''),
                'description': influencer.get('description', ''),
                'subscriber_count': int(influencer.get('subscriber_count', 0)),
                'video_count': int(influencer.get('video_count', 0)),
                'view_count': int(influencer.get('view_count', 0)),
                'category': influencer.get('category', ''),
                'country': influencer.get('country', ''),
                'language': influencer.get('language', ''),
                'contact_email': influencer.get('contact_info', {}).get('email', ''),
                'social_links': json.dumps(influencer.get('social_links', {})),
                'ai_analysis': json.dumps(influencer.get('ai_analysis', {})),
                'created_at': created_at,
                'updated_at': updated_at,
                'is_active': influencer.get('status', 'active') == 'active',
            }
        except Exception as e:
            logger.error(f"âŒ Failed to convert influencer {influencer.get('channel_id')}: {str(e)}")
            return None
    
    async def sync_campaigns_to_bigquery(self, batch_size: int = 100) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’Firestoreã‹ã‚‰BigQueryã«åŒæœŸ
        """
        logger.info("ğŸ”„ Starting campaign data sync to BigQuery")
        
        try:
            # Firestoreã‹ã‚‰ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            campaigns = await self.db_helper.get_all_documents(
                collection=DatabaseCollections.CAMPAIGNS,
                limit=1000
            )
            
            if not campaigns:
                logger.info("ğŸ“­ No campaigns found in Firestore")
                return {'synced_count': 0, 'error': None}
            
            logger.info(f"ğŸ“Š Found {len(campaigns)} campaigns to sync")
            
            # BigQueryç”¨ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›
            bigquery_rows = []
            for campaign in campaigns:
                row = self._convert_campaign_to_bigquery_format(campaign)
                if row:
                    bigquery_rows.append(row)
            
            # ãƒãƒƒãƒã§BigQueryã«æŒ¿å…¥
            synced_count = 0
            failed_count = 0
            
            for i in range(0, len(bigquery_rows), batch_size):
                batch = bigquery_rows[i:i + batch_size]
                
                success = self.bigquery_client.insert_rows(
                    table_name=BigQueryTables.CAMPAIGNS,
                    rows=batch
                )
                
                if success:
                    synced_count += len(batch)
                else:
                    failed_count += len(batch)
            
            return {
                'synced_count': synced_count,
                'failed_count': failed_count,
                'total_processed': len(bigquery_rows),
                'error': None
            }
            
        except Exception as e:
            logger.error(f"âŒ Campaign sync failed: {str(e)}")
            return {
                'synced_count': 0,
                'failed_count': 0,
                'error': str(e)
            }
    
    def _convert_campaign_to_bigquery_format(self, campaign: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        try:
            if not campaign.get('campaign_id'):
                return None
            
            created_at = self._parse_timestamp(campaign.get('created_at'))
            updated_at = self._parse_timestamp(campaign.get('updated_at', campaign.get('created_at')))
            
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’DATEã«å¤‰æ›
            start_date = self._parse_date(campaign.get('start_date'))
            end_date = self._parse_date(campaign.get('end_date'))
            
            return {
                'campaign_id': campaign.get('campaign_id'),
                'company_id': campaign.get('company_id', ''),
                'title': campaign.get('title', ''),
                'description': campaign.get('description', ''),
                'budget': float(campaign.get('budget', 0)),
                'target_category': campaign.get('target_category', ''),
                'target_demographics': json.dumps(campaign.get('target_demographics', {})),
                'requirements': json.dumps(campaign.get('requirements', {})),
                'status': campaign.get('status', 'draft'),
                'start_date': start_date,
                'end_date': end_date,
                'created_at': created_at,
                'updated_at': updated_at,
            }
        except Exception as e:
            logger.error(f"âŒ Failed to convert campaign {campaign.get('campaign_id')}: {str(e)}")
            return None
    
    async def generate_daily_metrics(self, target_date: datetime = None) -> Dict[str, Any]:
        """
        æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç”Ÿæˆã¨BigQueryã¸ã®ä¿å­˜
        
        Args:
            target_date: å¯¾è±¡æ—¥ä»˜ï¼ˆNone ã®å ´åˆã¯å‰æ—¥ï¼‰
            
        Returns:
            Dict: ç”Ÿæˆçµæœ
        """
        if target_date is None:
            target_date = datetime.now(timezone.utc) - timedelta(days=1)
        
        logger.info(f"ğŸ“Š Generating daily metrics for {target_date.date()}")
        
        try:
            # å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¸¦è¡Œã—ã¦è¨ˆç®—
            tasks = [
                self._calculate_influencer_metrics(target_date),
                self._calculate_campaign_metrics(target_date),
                self._calculate_negotiation_metrics(target_date),
                self._calculate_engagement_metrics(target_date),
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœã‚’çµ±åˆ
            daily_metrics = []
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"âŒ Metric calculation {i} failed: {str(result)}")
                    continue
                
                if result:
                    daily_metrics.extend(result)
            
            # BigQueryã«ä¿å­˜
            if daily_metrics:
                success = self.bigquery_client.insert_rows(
                    table_name=BigQueryTables.DAILY_METRICS,
                    rows=daily_metrics
                )
                
                if success:
                    logger.info(f"âœ… Saved {len(daily_metrics)} daily metrics to BigQuery")
                    return {
                        'metrics_generated': len(daily_metrics),
                        'target_date': target_date.date().isoformat(),
                        'success': True
                    }
                else:
                    return {
                        'metrics_generated': 0,
                        'error': 'Failed to save to BigQuery',
                        'success': False
                    }
            else:
                return {
                    'metrics_generated': 0,
                    'error': 'No metrics calculated',
                    'success': False
                }
                
        except Exception as e:
            logger.error(f"âŒ Daily metrics generation failed: {str(e)}")
            return {
                'metrics_generated': 0,
                'error': str(e),
                'success': False
            }
    
    async def _calculate_influencer_metrics(self, target_date: datetime) -> List[Dict[str, Any]]:
        """ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        try:
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ•°ã‚’å–å¾—
            active_influencers = await self.db_helper.query_documents(
                collection=DatabaseCollections.INFLUENCERS,
                conditions=[('status', '==', 'active')]
            )
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†å¸ƒã‚’è¨ˆç®—
            category_counts = {}
            total_engagement = 0
            engagement_count = 0
            
            for influencer in active_influencers:
                category = influencer.get('category', 'unknown')
                category_counts[category] = category_counts.get(category, 0) + 1
                
                # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã®è¨ˆç®—
                ai_analysis = influencer.get('ai_analysis', {})
                if isinstance(ai_analysis, dict) and 'engagement_rate' in ai_analysis:
                    total_engagement += float(ai_analysis['engagement_rate'])
                    engagement_count += 1
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ§‹ç¯‰
            metrics = []
            
            # å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            avg_engagement = total_engagement / engagement_count if engagement_count > 0 else 0
            
            metrics.append({
                'date': target_date.date().isoformat(),
                'metric_type': 'influencer_overview',
                'category': 'all',
                'total_influencers': len(active_influencers),
                'active_campaigns': 0,  # å¾Œã§ä»–ã®ã‚¿ã‚¹ã‚¯ã§è¨­å®š
                'completed_negotiations': 0,  # å¾Œã§ä»–ã®ã‚¿ã‚¹ã‚¯ã§è¨­å®š
                'avg_engagement_rate': round(avg_engagement, 4),
                'total_revenue': 0,  # å¾Œã§ä»–ã®ã‚¿ã‚¹ã‚¯ã§è¨­å®š
                'growth_metrics': json.dumps({
                    'new_influencers_today': 0,  # å®Ÿè£…è¦
                    'category_distribution': category_counts
                }),
                'created_at': datetime.now(timezone.utc).isoformat(),
            })
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            for category, count in category_counts.items():
                metrics.append({
                    'date': target_date.date().isoformat(),
                    'metric_type': 'category_breakdown',
                    'category': category,
                    'total_influencers': count,
                    'active_campaigns': 0,
                    'completed_negotiations': 0,
                    'avg_engagement_rate': 0,
                    'total_revenue': 0,
                    'growth_metrics': json.dumps({}),
                    'created_at': datetime.now(timezone.utc).isoformat(),
                })
            
            return metrics
            
        except Exception as e:
            logger.error(f"âŒ Influencer metrics calculation failed: {str(e)}")
            return []
    
    async def _calculate_campaign_metrics(self, target_date: datetime) -> List[Dict[str, Any]]:
        """ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        try:
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æ•°ã‚’å–å¾—
            active_campaigns = await self.db_helper.query_documents(
                collection=DatabaseCollections.CAMPAIGNS,
                conditions=[('status', '==', 'active')]
            )
            
            # ä»Šæ—¥ä½œæˆã•ã‚ŒãŸã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³
            today_campaigns = await self.db_helper.query_documents(
                collection=DatabaseCollections.CAMPAIGNS,
                conditions=[
                    ('created_at', '>=', target_date.isoformat()),
                    ('created_at', '<', (target_date + timedelta(days=1)).isoformat())
                ]
            )
            
            return [{
                'date': target_date.date().isoformat(),
                'metric_type': 'campaign_overview',
                'category': 'all',
                'total_influencers': 0,
                'active_campaigns': len(active_campaigns),
                'completed_negotiations': 0,
                'avg_engagement_rate': 0,
                'total_revenue': 0,
                'growth_metrics': json.dumps({
                    'new_campaigns_today': len(today_campaigns),
                    'campaigns_by_status': {}  # å®Ÿè£…è¦
                }),
                'created_at': datetime.now(timezone.utc).isoformat(),
            }]
            
        except Exception as e:
            logger.error(f"âŒ Campaign metrics calculation failed: {str(e)}")
            return []
    
    async def _calculate_negotiation_metrics(self, target_date: datetime) -> List[Dict[str, Any]]:
        """äº¤æ¸‰é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        try:
            # ä»Šæ—¥å®Œäº†ã—ãŸäº¤æ¸‰
            completed_negotiations = await self.db_helper.query_documents(
                collection=DatabaseCollections.NEGOTIATIONS,
                conditions=[
                    ('status', '==', 'completed'),
                    ('completed_at', '>=', target_date.isoformat()),
                    ('completed_at', '<', (target_date + timedelta(days=1)).isoformat())
                ]
            )
            
            # å£²ä¸Šè¨ˆç®—
            total_revenue = sum(
                float(neg.get('final_amount', 0)) 
                for neg in completed_negotiations
            )
            
            return [{
                'date': target_date.date().isoformat(),
                'metric_type': 'negotiation_overview',
                'category': 'all',
                'total_influencers': 0,
                'active_campaigns': 0,
                'completed_negotiations': len(completed_negotiations),
                'avg_engagement_rate': 0,
                'total_revenue': total_revenue,
                'growth_metrics': json.dumps({
                    'avg_deal_size': total_revenue / len(completed_negotiations) if completed_negotiations else 0,
                    'negotiation_success_rate': 0  # å®Ÿè£…è¦
                }),
                'created_at': datetime.now(timezone.utc).isoformat(),
            }]
            
        except Exception as e:
            logger.error(f"âŒ Negotiation metrics calculation failed: {str(e)}")
            return []
    
    async def _calculate_engagement_metrics(self, target_date: datetime) -> List[Dict[str, Any]]:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        # ç¾åœ¨ã®Firestoreãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’è¨ˆç®—
        # å®Ÿè£…ã¯å¾Œã§è¿½åŠ 
        return []
    
    def _parse_timestamp(self, timestamp_str: str) -> str:
        """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ–‡å­—åˆ—ã‚’BigQueryå½¢å¼ã«å¤‰æ›"""
        if not timestamp_str:
            return datetime.now(timezone.utc).isoformat()
        
        try:
            # æ—¢ã«ISOå½¢å¼ã®å ´åˆ
            if 'T' in timestamp_str:
                return timestamp_str
            
            # ãã®ä»–ã®å½¢å¼ã‚’è©¦è¡Œ
            dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            return dt.isoformat()
        except:
            return datetime.now(timezone.utc).isoformat()
    
    def _parse_date(self, date_str: str) -> Optional[str]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’BigQuery DATEå½¢å¼ã«å¤‰æ›"""
        if not date_str:
            return None
        
        try:
            # ISOæ—¥ä»˜å½¢å¼
            if 'T' in date_str:
                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return dt.date().isoformat()
            else:
                # ã™ã§ã«æ—¥ä»˜å½¢å¼
                return datetime.fromisoformat(date_str).date().isoformat()
        except:
            return None
    
    async def full_sync(self) -> Dict[str, Any]:
        """
        å®Œå…¨åŒæœŸ - å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Firestoreã‹ã‚‰BigQueryã«åŒæœŸ
        """
        logger.info("ğŸ”„ Starting full data sync")
        
        start_time = datetime.now(timezone.utc)
        
        # ä¸¦è¡Œã—ã¦ã™ã¹ã¦ã®åŒæœŸã‚’å®Ÿè¡Œ
        tasks = [
            self.sync_influencers_to_bigquery(),
            self.sync_campaigns_to_bigquery(),
            self.generate_daily_metrics(),
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = datetime.now(timezone.utc)
        duration = (end_time - start_time).total_seconds()
        
        # çµæœã‚’é›†è¨ˆ
        total_synced = 0
        total_failed = 0
        errors = []
        
        sync_types = ['influencers', 'campaigns', 'daily_metrics']
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                errors.append(f"{sync_types[i]}: {str(result)}")
            elif isinstance(result, dict):
                total_synced += result.get('synced_count', result.get('metrics_generated', 0))
                total_failed += result.get('failed_count', 0)
                if result.get('error'):
                    errors.append(f"{sync_types[i]}: {result['error']}")
        
        return {
            'total_synced': total_synced,
            'total_failed': total_failed,
            'duration_seconds': duration,
            'errors': errors,
            'success': len(errors) == 0,
            'completed_at': end_time.isoformat()
        }


# ä¾¿åˆ©ãªé–¢æ•°
def get_data_integration_service() -> DataIntegrationService:
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return DataIntegrationService()


# å®šæœŸå®Ÿè¡Œç”¨ã®é–¢æ•°
async def run_daily_sync():
    """æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿åŒæœŸã®å®Ÿè¡Œ"""
    service = get_data_integration_service()
    result = await service.full_sync()
    
    logger.info(f"ğŸ“Š Daily sync completed:")
    logger.info(f"  - Synced: {result['total_synced']} records")
    logger.info(f"  - Failed: {result['total_failed']} records")
    logger.info(f"  - Duration: {result['duration_seconds']:.2f} seconds")
    
    if result['errors']:
        logger.error(f"  - Errors: {result['errors']}")
    
    return result


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_daily_sync())