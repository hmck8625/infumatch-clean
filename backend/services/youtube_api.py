"""
YouTube Data API é€£æºã‚µãƒ¼ãƒ“ã‚¹

@description YouTube Data API v3 ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±å–å¾—
ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ç™ºè¦‹ã¨ãƒ‡ãƒ¼ã‚¿åé›†ã®ä¸­æ ¸æ©Ÿèƒ½

@author InfuMatch Development Team
@version 1.0.0
"""

import logging
import asyncio
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import time

import httpx
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.auth
from google.oauth2 import service_account

from core.config import get_settings
from core.database import FirestoreClient, DatabaseHelper, DatabaseCollections

logger = logging.getLogger(__name__)
settings = get_settings()


class YouTubeAPIClient:
    """
    YouTube Data API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    API ã®å‘¼ã³å‡ºã—ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’æä¾›
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = settings.YOUTUBE_API_KEY
        self.quota_limit = settings.YOUTUBE_QUOTA_LIMIT
        self.rate_limit = settings.YOUTUBE_RATE_LIMIT_PER_SECOND
        self.service = None
        self._last_request_time = 0
        self._daily_quota_used = 0
        self._initialize_service()
    
    def _initialize_service(self) -> None:
        """YouTube API ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        try:
            self.service = build(
                'youtube',
                'v3',
                developerKey=self.api_key,
                cache_discovery=False  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ï¼‰
            )
            logger.info("âœ… YouTube API service initialized")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize YouTube API service: {e}")
            raise
    
    async def _rate_limit_check(self) -> None:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ãƒã‚§ãƒƒã‚¯"""
        current_time = time.time()
        time_since_last = current_time - self._last_request_time
        
        # ç§’ã‚ãŸã‚Šã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        min_interval = 1.0 / self.rate_limit
        if time_since_last < min_interval:
            sleep_time = min_interval - time_since_last
            await asyncio.sleep(sleep_time)
        
        self._last_request_time = time.time()
    
    def _check_quota(self, cost: int) -> bool:
        """ã‚¯ã‚©ãƒ¼ã‚¿ä½¿ç”¨é‡ã®ãƒã‚§ãƒƒã‚¯"""
        if self._daily_quota_used + cost > self.quota_limit:
            logger.warning(f"âš ï¸ Daily quota limit reached: {self._daily_quota_used}/{self.quota_limit}")
            return False
        return True
    
    async def search_channels(
        self,
        query: str,
        max_results: int = 50,
        order: str = 'relevance'
    ) -> List[Dict[str, Any]]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            max_results: æœ€å¤§å–å¾—ä»¶æ•°ï¼ˆ1-50ï¼‰
            order: ã‚½ãƒ¼ãƒˆé † ('relevance', 'date', 'rating', 'viewCount')
            
        Returns:
            List[Dict]: ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        await self._rate_limit_check()
        
        # ã‚¯ã‚©ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯ï¼ˆæ¤œç´¢ã¯100ãƒ¦ãƒ‹ãƒƒãƒˆæ¶ˆè²»ï¼‰
        if not self._check_quota(100):
            raise Exception("Daily quota limit exceeded")
        
        try:
            logger.info(f"ğŸ” Searching channels for query: '{query}'")
            
            search_response = self.service.search().list(
                part='snippet',
                type='channel',
                q=query,
                maxResults=min(max_results, 50),  # APIåˆ¶é™
                order=order,
                regionCode='JP',  # æ—¥æœ¬ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å„ªå…ˆ
                relevanceLanguage='ja'  # æ—¥æœ¬èªã‚’å„ªå…ˆ
            ).execute()
            
            self._daily_quota_used += 100
            
            channels = []
            for item in search_response.get('items', []):
                channel_data = {
                    'channel_id': item['id']['channelId'],
                    'title': item['snippet']['title'],
                    'description': item['snippet']['description'],
                    'thumbnail_url': item['snippet']['thumbnails'].get('high', {}).get('url'),
                    'published_at': item['snippet']['publishedAt'],
                    'search_query': query
                }
                channels.append(channel_data)
            
            logger.info(f"âœ… Found {len(channels)} channels for query: '{query}'")
            return channels
            
        except HttpError as e:
            logger.error(f"âŒ YouTube API error: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Channel search failed: {e}")
            raise
    
    async def get_channel_details(self, channel_ids: List[str]) -> List[Dict[str, Any]]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            channel_ids: ãƒãƒ£ãƒ³ãƒãƒ«IDã®ãƒªã‚¹ãƒˆï¼ˆæœ€å¤§50ä»¶ï¼‰
            
        Returns:
            List[Dict]: è©³ç´°ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        await self._rate_limit_check()
        
        # ã‚¯ã‚©ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«è©³ç´°ã¯1ãƒ¦ãƒ‹ãƒƒãƒˆæ¶ˆè²»ï¼‰
        if not self._check_quota(1):
            raise Exception("Daily quota limit exceeded")
        
        try:
            # æœ€å¤§50ä»¶ã«åˆ¶é™
            channel_ids = channel_ids[:50]
            
            logger.info(f"ğŸ“Š Getting details for {len(channel_ids)} channels")
            
            channels_response = self.service.channels().list(
                part='snippet,statistics,contentDetails,topicDetails,brandingSettings',
                id=','.join(channel_ids)
            ).execute()
            
            self._daily_quota_used += 1
            
            channels = []
            for item in channels_response.get('items', []):
                channel_data = self._extract_channel_data(item)
                channels.append(channel_data)
            
            logger.info(f"âœ… Retrieved details for {len(channels)} channels")
            return channels
            
        except HttpError as e:
            logger.error(f"âŒ YouTube API error: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Channel details retrieval failed: {e}")
            raise
    
    def _extract_channel_data(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """
        API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            item: YouTube API ã®ãƒãƒ£ãƒ³ãƒãƒ«é …ç›®
            
        Returns:
            Dict: æ•´ç†ã•ã‚ŒãŸãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
        """
        snippet = item.get('snippet', {})
        statistics = item.get('statistics', {})
        content_details = item.get('contentDetails', {})
        topic_details = item.get('topicDetails', {})
        branding = item.get('brandingSettings', {})
        
        # çµ±è¨ˆæƒ…å ±ã®å®‰å…¨ãªå–å¾—ï¼ˆhiddenã®å ´åˆã‚‚ã‚ã‚‹ï¼‰
        subscriber_count = int(statistics.get('subscriberCount', 0))
        video_count = int(statistics.get('videoCount', 0))
        view_count = int(statistics.get('viewCount', 0))
        
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã®è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
        engagement_rate = 0.0
        if subscriber_count > 0 and video_count > 0:
            avg_views_per_video = view_count / video_count
            engagement_rate = (avg_views_per_video / subscriber_count) * 100
            engagement_rate = min(engagement_rate, 100.0)  # ä¸Šé™è¨­å®š
        
        return {
            'channel_id': item['id'],
            'channel_name': snippet.get('title', ''),
            'description': snippet.get('description', ''),
            'custom_url': snippet.get('customUrl', ''),
            'thumbnail_url': snippet.get('thumbnails', {}).get('high', {}).get('url'),
            'published_at': snippet.get('publishedAt'),
            'country': snippet.get('country'),
            'default_language': snippet.get('defaultLanguage'),
            
            # çµ±è¨ˆæƒ…å ±
            'subscriber_count': subscriber_count,
            'video_count': video_count,
            'view_count': view_count,
            'hidden_subscriber_count': statistics.get('hiddenSubscriberCount', False),
            'engagement_rate': round(engagement_rate, 2),
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±
            'uploads_playlist_id': content_details.get('relatedPlaylists', {}).get('uploads'),
            'topic_ids': topic_details.get('topicIds', []),
            'topic_categories': topic_details.get('topicCategories', []),
            
            # ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±
            'keywords': branding.get('channel', {}).get('keywords'),
            'banner_image_url': branding.get('image', {}).get('bannerExternalUrl'),
            
            # å–å¾—æ—¥æ™‚
            'fetched_at': datetime.utcnow().isoformat(),
        }
    
    async def get_recent_videos(
        self,
        channel_id: str,
        max_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€æ–°å‹•ç”»ã‚’å–å¾—
        
        Args:
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
            max_results: æœ€å¤§å–å¾—ä»¶æ•°
            
        Returns:
            List[Dict]: å‹•ç”»æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        await self._rate_limit_check()
        
        if not self._check_quota(1):
            raise Exception("Daily quota limit exceeded")
        
        try:
            # ã¾ãšãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆIDã‚’å–å¾—
            channel_response = self.service.channels().list(
                part='contentDetails',
                id=channel_id
            ).execute()
            
            items = channel_response.get('items', [])
            if not items:
                return []
            
            uploads_playlist_id = items[0]['contentDetails']['relatedPlaylists']['uploads']
            
            # ãƒ—ãƒ¬ã‚¤ãƒªã‚¹ãƒˆã‹ã‚‰æœ€æ–°å‹•ç”»ã‚’å–å¾—
            playlist_response = self.service.playlistItems().list(
                part='snippet,contentDetails',
                playlistId=uploads_playlist_id,
                maxResults=max_results
            ).execute()
            
            self._daily_quota_used += 2  # 2å›ã®APIå‘¼ã³å‡ºã—
            
            videos = []
            for item in playlist_response.get('items', []):
                video_data = {
                    'video_id': item['contentDetails']['videoId'],
                    'title': item['snippet']['title'],
                    'description': item['snippet']['description'],
                    'thumbnail_url': item['snippet']['thumbnails'].get('high', {}).get('url'),
                    'published_at': item['snippet']['publishedAt'],
                    'channel_id': channel_id
                }
                videos.append(video_data)
            
            return videos
            
        except HttpError as e:
            logger.error(f"âŒ YouTube API error: {e}")
            raise
        except Exception as e:
            logger.error(f"âŒ Recent videos retrieval failed: {e}")
            raise


class EmailExtractor:
    """
    ãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜æ–‡ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º
    
    æ­£è¦è¡¨ç¾ã¨ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚’ä½¿ç”¨
    """
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
    EMAIL_PATTERN = re.compile(
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    )
    
    # ãƒ“ã‚¸ãƒã‚¹ç”¨ãƒ¡ãƒ¼ãƒ«ã®åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    BUSINESS_KEYWORDS = [
        'ãŠä»•äº‹', 'ãƒ“ã‚¸ãƒã‚¹', 'business', 'work', 'collaboration', 'collab',
        'ã‚³ãƒ©ãƒœ', 'ä»•äº‹', 'ä¾é ¼', 'ç›¸è«‡', 'contact', 'inquiry', 'pr', 'ãƒ—ãƒ­ãƒ¢'
    ]
    
    @classmethod
    def extract_emails(cls, text: str) -> List[Dict[str, Any]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º
        
        Args:
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            List[Dict]: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        if not text:
            return []
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º
        emails = cls.EMAIL_PATTERN.findall(text.lower())
        
        if not emails:
            return []
        
        # é‡è¤‡é™¤å»ã¨å„ªå…ˆåº¦åˆ¤å®š
        unique_emails = []
        seen_emails = set()
        
        for email in emails:
            if email in seen_emails:
                continue
                
            seen_emails.add(email)
            priority = cls._calculate_priority(email, text)
            
            unique_emails.append({
                'email': email,
                'priority': priority,
                'context': cls._extract_context(email, text),
                'is_business': cls._is_business_email(email, text)
            })
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        unique_emails.sort(key=lambda x: x['priority'], reverse=True)
        
        return unique_emails
    
    @classmethod
    def _calculate_priority(cls, email: str, text: str) -> int:
        """
        ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å„ªå…ˆåº¦ã‚’è¨ˆç®—
        
        Args:
            email: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
            text: å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            int: å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆ1-10ï¼‰
        """
        priority = 1
        
        # ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³ã¯é«˜å„ªå…ˆåº¦
        if not any(domain in email for domain in ['gmail.com', 'yahoo.co.jp', 'hotmail.com']):
            priority += 3
        
        # ãƒ“ã‚¸ãƒã‚¹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¿‘ãã«ã‚ã‚‹
        email_index = text.lower().find(email)
        if email_index != -1:
            context_start = max(0, email_index - 50)
            context_end = min(len(text), email_index + len(email) + 50)
            context = text[context_start:context_end].lower()
            
            for keyword in cls.BUSINESS_KEYWORDS:
                if keyword in context:
                    priority += 2
                    break
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹è‡ªä½“ã«ãƒ“ã‚¸ãƒã‚¹ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹
        business_prefixes = ['business', 'contact', 'info', 'pr', 'collab', 'work']
        for prefix in business_prefixes:
            if prefix in email.split('@')[0]:
                priority += 1
                break
        
        return min(priority, 10)  # æœ€å¤§10
    
    @classmethod
    def _extract_context(cls, email: str, text: str) -> str:
        """
        ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å‘¨è¾ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            email: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
            text: å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            str: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ–‡å­—åˆ—
        """
        email_index = text.find(email)
        if email_index == -1:
            return ""
        
        context_start = max(0, email_index - 30)
        context_end = min(len(text), email_index + len(email) + 30)
        
        return text[context_start:context_end].strip()
    
    @classmethod
    def _is_business_email(cls, email: str, text: str) -> bool:
        """
        ãƒ“ã‚¸ãƒã‚¹ç”¨ãƒ¡ãƒ¼ãƒ«ã‹ã©ã†ã‹ã®åˆ¤å®š
        
        Args:
            email: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
            text: å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            bool: ãƒ“ã‚¸ãƒã‚¹ç”¨ã®å ´åˆ True
        """
        # ç‹¬è‡ªãƒ‰ãƒ¡ã‚¤ãƒ³ã¯é«˜ç¢ºç‡ã§ãƒ“ã‚¸ãƒã‚¹ç”¨
        if not any(domain in email for domain in ['gmail.com', 'yahoo.co.jp', 'hotmail.com']):
            return True
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹
        context = cls._extract_context(email, text)
        for keyword in cls.BUSINESS_KEYWORDS:
            if keyword in context.lower():
                return True
        
        return False


class YouTubeInfluencerService:
    """
    YouTube ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼é–¢é€£ã®ã‚µãƒ¼ãƒ“ã‚¹
    
    æ¤œç´¢ã€åˆ†æã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã®çµ±åˆæ©Ÿèƒ½
    """
    
    def __init__(self):
        self.api_client = YouTubeAPIClient()
        self.db_client = FirestoreClient()
        self.db_helper = DatabaseHelper(self.db_client)
        self.email_extractor = EmailExtractor()
    
    async def discover_influencers(
        self,
        search_queries: List[str],
        subscriber_min: int = 1000,
        subscriber_max: int = 100000,
        max_per_query: int = 20
    ) -> List[Dict[str, Any]]:
        """
        ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’ç™ºè¦‹
        
        Args:
            search_queries: æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
            subscriber_min: æœ€å°ç™»éŒ²è€…æ•°
            subscriber_max: æœ€å¤§ç™»éŒ²è€…æ•°
            max_per_query: ã‚¯ã‚¨ãƒªã‚ãŸã‚Šã®æœ€å¤§å–å¾—æ•°
            
        Returns:
            List[Dict]: ç™ºè¦‹ã•ã‚ŒãŸã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±
        """
        logger.info(f"ğŸ” Starting influencer discovery for {len(search_queries)} queries")
        
        all_influencers = []
        processed_channel_ids = set()
        
        for query in search_queries:
            try:
                # ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
                channels = await self.api_client.search_channels(
                    query=query,
                    max_results=max_per_query
                )
                
                if not channels:
                    continue
                
                # é‡è¤‡é™¤å»
                channel_ids = [
                    ch['channel_id'] for ch in channels 
                    if ch['channel_id'] not in processed_channel_ids
                ]
                
                if not channel_ids:
                    continue
                
                # è©³ç´°æƒ…å ±å–å¾—
                detailed_channels = await self.api_client.get_channel_details(channel_ids)
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç™»éŒ²è€…æ•°ç¯„å›²ï¼‰
                filtered_channels = [
                    ch for ch in detailed_channels
                    if subscriber_min <= ch['subscriber_count'] <= subscriber_max
                    and not ch['hidden_subscriber_count']
                ]
                
                # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
                for channel in filtered_channels:
                    emails = self.email_extractor.extract_emails(channel['description'])
                    channel['emails'] = emails
                    channel['has_business_email'] = any(e['is_business'] for e in emails)
                
                all_influencers.extend(filtered_channels)
                processed_channel_ids.update(channel_ids)
                
                logger.info(f"âœ… Query '{query}': Found {len(filtered_channels)} matching influencers")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ Failed to process query '{query}': {e}")
                continue
        
        logger.info(f"ğŸ‰ Discovery completed: {len(all_influencers)} total influencers found")
        return all_influencers
    
    async def save_influencers_to_db(self, influencers: List[Dict[str, Any]]) -> int:
        """
        ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        
        Args:
            influencers: ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            int: ä¿å­˜æˆåŠŸä»¶æ•°
        """
        logger.info(f"ğŸ’¾ Saving {len(influencers)} influencers to database")
        
        saved_count = 0
        
        for influencer in influencers:
            try:
                # Firestore ç”¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
                doc_data = {
                    'channel_id': influencer['channel_id'],
                    'channel_name': influencer['channel_name'],
                    'description': influencer['description'],
                    'custom_url': influencer.get('custom_url', ''),
                    'thumbnail_url': influencer.get('thumbnail_url', ''),
                    'subscriber_count': influencer['subscriber_count'],
                    'video_count': influencer['video_count'],
                    'view_count': influencer['view_count'],
                    'engagement_rate': influencer['engagement_rate'],
                    'emails': influencer.get('emails', []),
                    'has_business_email': influencer.get('has_business_email', False),
                    'topic_categories': influencer.get('topic_categories', []),
                    'country': influencer.get('country', ''),
                    'fetched_at': influencer['fetched_at'],
                    'data_quality_score': self._calculate_quality_score(influencer),
                    'status': 'active'
                }
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ãƒãƒ«IDã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¨ã—ã¦ä½¿ç”¨ï¼‰
                await self.db_helper.create_document(
                    collection=DatabaseCollections.INFLUENCERS,
                    data=doc_data,
                    document_id=influencer['channel_id']
                )
                
                saved_count += 1
                
            except Exception as e:
                logger.error(f"âŒ Failed to save influencer {influencer['channel_id']}: {e}")
                continue
        
        logger.info(f"âœ… Saved {saved_count}/{len(influencers)} influencers to database")
        return saved_count
    
    def _calculate_quality_score(self, influencer: Dict[str, Any]) -> float:
        """
        ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        Args:
            influencer: ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±
            
        Returns:
            float: å“è³ªã‚¹ã‚³ã‚¢ (0.0-1.0)
        """
        score = 0.0
        
        # åŸºæœ¬æƒ…å ±ã®å®Œå…¨æ€§
        if influencer.get('channel_name'):
            score += 0.2
        if influencer.get('description'):
            score += 0.2
        if influencer.get('thumbnail_url'):
            score += 0.1
        
        # çµ±è¨ˆæƒ…å ±ã®å¦¥å½“æ€§
        if influencer['subscriber_count'] > 0:
            score += 0.2
        if influencer['video_count'] > 0:
            score += 0.1
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æœ‰ç„¡
        if influencer.get('has_business_email'):
            score += 0.2
        
        return round(score, 2)
    
    async def update_influencer_analytics(self, channel_id: str) -> bool:
        """
        ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®åˆ†ææƒ…å ±ã‚’æ›´æ–°
        
        Args:
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
            
        Returns:
            bool: æ›´æ–°æˆåŠŸæ™‚ True
        """
        try:
            # æœ€æ–°ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            channels = await self.api_client.get_channel_details([channel_id])
            
            if not channels:
                return False
            
            channel = channels[0]
            
            # æœ€æ–°å‹•ç”»æƒ…å ±ã‚‚å–å¾—
            recent_videos = await self.api_client.get_recent_videos(channel_id)
            
            # æ›´æ–°ãƒ‡ãƒ¼ã‚¿
            update_data = {
                'subscriber_count': channel['subscriber_count'],
                'video_count': channel['video_count'],
                'view_count': channel['view_count'],
                'engagement_rate': channel['engagement_rate'],
                'recent_videos': recent_videos[:5],  # æœ€æ–°5ä»¶
                'last_analyzed': datetime.utcnow().isoformat(),
                'data_quality_score': self._calculate_quality_score(channel)
            }
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            success = await self.db_helper.update_document(
                collection=DatabaseCollections.INFLUENCERS,
                document_id=channel_id,
                data=update_data
            )
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ Failed to update analytics for {channel_id}: {e}")
            return False