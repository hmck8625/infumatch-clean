"""
YouTube ãƒ‡ãƒ¼ã‚¿ ãƒãƒƒãƒå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹

@description å¤§è¦æ¨¡ãªYouTubeãƒ‡ãƒ¼ã‚¿åé›†ãƒ»æ›´æ–°ã®ãƒãƒƒãƒå‡¦ç†
ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆæ›¸ã®Phase 1è¦ä»¶ã«å¯¾å¿œ

@author InfuMatch Development Team
@version 1.0.0
"""

import logging
import asyncio
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
import time

# from services.youtube_api import YouTubeInfluencerService, YouTubeAPIClient  # ãƒ†ã‚¹ãƒˆç”¨ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
from core.config import get_settings
from core.database import FirestoreClient, DatabaseHelper, DatabaseCollections

logger = logging.getLogger(__name__)
settings = get_settings()


@dataclass
class BatchConfig:
    """ãƒãƒƒãƒå‡¦ç†è¨­å®š"""
    max_channels_per_batch: int = 100
    max_concurrent_requests: int = 5
    delay_between_batches: float = 2.0
    quota_safety_margin: int = 1000
    retry_attempts: int = 3
    retry_delay: float = 5.0


class YouTubeBatchProcessor:
    """
    YouTube ãƒ‡ãƒ¼ã‚¿ã®å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†
    
    å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿åé›†ã€æ›´æ–°ã€åˆ†æå‡¦ç†ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œ
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.config = BatchConfig()
        self.youtube_service = YouTubeInfluencerService()
        self.api_client = YouTubeAPIClient()
        self.db_client = FirestoreClient()
        self.db_helper = DatabaseHelper(self.db_client)
        
        # å‡¦ç†çµ±è¨ˆ
        self.stats = {
            'channels_processed': 0,
            'channels_failed': 0,
            'api_calls_made': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def discover_influencers_batch(
        self,
        categories: List[str],
        subscriber_ranges: List[tuple] = None,
        max_per_category: int = 50
    ) -> Dict[str, Any]:
        """
        ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã§ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’å¤§è¦æ¨¡ç™ºè¦‹
        
        Args:
            categories: æ¤œç´¢ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆ
            subscriber_ranges: ç™»éŒ²è€…æ•°ç¯„å›²ã®ãƒªã‚¹ãƒˆ [(min, max), ...]
            max_per_category: ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®æœ€å¤§å–å¾—æ•°
            
        Returns:
            Dict: å‡¦ç†çµæœçµ±è¨ˆ
        """
        logger.info(f"ğŸš€ Starting batch influencer discovery for {len(categories)} categories")
        self.stats['start_time'] = datetime.utcnow()
        
        if subscriber_ranges is None:
            subscriber_ranges = [
                (1000, 10000),      # ãƒã‚¤ã‚¯ãƒ­ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
                (10000, 100000),    # ãƒŸãƒ‰ãƒ«ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼  
                (100000, 1000000),  # ãƒã‚¯ãƒ­ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
            ]
        
        all_results = []
        
        for category in categories:
            logger.info(f"ğŸ“‚ Processing category: {category}")
            
            try:
                # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã®æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ
                search_queries = self._generate_search_queries(category)
                
                for min_subs, max_subs in subscriber_ranges:
                    logger.info(f"ğŸ‘¥ Subscriber range: {min_subs:,} - {max_subs:,}")
                    
                    # ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ç™ºè¦‹
                    influencers = await self.youtube_service.discover_influencers(
                        search_queries=search_queries,
                        subscriber_min=min_subs,
                        subscriber_max=max_subs,
                        max_per_query=max_per_category // len(search_queries)
                    )
                    
                    if influencers:
                        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
                        for influencer in influencers:
                            influencer['discovered_category'] = category
                            influencer['subscriber_range'] = f"{min_subs}-{max_subs}"
                        
                        all_results.extend(influencers)
                        self.stats['channels_processed'] += len(influencers)
                        
                        logger.info(f"âœ… Found {len(influencers)} influencers in {category} ({min_subs:,}-{max_subs:,})")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                    await asyncio.sleep(self.config.delay_between_batches)
                
            except Exception as e:
                logger.error(f"âŒ Failed to process category {category}: {e}")
                self.stats['channels_failed'] += 1
                continue
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬ä¿å­˜
        if all_results:
            saved_count = await self.youtube_service.save_influencers_to_db(all_results)
            logger.info(f"ğŸ’¾ Saved {saved_count}/{len(all_results)} to database")
        
        self.stats['end_time'] = datetime.utcnow()
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        return {
            'total_discovered': len(all_results),
            'categories_processed': len(categories),
            'processing_time_seconds': duration,
            'channels_per_minute': len(all_results) / (duration / 60) if duration > 0 else 0,
            'stats': self.stats
        }
    
    def _generate_search_queries(self, category: str) -> List[str]:
        """
        ã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        
        Args:
            category: ã‚«ãƒ†ã‚´ãƒªå
            
        Returns:
            List[str]: æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
        """
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¤œç´¢ã‚¯ã‚¨ãƒªãƒãƒƒãƒ—
        query_map = {
            'beauty': [
                'ãƒ¡ã‚¤ã‚¯', 'ã‚³ã‚¹ãƒ¡', 'ã‚¹ã‚­ãƒ³ã‚±ã‚¢', 'beauty', 'makeup tutorial',
                'cosmetics review', 'skincare routine'
            ],
            'gaming': [
                'ã‚²ãƒ¼ãƒ å®Ÿæ³', 'gaming', 'gameplay', 'ã‚²ãƒ¼ãƒ æ”»ç•¥', 'esports',
                'game review', 'lets play'
            ],
            'cooking': [
                'æ–™ç†', 'ãƒ¬ã‚·ãƒ”', 'cooking', 'recipe', 'æ‰‹æ–™ç†', 'baking',
                'food', 'ãŠã†ã¡ã”ã¯ã‚“'
            ],
            'tech': [
                'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ã‚¬ã‚¸ã‚§ãƒƒãƒˆ', 'technology', 'tech review',
                'gadget', 'iPhone', 'Android', 'PC'
            ],
            'fitness': [
                'ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹', 'ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°', 'fitness', 'workout', 'yoga',
                'diet', 'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ', 'ç­‹ãƒˆãƒ¬'
            ],
            'fashion': [
                'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³', 'fashion', 'outfit', 'style', 'coordinate',
                'ãƒ–ãƒ©ãƒ³ãƒ‰', 'shopping'
            ],
            'travel': [
                'æ—…è¡Œ', 'travel', 'trip', 'è¦³å…‰', 'vlog', 'æµ·å¤–æ—…è¡Œ',
                'domestic travel', 'æ¸©æ³‰'
            ],
            'education': [
                'æ•™è‚²', 'education', 'å­¦ç¿’', 'tutorial', 'how to',
                'è‹±èª', 'language learning', 'study'
            ]
        }
        
        # ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹ã‚¯ã‚¨ãƒªã‚’å–å¾—ã€ãªã‘ã‚Œã°ã‚«ãƒ†ã‚´ãƒªåã‚’ãã®ã¾ã¾ä½¿ç”¨
        queries = query_map.get(category.lower(), [category])
        
        # æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã§ã‚«ãƒãƒ¼
        if category.lower() not in query_map:
            queries.extend([f"{category} japan", f"{category} japanese", f"{category} tutorial"])
        
        return queries[:5]  # æœ€å¤§5ã¤ã®ã‚¯ã‚¨ãƒªã«åˆ¶é™
    
    async def update_existing_influencers(
        self,
        batch_size: int = 50,
        days_since_last_update: int = 7
    ) -> Dict[str, Any]:
        """
        æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬æ›´æ–°
        
        Args:
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            days_since_last_update: æœ€çµ‚æ›´æ–°ã‹ã‚‰ã®æ—¥æ•°
            
        Returns:
            Dict: æ›´æ–°çµæœçµ±è¨ˆ
        """
        logger.info(f"ğŸ”„ Starting batch update for existing influencers")
        self.stats['start_time'] = datetime.utcnow()
        
        # æ›´æ–°å¯¾è±¡ã®å–å¾—
        cutoff_date = datetime.utcnow() - timedelta(days=days_since_last_update)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ›´æ–°å¯¾è±¡ã‚’å–å¾—
            query_conditions = [
                ('last_analyzed', '<', cutoff_date.isoformat()),
                ('status', '==', 'active')
            ]
            
            influencers = await self.db_helper.query_documents(
                collection=DatabaseCollections.INFLUENCERS,
                conditions=query_conditions,
                limit=1000  # ä¸€åº¦ã«æœ€å¤§1000ä»¶
            )
            
            logger.info(f"ğŸ“Š Found {len(influencers)} influencers to update")
            
            updated_count = 0
            failed_count = 0
            
            # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
            for i in range(0, len(influencers), batch_size):
                batch = influencers[i:i + batch_size]
                
                logger.info(f"ğŸ”„ Processing batch {i//batch_size + 1}/{(len(influencers) + batch_size - 1)//batch_size}")
                
                # ä¸¦è¡Œå‡¦ç†ã§æ›´æ–°
                update_tasks = [
                    self.youtube_service.update_influencer_analytics(inf['channel_id'])
                    for inf in batch
                ]
                
                results = await asyncio.gather(*update_tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        failed_count += 1
                        logger.error(f"âŒ Update failed: {result}")
                    elif result:
                        updated_count += 1
                    else:
                        failed_count += 1
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(self.config.delay_between_batches)
            
            self.stats['end_time'] = datetime.utcnow()
            duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
            
            return {
                'total_checked': len(influencers),
                'successfully_updated': updated_count,
                'failed_updates': failed_count,
                'processing_time_seconds': duration,
                'updates_per_minute': updated_count / (duration / 60) if duration > 0 else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ Batch update failed: {e}")
            return {
                'error': str(e),
                'total_checked': 0,
                'successfully_updated': 0,
                'failed_updates': 0
            }
    
    async def analyze_trending_channels(
        self,
        region: str = 'JP',
        max_results: int = 100
    ) -> Dict[str, Any]:
        """
        ãƒˆãƒ¬ãƒ³ãƒ‰ãƒãƒ£ãƒ³ãƒãƒ«ã®åˆ†æ
        
        Args:
            region: åœ°åŸŸã‚³ãƒ¼ãƒ‰
            max_results: æœ€å¤§å–å¾—æ•°
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        logger.info(f"ğŸ“ˆ Analyzing trending channels in {region}")
        
        try:
            # äººæ°—ã‚«ãƒ†ã‚´ãƒªã§ã®æ¤œç´¢
            trending_queries = [
                'ãƒã‚ºã£ãŸ', 'viral', 'trending', 'è©±é¡Œ', 'popular',
                'æ€¥ä¸Šæ˜‡', 'ãŠã™ã™ã‚', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°'
            ]
            
            all_channels = []
            
            for query in trending_queries:
                # æœ€æ–°ã®äººæ°—ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¤œç´¢
                channels = await self.api_client.search_channels(
                    query=query,
                    max_results=max_results // len(trending_queries),
                    order='viewCount'
                )
                
                if channels:
                    # è©³ç´°æƒ…å ±ã‚’å–å¾—
                    channel_ids = [ch['channel_id'] for ch in channels]
                    detailed_channels = await self.api_client.get_channel_details(channel_ids)
                    
                    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
                    for channel in detailed_channels:
                        emails = self.youtube_service.email_extractor.extract_emails(
                            channel['description']
                        )
                        channel['emails'] = emails
                        channel['trending_query'] = query
                    
                    all_channels.extend(detailed_channels)
                
                await asyncio.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            # é‡è¤‡é™¤å»
            unique_channels = {}
            for channel in all_channels:
                if channel['channel_id'] not in unique_channels:
                    unique_channels[channel['channel_id']] = channel
            
            trending_channels = list(unique_channels.values())
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            analysis = self._analyze_trends(trending_channels)
            
            return {
                'total_channels_analyzed': len(trending_channels),
                'trending_analysis': analysis,
                'top_channels': sorted(
                    trending_channels,
                    key=lambda x: x['engagement_rate'],
                    reverse=True
                )[:10]
            }
            
        except Exception as e:
            logger.error(f"âŒ Trending analysis failed: {e}")
            return {'error': str(e)}
    
    def _analyze_trends(self, channels: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        
        Args:
            channels: ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆ
            
        Returns:
            Dict: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ
        """
        if not channels:
            return {}
        
        # çµ±è¨ˆè¨ˆç®—
        subscriber_counts = [ch['subscriber_count'] for ch in channels]
        engagement_rates = [ch['engagement_rate'] for ch in channels]
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
        category_distribution = {}
        for channel in channels:
            categories = channel.get('topic_categories', [])
            for category in categories:
                category_distribution[category] = category_distribution.get(category, 0) + 1
        
        # ãƒ¡ãƒ¼ãƒ«é€£çµ¡å¯èƒ½ç‡
        contactable_count = sum(1 for ch in channels if ch.get('emails'))
        contactable_rate = contactable_count / len(channels) if channels else 0
        
        return {
            'average_subscriber_count': sum(subscriber_counts) / len(subscriber_counts) if subscriber_counts else 0,
            'median_subscriber_count': sorted(subscriber_counts)[len(subscriber_counts)//2] if subscriber_counts else 0,
            'average_engagement_rate': sum(engagement_rates) / len(engagement_rates) if engagement_rates else 0,
            'category_distribution': sorted(
                category_distribution.items(),
                key=lambda x: x[1],
                reverse=True
            )[:10],
            'contactable_rate': round(contactable_rate * 100, 2),
            'total_channels': len(channels)
        }
    
    async def cleanup_old_data(self, days_to_keep: int = 90) -> Dict[str, Any]:
        """
        å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            days_to_keep: ä¿æŒæ—¥æ•°
            
        Returns:
            Dict: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ
        """
        logger.info(f"ğŸ§¹ Starting data cleanup (keeping last {days_to_keep} days)")
        
        cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
        
        try:
            # éã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’ç‰¹å®š
            query_conditions = [
                ('fetched_at', '<', cutoff_date.isoformat()),
                ('status', '==', 'inactive')
            ]
            
            old_influencers = await self.db_helper.query_documents(
                collection=DatabaseCollections.INFLUENCERS,
                conditions=query_conditions
            )
            
            # å‰Šé™¤å®Ÿè¡Œ
            deleted_count = 0
            for influencer in old_influencers:
                try:
                    await self.db_helper.delete_document(
                        collection=DatabaseCollections.INFLUENCERS,
                        document_id=influencer['channel_id']
                    )
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"âŒ Failed to delete {influencer['channel_id']}: {e}")
            
            return {
                'deleted_count': deleted_count,
                'cutoff_date': cutoff_date.isoformat(),
                'cleanup_completed': True
            }
            
        except Exception as e:
            logger.error(f"âŒ Cleanup failed: {e}")
            return {'error': str(e), 'cleanup_completed': False}


# ãƒãƒƒãƒå‡¦ç†ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
async def run_daily_batch():
    """æ—¥æ¬¡ãƒãƒƒãƒå‡¦ç†"""
    processor = YouTubeBatchProcessor()
    
    # 1. æ–°è¦ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ç™ºè¦‹
    discovery_result = await processor.discover_influencers_batch(
        categories=['beauty', 'gaming', 'cooking', 'tech', 'fitness'],
        max_per_category=20
    )
    
    # 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°
    update_result = await processor.update_existing_influencers(
        batch_size=50,
        days_since_last_update=7
    )
    
    # 3. ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    trend_result = await processor.analyze_trending_channels()
    
    logger.info(f"ğŸ“Š Daily batch completed:")
    logger.info(f"  - Discovered: {discovery_result.get('total_discovered', 0)} new influencers")
    logger.info(f"  - Updated: {update_result.get('successfully_updated', 0)} existing influencers")
    logger.info(f"  - Analyzed: {trend_result.get('total_channels_analyzed', 0)} trending channels")
    
    return {
        'discovery': discovery_result,
        'updates': update_result,
        'trends': trend_result
    }


if __name__ == "__main__":
    # ãƒãƒƒãƒå‡¦ç†ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_daily_batch())