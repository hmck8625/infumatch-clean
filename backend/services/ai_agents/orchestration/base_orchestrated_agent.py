"""
ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

@description ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åŸºåº•ã‚¯ãƒ©ã‚¹
@author InfuMatch Development Team
@version 2.0.0
"""

import logging
import asyncio
import time
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from datetime import datetime

from ..base_agent import BaseAgent, AgentConfig
from .agent_message import AgentMessage, MessageType, Priority
from .negotiation_state import NegotiationState


logger = logging.getLogger(__name__)


class BaseOrchestratedAgent(BaseAgent):
    """
    ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œãƒ™ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤ºã‚’å—ã‘ã¦å°‚é–€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€
    çµæœã‚’æ§‹é€ åŒ–ã—ã¦è¿”ã™æ©Ÿèƒ½ã‚’æä¾›
    """
    
    def __init__(self, config: AgentConfig, agent_id: str, specialization: str):
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            config: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
            agent_id: ä¸€æ„ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID
            specialization: å°‚é–€åˆ†é‡
        """
        super().__init__(config)
        self.agent_id = agent_id
        self.specialization = specialization
        self.performance_metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "average_confidence": 0.0,
            "average_processing_time": 0.0,
            "last_activity": None
        }
        
        logger.info(f"ğŸ¤– {self.agent_id} ({self.specialization}) ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
    
    async def process_message(self, message: AgentMessage, state: NegotiationState) -> AgentMessage:
        """
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã¦çµæœã‚’è¿”ã™
        
        Args:
            message: å—ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            state: ç¾åœ¨ã®äº¤æ¸‰çŠ¶æ…‹
            
        Returns:
            AgentMessage: å‡¦ç†çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ“¨ {self.agent_id}: ã‚¿ã‚¹ã‚¯é–‹å§‹ - {message.task_type}")
            
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
            if message.message_type == MessageType.TASK_REQUEST:
                result = await self.execute_task(message.task_type, message.payload, state)
                
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                # æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
                self._update_performance_metrics(True, result.get("confidence", 0.0), processing_time_ms)
                
                # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
                response = AgentMessage.create_task_result(
                    sender_id=self.agent_id,
                    recipient_id=message.sender_id,
                    task_type=message.task_type,
                    payload=result,
                    confidence_score=result.get("confidence", 0.0),
                    parent_message_id=message.message_id,
                    correlation_id=message.correlation_id,
                    processing_time_ms=processing_time_ms
                )
                
                logger.info(f"âœ… {self.agent_id}: ã‚¿ã‚¹ã‚¯å®Œäº† - {message.task_type} (ä¿¡é ¼åº¦: {result.get('confidence', 0.0):.2f})")
                return response
                
            elif message.message_type == MessageType.EVALUATION_REQUEST:
                # ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æˆæœã‚’è©•ä¾¡
                evaluation = await self.evaluate_peer_result(message.payload, state)
                
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                response = AgentMessage.create_task_result(
                    sender_id=self.agent_id,
                    recipient_id=message.sender_id,
                    task_type="peer_evaluation",
                    payload=evaluation,
                    confidence_score=evaluation.get("confidence", 0.0),
                    parent_message_id=message.message_id,
                    correlation_id=message.correlation_id,
                    processing_time_ms=processing_time_ms
                )
                
                return response
            
            else:
                raise ValueError(f"Unsupported message type: {message.message_type}")
                
        except Exception as e:
            logger.error(f"âŒ {self.agent_id}: ã‚¿ã‚¹ã‚¯å¤±æ•— - {message.task_type}: {str(e)}")
            
            # å¤±æ•—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
            processing_time_ms = int((time.time() - start_time) * 1000)
            self._update_performance_metrics(False, 0.0, processing_time_ms)
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            return AgentMessage.create_error_report(
                sender_id=self.agent_id,
                recipient_id=message.sender_id,
                error_message=str(e),
                error_details={
                    "task_type": message.task_type,
                    "specialization": self.specialization,
                    "processing_time_ms": processing_time_ms
                },
                parent_message_id=message.message_id,
                correlation_id=message.correlation_id
            )
    
    @abstractmethod
    async def execute_task(self, task_type: str, payload: Dict[str, Any], state: NegotiationState) -> Dict[str, Any]:
        """
        å°‚é–€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        
        Args:
            task_type: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            payload: ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
            state: äº¤æ¸‰çŠ¶æ…‹
            
        Returns:
            Dict: å®Ÿè¡Œçµæœï¼ˆconfidenceãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€ï¼‰
        """
        pass
    
    async def evaluate_peer_result(self, peer_result: Dict[str, Any], state: NegotiationState) -> Dict[str, Any]:
        """
        ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çµæœã‚’è©•ä¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…ï¼‰
        
        Args:
            peer_result: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çµæœ
            state: äº¤æ¸‰çŠ¶æ…‹
            
        Returns:
            Dict: è©•ä¾¡çµæœ
        """
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è©•ä¾¡ãªã—
        return {
            "evaluation_score": 0.5,
            "feedback": "è©•ä¾¡æ©Ÿèƒ½æœªå®Ÿè£…",
            "confidence": 0.1
        }
    
    def get_capabilities(self) -> Dict[str, Any]:
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èƒ½åŠ›æƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict: èƒ½åŠ›æƒ…å ±
        """
        return {
            "agent_id": self.agent_id,
            "specialization": self.specialization,
            "supported_tasks": self.get_supported_tasks(),
            "performance_metrics": self.performance_metrics,
            "status": "active"
        }
    
    @abstractmethod
    def get_supported_tasks(self) -> List[str]:
        """
        ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Returns:
            List[str]: ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
        """
        pass
    
    def _update_performance_metrics(self, success: bool, confidence: float, processing_time_ms: int):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°"""
        self.performance_metrics["total_tasks"] += 1
        
        if success:
            self.performance_metrics["successful_tasks"] += 1
        
        # å¹³å‡ä¿¡é ¼åº¦ã‚’æ›´æ–°
        total_tasks = self.performance_metrics["total_tasks"]
        current_avg_confidence = self.performance_metrics["average_confidence"]
        self.performance_metrics["average_confidence"] = (
            (current_avg_confidence * (total_tasks - 1) + confidence) / total_tasks
        )
        
        # å¹³å‡å‡¦ç†æ™‚é–“ã‚’æ›´æ–°
        current_avg_time = self.performance_metrics["average_processing_time"]
        self.performance_metrics["average_processing_time"] = (
            (current_avg_time * (total_tasks - 1) + processing_time_ms) / total_tasks
        )
        
        self.performance_metrics["last_activity"] = datetime.now().isoformat()
    
    def get_success_rate(self) -> float:
        """æˆåŠŸç‡ã‚’å–å¾—"""
        if self.performance_metrics["total_tasks"] == 0:
            return 0.0
        return self.performance_metrics["successful_tasks"] / self.performance_metrics["total_tasks"]
    
    def get_status_summary(self) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¦ç´„ã‚’å–å¾—"""
        return {
            "agent_id": self.agent_id,
            "specialization": self.specialization,
            "success_rate": self.get_success_rate(),
            "average_confidence": self.performance_metrics["average_confidence"],
            "average_processing_time_ms": self.performance_metrics["average_processing_time"],
            "total_tasks_completed": self.performance_metrics["total_tasks"],
            "last_activity": self.performance_metrics["last_activity"]
        }