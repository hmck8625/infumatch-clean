"""
AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹

@description Vertex AI/Gemini ã‚’æ´»ç”¨ã—ãŸé«˜åº¦ãªåˆ†ææ©Ÿèƒ½
ã‚«ãƒ†ã‚´ãƒªåˆ†æã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºã® AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

@author InfuMatch Development Team  
@version 1.0.0
"""

import logging
import asyncio
import json
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta

import google.generativeai as genai
# from google.cloud import aiplatform  # ä½¿ç”¨ã—ãªã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from google.oauth2 import service_account  # ä½¿ç”¨ã—ãªã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

from core.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class GeminiClient:
    """
    Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    é«˜ç²¾åº¦ãªè‡ªç„¶è¨€èªå‡¦ç†ã«ã‚ˆã‚‹åˆ†æ
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.api_key = settings.GEMINI_API_KEY
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # å®‰å…¨è¨­å®š
        self.safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH", 
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
        
        logger.info("âœ… Gemini client initialized")
    
    async def analyze_text(
        self,
        prompt: str,
        text: str,
        max_retries: int = 3
    ) -> Optional[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®å®Ÿè¡Œ
        
        Args:
            prompt: åˆ†ææŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            text: åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            Optional[str]: åˆ†æçµæœ
        """
        full_prompt = f"{prompt}\n\nåˆ†æå¯¾è±¡:\n{text}"
        
        for attempt in range(max_retries):
            try:
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    full_prompt,
                    safety_settings=self.safety_settings,
                    generation_config={
                        'temperature': 0.1,  # ä¸€è²«æ€§ã‚’é‡è¦–
                        'top_p': 0.8,
                        'top_k': 40,
                        'max_output_tokens': 2048,
                    }
                )
                
                if response.text:
                    return response.text.strip()
                else:
                    logger.warning("âš ï¸ Empty response from Gemini")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Gemini API attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                continue
        
        logger.error("âŒ All Gemini API attempts failed")
        return None


class CategoryAnalyzer:
    """
    YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã®ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    AI ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾åº¦ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã¨ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªç‰¹å®š
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.gemini = GeminiClient()
        
        # ä¸»è¦ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©
        self.main_categories = {
            'beauty': {
                'name': 'ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼ãƒ»ã‚³ã‚¹ãƒ¡',
                'subcategories': ['ãƒ¡ã‚¤ã‚¯', 'ã‚¹ã‚­ãƒ³ã‚±ã‚¢', 'ãƒ˜ã‚¢ã‚±ã‚¢', 'ãƒã‚¤ãƒ«', 'é¦™æ°´']
            },
            'gaming': {
                'name': 'ã‚²ãƒ¼ãƒ ',
                'subcategories': ['å®Ÿæ³', 'æ”»ç•¥', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'eã‚¹ãƒãƒ¼ãƒ„', 'ãƒ¬ãƒˆãƒ­ã‚²ãƒ¼ãƒ ']
            },
            'cooking': {
                'name': 'æ–™ç†ãƒ»ã‚°ãƒ«ãƒ¡',
                'subcategories': ['ãƒ¬ã‚·ãƒ”', 'ãƒ™ãƒ¼ã‚­ãƒ³ã‚°', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ãŠé…’', 'é£Ÿãƒ¬ãƒ']
            },
            'tech': {
                'name': 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼',
                'subcategories': ['ã‚¬ã‚¸ã‚§ãƒƒãƒˆ', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'AI', 'ã‚¹ãƒãƒ›']
            },
            'lifestyle': {
                'name': 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«',
                'subcategories': ['VLOG', 'æ—¥å¸¸', 'ãƒŸãƒ‹ãƒãƒªã‚¹ãƒˆ', 'ã‚¤ãƒ³ãƒ†ãƒªã‚¢', 'ãƒšãƒƒãƒˆ']
            },
            'education': {
                'name': 'æ•™è‚²ãƒ»å­¦ç¿’',
                'subcategories': ['èªå­¦', 'å‹‰å¼·æ³•', 'è³‡æ ¼', 'æ­´å²', 'ç§‘å­¦']
            },
            'fitness': {
                'name': 'ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ãƒ»å¥åº·',
                'subcategories': ['ç­‹ãƒˆãƒ¬', 'ãƒ¨ã‚¬', 'ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ', 'å¥åº·é£Ÿå“', 'ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°']
            },
            'fashion': {
                'name': 'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³',
                'subcategories': ['ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆ', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 'ãƒ—ãƒãƒ—ãƒ©', 'ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼', 'å¤ç€']
            },
            'travel': {
                'name': 'æ—…è¡Œ',
                'subcategories': ['æµ·å¤–æ—…è¡Œ', 'å›½å†…æ—…è¡Œ', 'ã‚°ãƒ«ãƒ¡æ—…', 'ä¸€äººæ—…', 'ãƒãƒƒã‚¯ãƒ‘ãƒƒã‚¯']
            },
            'business': {
                'name': 'ãƒ“ã‚¸ãƒã‚¹ãƒ»èµ·æ¥­',
                'subcategories': ['èµ·æ¥­', 'æŠ•è³‡', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'å‰¯æ¥­', 'ã‚­ãƒ£ãƒªã‚¢']
            }
        }
    
    async def analyze_channel_category(
        self,
        channel_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ã‚«ãƒ†ã‚´ãƒªåˆ†æ
        
        Args:
            channel_data: ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±
            
        Returns:
            Dict: ã‚«ãƒ†ã‚´ãƒªåˆ†æçµæœ
        """
        try:
            # åˆ†æç”¨ãƒ†ã‚­ã‚¹ãƒˆã®æº–å‚™
            analysis_text = self._prepare_analysis_text(channel_data)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = self._create_category_prompt()
            
            # AIåˆ†æå®Ÿè¡Œ
            result = await self.gemini.analyze_text(prompt, analysis_text)
            
            if not result:
                return self._fallback_category_analysis(channel_data)
            
            # çµæœãƒ‘ãƒ¼ã‚¹
            analysis = self._parse_category_result(result)
            
            # ä¿¡é ¼åº¦è¨ˆç®—
            confidence = self._calculate_confidence(analysis, channel_data)
            analysis['confidence'] = confidence
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Category analysis failed: {e}")
            return self._fallback_category_analysis(channel_data)
    
    def _prepare_analysis_text(self, channel_data: Dict[str, Any]) -> str:
        """åˆ†æç”¨ãƒ†ã‚­ã‚¹ãƒˆã®æº–å‚™"""
        texts = []
        
        # ãƒãƒ£ãƒ³ãƒãƒ«å
        if channel_data.get('channel_name'):
            texts.append(f"ãƒãƒ£ãƒ³ãƒãƒ«å: {channel_data['channel_name']}")
        
        # èª¬æ˜æ–‡
        if channel_data.get('description'):
            # é•·ã™ãã‚‹å ´åˆã¯å…ˆé ­éƒ¨åˆ†ã®ã¿
            desc = channel_data['description'][:1000]
            texts.append(f"èª¬æ˜æ–‡: {desc}")
        
        # æœ€æ–°å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«
        if channel_data.get('recent_videos'):
            video_titles = [v.get('title', '') for v in channel_data['recent_videos'][:5]]
            if video_titles:
                texts.append(f"æœ€æ–°å‹•ç”»: {', '.join(video_titles)}")
        
        # ã‚«ã‚¹ã‚¿ãƒ URL
        if channel_data.get('custom_url'):
            texts.append(f"URL: {channel_data['custom_url']}")
        
        return '\n'.join(texts)
    
    def _create_category_prompt(self) -> str:
        """ã‚«ãƒ†ã‚´ãƒªåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        categories_list = '\n'.join([
            f"- {key}: {value['name']} ({', '.join(value['subcategories'])})"
            for key, value in self.main_categories.items()
        ])
        
        return f"""
ã‚ãªãŸã¯ YouTube ãƒãƒ£ãƒ³ãƒãƒ«ã®å°‚é–€åˆ†æè€…ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±ã‹ã‚‰ã€æœ€ã‚‚é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªã¨ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒª:
{categories_list}

åˆ†ææŒ‡ç¤º:
1. ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤é¸æŠï¼ˆä¸Šè¨˜ã‹ã‚‰å¿…ãšé¸æŠï¼‰
2. ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚’1-3å€‹ç‰¹å®š
3. ã‚«ãƒ†ã‚´ãƒªã®ç†ç”±ã‚’å…·ä½“çš„ã«èª¬æ˜
4. ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©æ€§åº¦ã‚’5æ®µéšè©•ä¾¡
5. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã‚’æ¨å®š

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
    "main_category": "ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼",
    "main_category_name": "ã‚«ãƒ†ã‚´ãƒªå",
    "sub_categories": ["ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª1", "ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª2"],
    "reasoning": "åˆ¤å®šç†ç”±ã®è©³ç´°èª¬æ˜",
    "collaboration_score": 4,
    "target_audience": "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã®èª¬æ˜",
    "content_style": "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‰¹å¾´",
    "brand_collaboration_potential": "ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ©ãƒœã®å¯èƒ½æ€§"
}}
"""
    
    def _parse_category_result(self, result: str) -> Dict[str, Any]:
        """AIåˆ†æçµæœã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
        except json.JSONDecodeError:
            pass
        
        # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±æŠ½å‡º
        return self._extract_from_text(result)
    
    def _extract_from_text(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆè§£æ
        main_category = 'lifestyle'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ¤å®š
        for category, info in self.main_categories.items():
            for subcategory in info['subcategories']:
                if subcategory.lower() in text.lower():
                    main_category = category
                    break
        
        return {
            'main_category': main_category,
            'main_category_name': self.main_categories[main_category]['name'],
            'sub_categories': ['ä¸€èˆ¬'],
            'reasoning': 'ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ã‚ˆã‚‹æ¨å®š',
            'collaboration_score': 3,
            'target_audience': 'ä¸æ˜',
            'content_style': 'ä¸æ˜',
            'brand_collaboration_potential': 'è¦è©³ç´°åˆ†æ'
        }
    
    def _calculate_confidence(
        self,
        analysis: Dict[str, Any],
        channel_data: Dict[str, Any]
    ) -> float:
        """åˆ†æçµæœã®ä¿¡é ¼åº¦è¨ˆç®—"""
        confidence = 0.0
        
        # èª¬æ˜æ–‡ã®é•·ã•
        desc_length = len(channel_data.get('description', ''))
        if desc_length > 100:
            confidence += 0.3
        elif desc_length > 50:
            confidence += 0.2
        
        # å‹•ç”»æ•°
        video_count = channel_data.get('video_count', 0)
        if video_count > 50:
            confidence += 0.2
        elif video_count > 10:
            confidence += 0.1
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šã®ä¸€è²«æ€§
        if analysis.get('reasoning') and len(analysis['reasoning']) > 50:
            confidence += 0.3
        
        # ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã®æ•°
        sub_cats = analysis.get('sub_categories', [])
        if 1 <= len(sub_cats) <= 3:
            confidence += 0.2
        
        return round(min(confidence, 1.0), 2)
    
    def _fallback_category_analysis(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        return {
            'main_category': 'lifestyle',
            'main_category_name': 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«',
            'sub_categories': ['ä¸€èˆ¬'],
            'reasoning': 'è‡ªå‹•åˆ†æãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ãƒ†ã‚´ãƒªã‚’è¨­å®š',
            'collaboration_score': 3,
            'target_audience': 'è©³ç´°åˆ†æãŒå¿…è¦',
            'content_style': 'è¦æ‰‹å‹•ç¢ºèª',
            'brand_collaboration_potential': 'è¦è©³ç´°åˆ†æ',
            'confidence': 0.1
        }


class AdvancedEmailExtractor:
    """
    AIå¼·åŒ–ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    Gemini ã‚’æ´»ç”¨ã—ãŸé«˜ç²¾åº¦ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.gemini = GeminiClient()
    
    async def extract_business_emails(
        self,
        channel_data: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ãƒ“ã‚¸ãƒã‚¹ç”¨ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®é«˜ç²¾åº¦æŠ½å‡º
        
        Args:
            channel_data: ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±
            
        Returns:
            List[Dict]: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æƒ…å ±
        """
        try:
            # åˆ†æãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
            text = channel_data.get('description', '')
            if not text:
                return []
            
            # AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = self._create_email_extraction_prompt()
            
            # AIåˆ†æå®Ÿè¡Œ
            result = await self.gemini.analyze_text(prompt, text)
            
            if not result:
                return []
            
            # çµæœãƒ‘ãƒ¼ã‚¹
            emails = self._parse_email_result(result)
            
            return emails
            
        except Exception as e:
            logger.error(f"âŒ Advanced email extraction failed: {e}")
            return []
    
    def _create_email_extraction_prompt(self) -> str:
        """ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®YouTubeãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜æ–‡ã‹ã‚‰ã€ãƒ“ã‚¸ãƒã‚¹ç”¨ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºæ¡ä»¶:
1. ãƒ“ã‚¸ãƒã‚¹ãƒ»ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã¿
2. å€‹äººçš„ãªãƒ¡ãƒ¼ãƒ«ã¯é™¤å¤–
3. ä¿¡é ¼åº¦ã‚’1-10ã§è©•ä¾¡
4. ç”¨é€”ã‚’ç‰¹å®šï¼ˆãŠä»•äº‹ä¾é ¼ã€ã‚³ãƒ©ãƒœã€PRç­‰ï¼‰

å‡ºåŠ›å½¢å¼ï¼ˆJSONé…åˆ—ï¼‰:
[
    {
        "email": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
        "confidence": 8,
        "purpose": "ãŠä»•äº‹ä¾é ¼",
        "context": "å‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆ",
        "is_primary": true
    }
]

ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã®é…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
"""
    
    def _parse_email_result(self, result: str) -> List[Dict[str, Any]]:
        """AIåˆ†æçµæœã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_match = re.search(r'\[.*\]', result, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                emails = json.loads(json_str)
                
                # çµæœã®æ¤œè¨¼
                validated_emails = []
                for email_info in emails:
                    if self._validate_email_info(email_info):
                        validated_emails.append(email_info)
                
                return validated_emails
        except json.JSONDecodeError:
            pass
        
        return []
    
    def _validate_email_info(self, email_info: Dict[str, Any]) -> bool:
        """ãƒ¡ãƒ¼ãƒ«æƒ…å ±ã®æ¤œè¨¼"""
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
        if not all(key in email_info for key in ['email', 'confidence']):
            return False
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å½¢å¼ã®ç¢ºèª
        email = email_info['email']
        email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        if not email_pattern.match(email):
            return False
        
        # ä¿¡é ¼åº¦ã®ç¢ºèª
        confidence = email_info['confidence']
        if not isinstance(confidence, (int, float)) or not (1 <= confidence <= 10):
            return False
        
        return True


class TrendAnalyzer:
    """
    ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã¨å°†æ¥æ€§äºˆæ¸¬
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.gemini = GeminiClient()
    
    async def analyze_growth_trend(
        self,
        channel_data: Dict[str, Any],
        historical_data: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        
        Args:
            channel_data: ç¾åœ¨ã®ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±
            historical_data: éå»ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœ
        """
        try:
            # åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™
            analysis_data = self._prepare_trend_data(channel_data, historical_data)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = self._create_trend_prompt()
            
            # AIåˆ†æå®Ÿè¡Œ
            result = await self.gemini.analyze_text(prompt, analysis_data)
            
            if not result:
                return self._fallback_trend_analysis()
            
            # çµæœãƒ‘ãƒ¼ã‚¹
            trend_analysis = self._parse_trend_result(result)
            
            return trend_analysis
            
        except Exception as e:
            logger.error(f"âŒ Trend analysis failed: {e}")
            return self._fallback_trend_analysis()
    
    def _prepare_trend_data(
        self,
        channel_data: Dict[str, Any],
        historical_data: List[Dict[str, Any]] = None
    ) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        data_points = []
        
        # ç¾åœ¨ã®çµ±è¨ˆ
        data_points.append(f"ç™»éŒ²è€…æ•°: {channel_data.get('subscriber_count', 0):,}")
        data_points.append(f"å‹•ç”»æ•°: {channel_data.get('video_count', 0):,}")
        data_points.append(f"ç·å†ç”Ÿå›æ•°: {channel_data.get('view_count', 0):,}")
        data_points.append(f"ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {channel_data.get('engagement_rate', 0)}%")
        
        # ãƒãƒ£ãƒ³ãƒãƒ«é–‹è¨­æ—¥
        if channel_data.get('published_at'):
            data_points.append(f"é–‹è¨­æ—¥: {channel_data['published_at']}")
        
        # æœ€æ–°å‹•ç”»æƒ…å ±
        if channel_data.get('recent_videos'):
            video_info = []
            for video in channel_data['recent_videos'][:3]:
                video_info.append(f"ã€Œ{video.get('title', '')}ã€({video.get('published_at', '')})")
            data_points.append(f"æœ€æ–°å‹•ç”»: {', '.join(video_info)}")
        
        return '\n'.join(data_points)
    
    def _create_trend_prompt(self) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã®æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æˆé•·æ€§ã¨å°†æ¥æ€§ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

åˆ†æé …ç›®:
1. æˆé•·æ®µéšã®åˆ¤å®šï¼ˆå°å…¥æœŸ/æˆé•·æœŸ/æˆç†ŸæœŸ/è¡°é€€æœŸï¼‰
2. æˆé•·ç‡ã®æ¨å®š
3. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒˆãƒ¬ãƒ³ãƒ‰ã¸ã®é©å¿œåº¦
4. ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€é©ã‚¿ã‚¤ãƒŸãƒ³ã‚°
5. ä»Šå¾Œ6ãƒ¶æœˆã®æˆé•·äºˆæ¸¬

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{
    "growth_stage": "æˆé•·æœŸ",
    "growth_rate": "é«˜æˆé•·",
    "trend_adaptation": 8,
    "collaboration_timing": "æœ€é©",
    "future_prediction": {
        "6_months": {
            "subscriber_growth": "+25%",
            "engagement_trend": "ä¸Šæ˜‡",
            "risk_factors": ["ãƒªã‚¹ã‚¯è¦å› "]
        }
    },
    "recommendations": ["æ¨å¥¨äº‹é …1", "æ¨å¥¨äº‹é …2"]
}
"""
    
    def _parse_trend_result(self, result: str) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æçµæœã®ãƒ‘ãƒ¼ã‚¹"""
        try:
            json_match = re.search(r'\{.*\}', result, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
        except json.JSONDecodeError:
            pass
        
        return self._fallback_trend_analysis()
    
    def _fallback_trend_analysis(self) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        return {
            'growth_stage': 'ä¸æ˜',
            'growth_rate': 'è¦åˆ†æ',
            'trend_adaptation': 5,
            'collaboration_timing': 'è¦æ¤œè¨',
            'future_prediction': {
                '6_months': {
                    'subscriber_growth': 'ä¸æ˜',
                    'engagement_trend': 'è¦è¦³å¯Ÿ',
                    'risk_factors': ['ãƒ‡ãƒ¼ã‚¿ä¸è¶³']
                }
            },
            'recommendations': ['è©³ç´°åˆ†æãŒå¿…è¦']
        }


# çµ±åˆAIåˆ†æã‚µãƒ¼ãƒ“ã‚¹
class IntegratedAIAnalyzer:
    """
    çµ±åˆAIåˆ†æã‚µãƒ¼ãƒ“ã‚¹
    
    ã™ã¹ã¦ã®AIåˆ†ææ©Ÿèƒ½ã‚’çµ±åˆã—ã¦å®Ÿè¡Œ
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.category_analyzer = CategoryAnalyzer()
        self.email_extractor = AdvancedEmailExtractor()
        self.trend_analyzer = TrendAnalyzer()
    
    async def comprehensive_analysis(
        self,
        channel_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«ã®åŒ…æ‹¬çš„AIåˆ†æ
        
        Args:
            channel_data: ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±
            
        Returns:
            Dict: åŒ…æ‹¬çš„åˆ†æçµæœ
        """
        logger.info(f"ğŸ¤– Starting comprehensive AI analysis for {channel_data.get('channel_name', 'Unknown')}")
        
        try:
            # ä¸¦è¡Œã—ã¦å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
            tasks = [
                self.category_analyzer.analyze_channel_category(channel_data),
                self.email_extractor.extract_business_emails(channel_data),
                self.trend_analyzer.analyze_growth_trend(channel_data)
            ]
            
            category_result, email_result, trend_result = await asyncio.gather(*tasks)
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            overall_score = self._calculate_overall_score(
                category_result,
                email_result,
                trend_result,
                channel_data
            )
            
            return {
                'channel_id': channel_data.get('channel_id'),
                'analysis_timestamp': datetime.utcnow().isoformat(),
                'category_analysis': category_result,
                'email_analysis': email_result,
                'trend_analysis': trend_result,
                'overall_score': overall_score,
                'recommendation': self._generate_recommendation(
                    category_result, email_result, trend_result, overall_score
                )
            }
            
        except Exception as e:
            logger.error(f"âŒ Comprehensive analysis failed: {e}")
            return {
                'error': str(e),
                'analysis_timestamp': datetime.utcnow().isoformat()
            }
    
    def _calculate_overall_score(
        self,
        category_result: Dict[str, Any],
        email_result: List[Dict[str, Any]],
        trend_result: Dict[str, Any],
        channel_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        scores = {}
        
        # ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢
        collaboration_score = category_result.get('collaboration_score', 3)
        confidence = category_result.get('confidence', 0.5)
        scores['category_score'] = collaboration_score * confidence
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¹ã‚³ã‚¢
        if email_result:
            avg_confidence = sum(e.get('confidence', 5) for e in email_result) / len(email_result)
            scores['contactability_score'] = min(avg_confidence, 10)
        else:
            scores['contactability_score'] = 0
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢
        trend_adaptation = trend_result.get('trend_adaptation', 5)
        scores['trend_score'] = trend_adaptation
        
        # ãƒãƒ£ãƒ³ãƒãƒ«è¦æ¨¡ã‚¹ã‚³ã‚¢
        subscriber_count = channel_data.get('subscriber_count', 0)
        if subscriber_count >= 100000:
            scores['scale_score'] = 10
        elif subscriber_count >= 10000:
            scores['scale_score'] = 8
        elif subscriber_count >= 1000:
            scores['scale_score'] = 6
        else:
            scores['scale_score'] = 3
        
        # ç·åˆã‚¹ã‚³ã‚¢
        overall = (
            scores['category_score'] * 0.3 +
            scores['contactability_score'] * 0.3 +
            scores['trend_score'] * 0.2 +
            scores['scale_score'] * 0.2
        )
        
        scores['overall'] = round(overall, 2)
        scores['grade'] = self._score_to_grade(overall)
        
        return scores
    
    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’ã‚°ãƒ¬ãƒ¼ãƒ‰ã«å¤‰æ›"""
        if score >= 8.5:
            return 'S'
        elif score >= 7.5:
            return 'A'
        elif score >= 6.5:
            return 'B'
        elif score >= 5.5:
            return 'C'
        else:
            return 'D'
    
    def _generate_recommendation(
        self,
        category_result: Dict[str, Any],
        email_result: List[Dict[str, Any]],
        trend_result: Dict[str, Any],
        overall_score: Dict[str, Any]
    ) -> str:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        grade = overall_score.get('grade', 'D')
        
        if grade in ['S', 'A']:
            recommendations.append("é«˜ã„å”æ¥­ä¾¡å€¤ã‚’æŒã¤å„ªè‰¯ãƒãƒ£ãƒ³ãƒãƒ«ã§ã™")
        elif grade == 'B':
            recommendations.append("å”æ¥­ä¾¡å€¤ãŒã‚ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã§ã™")
        else:
            recommendations.append("æ…é‡ãªæ¤œè¨ãŒå¿…è¦ãªãƒãƒ£ãƒ³ãƒãƒ«ã§ã™")
        
        # ãƒ¡ãƒ¼ãƒ«é€£çµ¡å¯èƒ½æ€§
        if email_result:
            recommendations.append("ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã§ã®é€£çµ¡ãŒå¯èƒ½ã§ã™")
        else:
            recommendations.append("ç›´æ¥é€£çµ¡æ‰‹æ®µã®ç¢ºèªãŒå¿…è¦ã§ã™")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ³
        growth_stage = trend_result.get('growth_stage', '')
        if growth_stage == 'æˆé•·æœŸ':
            recommendations.append("æˆé•·æœŸã«ã‚ã‚Šã€å”æ¥­ã«æœ€é©ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã™")
        
        return ' / '.join(recommendations)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def test_analysis():
        analyzer = IntegratedAIAnalyzer()
        
        test_channel = {
            'channel_id': 'test123',
            'channel_name': 'ãƒ†ã‚¹ãƒˆãƒãƒ£ãƒ³ãƒãƒ«',
            'description': 'ãƒ¡ã‚¤ã‚¯ã‚¢ãƒƒãƒ—ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’æŠ•ç¨¿ã—ã¦ã„ã¾ã™ã€‚ãŠä»•äº‹ã®ã”ä¾é ¼ã¯ business@example.com ã¾ã§ã€‚',
            'subscriber_count': 50000,
            'video_count': 100,
            'view_count': 5000000,
            'engagement_rate': 3.5
        }
        
        result = await analyzer.comprehensive_analysis(test_channel)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    asyncio.run(test_analysis())