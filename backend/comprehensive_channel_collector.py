#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„YouTubeãƒãƒ£ãƒ³ãƒãƒ«åé›†ãƒ»ç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ 

@description AIåˆ†æä»˜ãã§YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã‚’åé›†ã—ã€Firestoreãƒ»BigQueryã«è‡ªå‹•ç™»éŒ²
@author InfuMatch Development Team
@version 3.0.0
"""

import os
import json
import asyncio
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.cloud import firestore
from google.cloud import bigquery

# AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
YOUTUBE_API_KEY = "AIzaSyDtPl5WSRdxk744ha5Tlwno4iTBZVO96r4"
PROJECT_ID = "hackathon-462905"

class ComprehensiveChannelCollector:
    """
    åŒ…æ‹¬çš„YouTubeãƒãƒ£ãƒ³ãƒãƒ«åé›†ãƒ»ç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    1. YouTube APIçµŒç”±ã§ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢ãƒ»å–å¾—
    2. AIåˆ†æã«ã‚ˆã‚‹é«˜åº¦ãªåˆ†æ
    3. Firestoreè‡ªå‹•ç™»éŒ²
    4. BigQueryè‡ªå‹•åŒæœŸ
    5. é€²æ—ç®¡ç†ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    """
    
    def __init__(self, api_key: str = YOUTUBE_API_KEY):
        self.api_key = api_key
        self.service = build('youtube', 'v3', developerKey=api_key)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.firestore_db = firestore.Client(project=PROJECT_ID)
        self.bigquery_client = bigquery.Client(project=PROJECT_ID)
        
        # åé›†ãƒ‡ãƒ¼ã‚¿
        self.collected_channels = []
        self.stats = {
            'searched': 0,
            'filtered': 0,
            'analyzed': 0,
            'saved_firestore': 0,
            'saved_bigquery': 0,
            'errors': 0
        }
    
    def extract_emails_from_description(self, description: str) -> List[str]:
        """æ¦‚è¦æ¬„ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º"""
        if not description:
            return []
        
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, description)
        return list(set(emails))  # é‡è¤‡é™¤å»
    
    async def search_channels_by_category(self, category: str, search_queries: List[str], target_count: int = 35) -> List[Dict[str, Any]]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢"""
        print(f"\nğŸ” {category}ç³»ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢é–‹å§‹")
        print(f"ğŸ“‹ æ¤œç´¢ã‚¯ã‚¨ãƒª: {len(search_queries)} ä»¶")
        print(f"ğŸ¯ ç›®æ¨™åé›†æ•°: {target_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        
        all_channels = []
        seen_channel_ids = set()
        
        for i, query in enumerate(search_queries, 1):
            try:
                print(f"  {i:2d}. '{query}' æ¤œç´¢ä¸­...")
                
                search_request = self.service.search().list(
                    part='snippet',
                    q=query,
                    type='channel',
                    regionCode='JP',
                    relevanceLanguage='ja',
                    order='relevance',
                    maxResults=min(50, target_count // len(search_queries) + 10)
                )
                
                search_response = search_request.execute()
                
                for item in search_response.get('items', []):
                    channel_id = item['id']['channelId']
                    if channel_id not in seen_channel_ids:
                        seen_channel_ids.add(channel_id)
                        all_channels.append({
                            'channel_id': channel_id,
                            'channel_title': item['snippet']['title'],
                            'description': item['snippet']['description'],
                            'query_source': query
                        })
                
                print(f"     âœ… {len(search_response.get('items', []))} ä»¶ç™ºè¦‹")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(0.5)
                
            except HttpError as e:
                print(f"     âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['searched'] = len(all_channels)
        print(f"\nğŸ“Š æ¤œç´¢çµæœ: {len(all_channels)} ãƒãƒ£ãƒ³ãƒãƒ«ç™ºè¦‹")
        
        return all_channels[:target_count]
    
    async def get_channel_details_with_ai(self, channels: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ãƒãƒ«è©³ç´°å–å¾— + AIåˆ†æ"""
        print(f"\nğŸ¤– {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°å–å¾— + AIåˆ†æä¸­...")
        
        enhanced_channels = []
        
        # ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’æŠ½å‡º
        channel_ids = [ch['channel_id'] for ch in channels]
        
        try:
            # ãƒãƒƒãƒã§è©³ç´°å–å¾—
            details_request = self.service.channels().list(
                part='snippet,statistics',
                id=','.join(channel_ids[:50])  # APIåˆ¶é™å¯¾å¿œ
            )
            details_response = details_request.execute()
            
            for item in details_response.get('items', []):
                try:
                    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                    channel_id = item['id']
                    snippet = item['snippet']
                    statistics = item['statistics']
                    
                    subscriber_count = int(statistics.get('subscriberCount', 0))
                    video_count = int(statistics.get('videoCount', 0))
                    view_count = int(statistics.get('viewCount', 0))
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼: 10K-500Kï¼‰
                    if not (10000 <= subscriber_count <= 500000):
                        continue
                    
                    # ãƒ¡ãƒ¼ãƒ«æŠ½å‡º
                    description = snippet.get('description', '')
                    emails = self.extract_emails_from_description(description)
                    
                    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨å®š
                    engagement_estimate = (subscriber_count / video_count * 100) if video_count > 0 else 0
                    
                    # åŸºæœ¬ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
                    channel_data = {
                        'channel_id': channel_id,
                        'channel_title': snippet.get('title', ''),
                        'description': description,
                        'subscriber_count': subscriber_count,
                        'video_count': video_count,
                        'view_count': view_count,
                        'country': snippet.get('country', 'JP'),
                        'emails': emails,
                        'has_contact': len(emails) > 0,
                        'engagement_estimate': round(engagement_estimate, 2),
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    }
                    
                    # ğŸ¤– AIåˆ†æå®Ÿè¡Œ
                    print(f"ğŸ¤– AIåˆ†æä¸­: {channel_data['channel_title']}")
                    ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
                    
                    # AIåˆ†æçµæœã‚’çµ±åˆ
                    enhanced_channel = {
                        **channel_data,
                        'ai_analysis': ai_analysis,
                        'category': ai_analysis.get('category_tags', {}).get('primary_category', 'æœªåˆ†é¡'),
                        'sub_categories': ai_analysis.get('category_tags', {}).get('sub_categories', []),
                        'content_themes': ai_analysis.get('category_tags', {}).get('content_themes', []),
                        'recommended_products': ai_analysis.get('product_matching', {}).get('recommended_products', []),
                        'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                        'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5)
                    }
                    
                    enhanced_channels.append(enhanced_channel)
                    self.stats['analyzed'] += 1
                    
                    # çµæœè¡¨ç¤º
                    print(f"âœ… å®Œäº†: {channel_data['channel_title']}")
                    print(f"   ğŸ“Š ç™»éŒ²è€…: {subscriber_count:,}")
                    print(f"   ğŸ¯ ã‚«ãƒ†ã‚´ãƒª: {enhanced_channel['category']}")
                    print(f"   ğŸ›¡ï¸ å®‰å…¨æ€§: {enhanced_channel['brand_safety_score']:.2f}")
                    print(f"   ğŸ“ˆ ä¿¡é ¼åº¦: {enhanced_channel['analysis_confidence']:.2f}")
                    if enhanced_channel['recommended_products']:
                        top_product = enhanced_channel['recommended_products'][0]
                        print(f"   ğŸ’¼ æ¨å¥¨å•†æ: {top_product.get('category', 'N/A')}")
                    print()
                    
                except Exception as e:
                    print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({item.get('id', 'Unknown')}): {e}")
                    self.stats['errors'] += 1
                    continue
            
            self.stats['filtered'] = len(enhanced_channels)
            return enhanced_channels
            
        except HttpError as e:
            print(f"âŒ YouTube API ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['errors'] += 1
            return []
        except Exception as e:
            print(f"âŒ è©³ç´°å–å¾—å¤±æ•—: {e}")
            self.stats['errors'] += 1
            return []
    
    def convert_to_firestore_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """Firestoreå½¢å¼ã«å¤‰æ›"""
        ai_analysis = channel_data.get('ai_analysis', {})
        brand_safety = ai_analysis.get('brand_safety', {})
        category_tags = ai_analysis.get('category_tags', {})
        product_matching = ai_analysis.get('product_matching', {})
        
        return {
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'],
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_info': {
                'emails': channel_data.get('emails', []),
                'primary_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None
            },
            'engagement_metrics': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'avg_views_per_video': channel_data['view_count'] / channel_data['video_count'] if channel_data['video_count'] > 0 else 0,
                'has_contact': channel_data.get('has_contact', False)
            },
            'ai_analysis': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'content_quality_score': 0.8,
                'brand_safety_score': brand_safety.get('overall_safety_score', 0.8),
                'growth_potential': 0.7,
                'full_analysis': ai_analysis,
                'advanced': {
                    'enhanced_at': datetime.now(timezone.utc).isoformat(),
                    'category': category_tags.get('primary_category', 'æœªåˆ†é¡'),
                    'sub_categories': category_tags.get('sub_categories', []),
                    'content_themes': category_tags.get('content_themes', []),
                    'safety_score': brand_safety.get('overall_safety_score', 0.8),
                    'confidence': ai_analysis.get('analysis_confidence', 0.5),
                    'target_age': category_tags.get('target_age_group', 'ä¸æ˜'),
                    'top_product': product_matching.get('recommended_products', [{}])[0].get('category', 'æœªå®š') if product_matching.get('recommended_products') else 'æœªå®š',
                    'match_score': product_matching.get('recommended_products', [{}])[0].get('match_score', 0.5) if product_matching.get('recommended_products') else 0.5
                }
            },
            'status': 'active',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'last_analyzed': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'fetched_at': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'data_source': 'youtube_api',
            'collection_method': 'comprehensive_ai_enhanced'
        }
    
    async def save_to_firestore(self, channels: List[Dict[str, Any]]) -> bool:
        """Firestoreã«ä¿å­˜"""
        print(f"\nğŸ”¥ Firestoreã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        success_count = 0
        
        for i, channel_data in enumerate(channels, 1):
            try:
                # Firestoreå½¢å¼ã«å¤‰æ›
                firestore_doc = self.convert_to_firestore_format(channel_data)
                
                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                doc_ref = self.firestore_db.collection('influencers').document(firestore_doc['channel_id'])
                existing_doc = doc_ref.get()
                
                if existing_doc.exists:
                    print(f"âš ï¸  {i:2d}. {firestore_doc['channel_title']} (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—)")
                    continue
                
                # Firestoreã«ä¿å­˜
                doc_ref.set(firestore_doc)
                
                print(f"âœ… {i:2d}. {firestore_doc['channel_title']} (ç™»éŒ²è€…: {firestore_doc['subscriber_count']:,})")
                success_count += 1
                
            except Exception as e:
                print(f"âŒ {i:2d}. Firestoreä¿å­˜å¤±æ•— ({channel_data.get('channel_title', 'Unknown')}): {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['saved_firestore'] = success_count
        print(f"\nğŸ“Š Firestoreä¿å­˜çµæœ: {success_count}/{len(channels)} ä»¶æˆåŠŸ")
        
        return success_count > 0
    
    def convert_to_bigquery_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """BigQueryå½¢å¼ã«å¤‰æ›"""
        ai_analysis = channel_data.get('ai_analysis', {})
        
        return {
            'influencer_id': channel_data['channel_id'],
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'][:1000],  # BigQueryæ–‡å­—åˆ—é•·åˆ¶é™
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None,
            'social_links': json.dumps({'emails': channel_data.get('emails', [])}),
            'ai_analysis_json': json.dumps(ai_analysis),
            'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
            'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5),
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'is_active': True
        }
    
    async def save_to_bigquery(self, channels: List[Dict[str, Any]]) -> bool:
        """BigQueryã«ä¿å­˜"""
        print(f"\nğŸ—ï¸ BigQueryã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§ã‚’å–å¾—
            dataset_id = "infumatch_data"  # å®Ÿéš›ã«ä½¿ç”¨ä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            table_id = "influencers"
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ç¢ºèªãƒ»ä½œæˆ
            table_ref = self.bigquery_client.dataset(dataset_id).table(table_id)
            
            try:
                table = self.bigquery_client.get_table(table_ref)
                print(f"âœ… BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª: {dataset_id}.{table_id}")
            except Exception:
                print(f"âš ï¸ BigQueryãƒ†ãƒ¼ãƒ–ãƒ« {dataset_id}.{table_id} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            rows_to_insert = []
            for channel_data in channels:
                bq_row = self.convert_to_bigquery_format(channel_data)
                rows_to_insert.append(bq_row)
            
            # BigQueryã«æŒ¿å…¥
            errors = self.bigquery_client.insert_rows_json(table, rows_to_insert)
            
            if not errors:
                self.stats['saved_bigquery'] = len(rows_to_insert)
                print(f"âœ… BigQueryä¿å­˜æˆåŠŸ: {len(rows_to_insert)} ä»¶")
                return True
            else:
                print(f"âŒ BigQueryä¿å­˜ã‚¨ãƒ©ãƒ¼: {errors}")
                self.stats['errors'] += 1
                return False
                
        except Exception as e:
            print(f"âŒ BigQueryä¿å­˜å¤±æ•—: {e}")
            self.stats['errors'] += 1
            return False
    
    def save_to_json(self, channels: List[Dict[str, Any]], filename: str):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(channels, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âŒ JSONä¿å­˜å¤±æ•—: {e}")
    
    async def collect_category_channels(self, category: str, search_queries: List[str], target_count: int = 35) -> List[Dict[str, Any]]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒãƒ£ãƒ³ãƒãƒ«åé›†"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ {category}ç³»ãƒãƒ£ãƒ³ãƒãƒ«åé›†é–‹å§‹")
        print(f"{'='*80}")
        
        # 1. ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
        channels = await self.search_channels_by_category(category, search_queries, target_count)
        
        if not channels:
            print(f"âŒ {category}ç³»ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return []
        
        # 2. è©³ç´°å–å¾— + AIåˆ†æ
        enhanced_channels = await self.get_channel_details_with_ai(channels)
        
        if not enhanced_channels:
            print(f"âŒ {category}ç³»ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°å–å¾—ã«å¤±æ•—")
            return []
        
        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        await self.save_to_firestore(enhanced_channels)
        await self.save_to_bigquery(enhanced_channels)
        
        # 4. JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{category.lower()}_channels_{timestamp}.json"
        self.save_to_json(enhanced_channels, filename)
        
        self.collected_channels.extend(enhanced_channels)
        
        return enhanced_channels
    
    def print_results_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ åé›†çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")
        
        print(f"ğŸ“Š çµ±è¨ˆæƒ…å ±:")
        print(f"  - æ¤œç´¢ç™ºè¦‹: {self.stats['searched']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {self.stats['filtered']} ãƒãƒ£ãƒ³ãƒãƒ«") 
        print(f"  - AIåˆ†æå®Œäº†: {self.stats['analyzed']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - Firestoreä¿å­˜: {self.stats['saved_firestore']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - BigQueryä¿å­˜: {self.stats['saved_bigquery']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors']}")
        
        if self.collected_channels:
            # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
            categories = {}
            total_subscribers = 0
            avg_safety = 0
            avg_confidence = 0
            
            for channel in self.collected_channels:
                cat = channel.get('category', 'æœªåˆ†é¡')
                categories[cat] = categories.get(cat, 0) + 1
                total_subscribers += channel.get('subscriber_count', 0)
                avg_safety += channel.get('brand_safety_score', 0)
                avg_confidence += channel.get('analysis_confidence', 0)
            
            total_count = len(self.collected_channels)
            avg_safety = avg_safety / total_count if total_count > 0 else 0
            avg_confidence = avg_confidence / total_count if total_count > 0 else 0
            
            print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {category}: {count} ãƒãƒ£ãƒ³ãƒãƒ«")
            
            print(f"\nğŸ“ˆ å“è³ªæŒ‡æ¨™:")
            print(f"  - ç·ç™»éŒ²è€…æ•°: {total_subscribers:,} äºº")
            print(f"  - å¹³å‡å®‰å…¨æ€§ã‚¹ã‚³ã‚¢: {avg_safety:.2f}")
            print(f"  - å¹³å‡AIä¿¡é ¼åº¦: {avg_confidence:.2f}")

# ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¤œç´¢ã‚¯ã‚¨ãƒªå®šç¾©
SEARCH_QUERIES = {
    'ã‚²ãƒ¼ãƒ ': [
        "ã‚²ãƒ¼ãƒ å®Ÿæ³", "å®Ÿæ³ãƒ—ãƒ¬ã‚¤", "ã‚²ãƒ¼ãƒ é…ä¿¡", "ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆ å®Ÿæ³",
        "ãƒ•ã‚©ãƒ¼ãƒˆãƒŠã‚¤ãƒˆ å®Ÿæ³", "ã‚¨ãƒ¼ãƒšãƒƒã‚¯ã‚¹ å®Ÿæ³", "ã‚²ãƒ¼ãƒ æ”»ç•¥",
        "gaming japan", "æ—¥æœ¬ ã‚²ãƒ¼ãƒ å®Ÿæ³", "ã‚²ãƒ¼ãƒ å®Ÿæ³è€…"
    ],
    'ãƒ“ã‚¸ãƒã‚¹': [
        "ãƒ“ã‚¸ãƒã‚¹ èµ·æ¥­", "çµŒå–¶ ã‚³ãƒ³ã‚µãƒ«", "æŠ•è³‡ æ ªå¼", "å‰¯æ¥­ ç¨¼ã",
        "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚° æˆ¦ç•¥", "çµŒæ¸ˆ è§£èª¬", "ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ ç‹¬ç«‹",
        "business japan", "è»¢è· ã‚­ãƒ£ãƒªã‚¢", "è³‡ç”£é‹ç”¨ æŠ•è³‡"
    ],
    'æ–™ç†': [
        "æ–™ç† ãƒ¬ã‚·ãƒ”", "ã‚¯ãƒƒã‚­ãƒ³ã‚° ç°¡å˜", "ã‚°ãƒ«ãƒ¡ é£Ÿã¹ç‰©", "ãŠå¼å½“ ä½œã‚Šæ–¹",
        "ãŠè“å­ä½œã‚Š ã‚¹ã‚¤ãƒ¼ãƒ„", "å’Œé£Ÿ æ—¥æœ¬æ–™ç†", "å®¶åº­æ–™ç† æ™‚çŸ­",
        "cooking japan", "ãƒ™ãƒ¼ã‚­ãƒ³ã‚° ãƒ‘ãƒ³", "é£Ÿæ ç¯€ç´„"
    ]
}

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    collector = ComprehensiveChannelCollector()
    
    print("ğŸ¤– åŒ…æ‹¬çš„YouTubeãƒãƒ£ãƒ³ãƒãƒ«åé›†ãƒ»ç™»éŒ²ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print("æ©Ÿèƒ½: AIåˆ†æ + Firestore + BigQueryè‡ªå‹•ç™»éŒ²")
    print()
    
    try:
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’åé›†
        for category, queries in SEARCH_QUERIES.items():
            await collector.collect_category_channels(category, queries, target_count=35)
            
            # ã‚«ãƒ†ã‚´ãƒªé–“ã®ä¼‘æ†©
            print(f"\nâ±ï¸ æ¬¡ã®ã‚«ãƒ†ã‚´ãƒªã¾ã§5ç§’ä¼‘æ†©...")
            await asyncio.sleep(5)
        
        # æœ€çµ‚çµæœè¡¨ç¤º
        collector.print_results_summary()
        
        print(f"\nğŸ‰ ã™ã¹ã¦ã®åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ åé›†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        collector.print_results_summary()

if __name__ == "__main__":
    asyncio.run(main())