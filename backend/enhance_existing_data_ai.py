#!/usr/bin/env python3
"""
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†æå¼·åŒ–

@description æ—¢å­˜ã®BigQuery/Firestoreãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„AIåˆ†æçµæœã‚’è¿½åŠ 
æ—¢å­˜ã®ai_analysisåˆ—ã‚’æ–°ã—ã„é«˜åº¦ãªAIåˆ†æã§ç½®ãæ›ãˆ

@author InfuMatch Development Team
@version 1.0.0
"""

import asyncio
import json
from datetime import datetime
from google.cloud import firestore, bigquery
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
PROJECT_ID = "hackathon-462905"

class ExistingDataAIEnhancer:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†æå¼·åŒ–å™¨"""
    
    def __init__(self):
        self.project_id = PROJECT_ID
        self.bigquery_client = bigquery.Client(project=self.project_id)
        self.firestore_client = firestore.Client(project=self.project_id)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        self.enhanced_count = 0
        self.failed_count = 0
        
    def fetch_existing_channels(self, limit=5):
        """æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            print(f"ğŸ“Š BigQueryã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ï¼ˆæœ€å¤§{limit}ä»¶ï¼‰...")
            
            query = f"""
            SELECT 
                influencer_id,
                channel_id,
                channel_title,
                description,
                subscriber_count,
                video_count,
                view_count,
                category,
                country,
                language,
                contact_email,
                social_links,
                ai_analysis,
                created_at,
                updated_at,
                is_active
            FROM `hackathon-462905.infumatch_data.influencers`
            WHERE is_active = true
            ORDER BY subscriber_count DESC
            LIMIT {limit}
            """
            
            query_job = self.bigquery_client.query(query)
            results = query_job.result()
            
            channels = []
            for row in results:
                # social_linksã‹ã‚‰emailsã‚’å–å¾—
                social_links = {}
                try:
                    if row.social_links:
                        social_links = json.loads(row.social_links)
                except:
                    social_links = {}
                
                emails = social_links.get('emails', [])
                if row.contact_email:
                    emails.append(row.contact_email)
                
                channel_data = {
                    'influencer_id': row.influencer_id,
                    'channel_id': row.channel_id,
                    'channel_title': row.channel_title,
                    'description': row.description or '',
                    'subscriber_count': int(row.subscriber_count),
                    'video_count': int(row.video_count),
                    'view_count': int(row.view_count),
                    'category': row.category,
                    'country': row.country or 'JP',
                    'language': row.language or 'ja',
                    'contact_email': row.contact_email or '',
                    'emails': emails,
                    'has_contact': len(emails) > 0 or bool(row.contact_email),
                    'social_links': social_links,
                    'old_ai_analysis': row.ai_analysis,
                    'created_at': row.created_at,
                    'updated_at': row.updated_at,
                    'is_active': row.is_active,
                    'engagement_estimate': 0.0  # è¨ˆç®—ã™ã‚‹
                }
                
                # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨å®šå€¤è¨ˆç®—
                if channel_data['video_count'] > 0 and channel_data['subscriber_count'] > 0:
                    channel_data['engagement_estimate'] = round(
                        (channel_data['view_count'] / channel_data['video_count'] / channel_data['subscriber_count']) * 100, 2
                    )
                
                channels.append(channel_data)
            
            print(f"âœ… {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return channels
            
        except Exception as e:
            print(f"âŒ BigQueryãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def enhance_channel_with_advanced_ai(self, channel_data):
        """1ã¤ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«é«˜åº¦ãªAIåˆ†æã‚’è¿½åŠ """
        try:
            print(f"ğŸ¤– é«˜åº¦AIåˆ†æä¸­: {channel_data['channel_title']}")
            
            # æ–°ã—ã„AIåˆ†æå®Ÿè¡Œ
            advanced_ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°AIåˆ†æã‚’çµ±åˆ
            enhanced_data = {
                **channel_data,
                'advanced_ai_analysis': advanced_ai_analysis,
                'ai_category': advanced_ai_analysis.get('category_tags', {}).get('primary_category', channel_data['category']),
                'ai_sub_categories': advanced_ai_analysis.get('category_tags', {}).get('sub_categories', []),
                'ai_content_themes': advanced_ai_analysis.get('category_tags', {}).get('content_themes', []),
                'ai_target_age': advanced_ai_analysis.get('category_tags', {}).get('target_age_group', 'ä¸æ˜'),
                'ai_confidence_score': advanced_ai_analysis.get('category_tags', {}).get('confidence_score', 0.5),
                'ai_brand_safety_score': advanced_ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                'ai_analysis_confidence': advanced_ai_analysis.get('analysis_confidence', 0.5),
                'ai_enhanced_at': datetime.now().isoformat()
            }
            
            # å•†æãƒãƒƒãƒãƒ³ã‚°æƒ…å ±
            product_matching = advanced_ai_analysis.get('product_matching', {})
            recommended_products = product_matching.get('recommended_products', [])
            
            if recommended_products:
                enhanced_data['ai_top_product_category'] = recommended_products[0].get('category', 'ä¸æ˜')
                enhanced_data['ai_top_product_match_score'] = recommended_products[0].get('match_score', 0.0)
                enhanced_data['ai_collaboration_formats'] = product_matching.get('collaboration_formats', [])
            else:
                enhanced_data['ai_top_product_category'] = 'ä¸æ˜'
                enhanced_data['ai_top_product_match_score'] = 0.0
                enhanced_data['ai_collaboration_formats'] = []
            
            # ãƒãƒ£ãƒ³ãƒãƒ«æ¦‚è¦æƒ…å ±
            channel_summary = advanced_ai_analysis.get('channel_summary', {})
            enhanced_data['ai_content_style'] = channel_summary.get('content_style', 'ä¸æ˜')
            enhanced_data['ai_expertise_level'] = channel_summary.get('expertise_level', 'ä¸­')
            enhanced_data['ai_entertainment_value'] = channel_summary.get('entertainment_value', 'ä¸­')
            enhanced_data['ai_educational_value'] = channel_summary.get('educational_value', 'ä¸­')
            
            # ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹æƒ…å ±
            audience_profile = advanced_ai_analysis.get('audience_profile', {})
            enhanced_data['ai_audience_size'] = audience_profile.get('audience_size', 'ä¸æ˜')
            enhanced_data['ai_engagement_level'] = audience_profile.get('engagement_level', 'ä¸­')
            enhanced_data['ai_reach_potential'] = audience_profile.get('reach_potential', 'ä¸­ç¨‹åº¦')
            
            demographics = audience_profile.get('estimated_demographics', {})
            enhanced_data['ai_target_demographics'] = {
                'age': demographics.get('age', '20-40æ­³'),
                'gender': demographics.get('gender', 'ç”·å¥³åŠã€…'),
                'income': demographics.get('income', 'ä¸­')
            }
            
            print(f"âœ… é«˜åº¦AIåˆ†æå®Œäº†: {channel_data['channel_title']}")
            print(f"   ğŸ¯ AIã‚«ãƒ†ã‚´ãƒª: {enhanced_data['ai_category']}")
            print(f"   ğŸ­ ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª: {', '.join(enhanced_data['ai_sub_categories'][:2])}")
            print(f"   ğŸ›¡ï¸ å®‰å…¨æ€§: {enhanced_data['ai_brand_safety_score']:.2f}")
            print(f"   ğŸ“ˆ ä¿¡é ¼åº¦: {enhanced_data['ai_analysis_confidence']:.2f}")
            print(f"   ğŸ’¼ æ¨å¥¨å•†æ: {enhanced_data['ai_top_product_category']}")
            print(f"   ğŸ‘¥ å¯¾è±¡å¹´é½¢: {enhanced_data['ai_target_age']}")
            
            return enhanced_data
            
        except Exception as e:
            print(f"âŒ é«˜åº¦AIåˆ†æã‚¨ãƒ©ãƒ¼ ({channel_data['channel_title']}): {e}")
            return None
    
    def update_bigquery_with_enhanced_ai(self, enhanced_channels):
        """BigQueryã«AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        try:
            print("ğŸ“Š BigQueryã«AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ä¸­...")
            
            table_id = f"{self.project_id}.infumatch_data.influencers"
            updated_count = 0
            
            for channel in enhanced_channels:
                if channel:
                    try:
                        # æ›´æ–°ç”¨ã‚¯ã‚¨ãƒª
                        update_query = f"""
                        UPDATE `{table_id}`
                        SET 
                            ai_analysis = @ai_analysis,
                            updated_at = CURRENT_TIMESTAMP()
                        WHERE channel_id = @channel_id
                        """
                        
                        # ai_analysisã‚’æ›´æ–°ï¼ˆæ–°ã—ã„é«˜åº¦åˆ†æã§ç½®ãæ›ãˆï¼‰
                        new_ai_analysis = {
                            # æ—¢å­˜ã®ç°¡æ˜“åˆ†ææƒ…å ±
                            "engagement_rate": channel['engagement_estimate'],
                            "content_quality_score": 0.8,
                            "brand_safety_score": channel['ai_brand_safety_score'],
                            "growth_potential": 0.7,
                            
                            # æ–°ã—ã„é«˜åº¦AIåˆ†ææƒ…å ±
                            "advanced_analysis": {
                                "category_tags": {
                                    "primary_category": channel['ai_category'],
                                    "sub_categories": channel['ai_sub_categories'],
                                    "content_themes": channel['ai_content_themes'],
                                    "target_age_group": channel['ai_target_age'],
                                    "confidence_score": channel['ai_confidence_score']
                                },
                                "product_matching": {
                                    "top_category": channel['ai_top_product_category'],
                                    "match_score": channel['ai_top_product_match_score'],
                                    "collaboration_formats": channel['ai_collaboration_formats']
                                },
                                "content_analysis": {
                                    "content_style": channel['ai_content_style'],
                                    "expertise_level": channel['ai_expertise_level'],
                                    "entertainment_value": channel['ai_entertainment_value'],
                                    "educational_value": channel['ai_educational_value']
                                },
                                "audience_profile": {
                                    "audience_size": channel['ai_audience_size'],
                                    "engagement_level": channel['ai_engagement_level'],
                                    "reach_potential": channel['ai_reach_potential'],
                                    "demographics": channel['ai_target_demographics']
                                },
                                "analysis_meta": {
                                    "confidence": channel['ai_analysis_confidence'],
                                    "enhanced_at": channel['ai_enhanced_at'],
                                    "version": "2.0"
                                }
                            },
                            
                            # å®Œå…¨ãªåˆ†æçµæœ
                            "full_analysis": channel['advanced_ai_analysis']
                        }
                        
                        job_config = bigquery.QueryJobConfig(
                            query_parameters=[
                                bigquery.ScalarQueryParameter("ai_analysis", "STRING", json.dumps(new_ai_analysis, ensure_ascii=False)),
                                bigquery.ScalarQueryParameter("channel_id", "STRING", channel['channel_id'])
                            ]
                        )
                        
                        query_job = self.bigquery_client.query(update_query, job_config=job_config)
                        query_job.result()
                        
                        updated_count += 1
                        print(f"  âœ… {channel['channel_title']} æ›´æ–°å®Œäº†")
                        
                    except Exception as e:
                        print(f"  âŒ {channel['channel_title']} æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            print(f"âœ… BigQueryã« {updated_count} ä»¶ã®AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            return updated_count
            
        except Exception as e:
            print(f"âŒ BigQueryæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def update_firestore_with_enhanced_ai(self, enhanced_channels):
        """Firestoreã«AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        try:
            print("ğŸ”¥ Firestoreã«AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ä¸­...")
            
            collection_ref = self.firestore_client.collection('influencers')
            updated_count = 0
            
            for channel in enhanced_channels:
                if channel:
                    try:
                        # Firestoreç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                        firestore_data = {
                            'influencer_id': channel['influencer_id'],
                            'channel_id': channel['channel_id'],
                            'channel_title': channel['channel_title'],
                            'description': channel['description'],
                            'subscriber_count': channel['subscriber_count'],
                            'video_count': channel['video_count'],
                            'view_count': channel['view_count'],
                            'category': channel['category'],
                            'country': channel['country'],
                            'language': channel['language'],
                            'contact_email': channel['contact_email'],
                            'emails': channel['emails'],
                            'has_contact': channel['has_contact'],
                            'social_links': channel['social_links'],
                            'engagement_estimate': channel['engagement_estimate'],
                            'created_at': channel['created_at'],
                            'updated_at': datetime.now(),
                            'is_active': channel['is_active'],
                            
                            # AIå¼·åŒ–æƒ…å ±
                            'ai_analysis': {
                                # æ—¢å­˜ã®ç°¡æ˜“åˆ†æ
                                "engagement_rate": channel['engagement_estimate'],
                                "content_quality_score": 0.8,
                                "brand_safety_score": channel['ai_brand_safety_score'],
                                "growth_potential": 0.7,
                                
                                # æ–°ã—ã„é«˜åº¦åˆ†æ
                                "advanced": {
                                    "category": channel['ai_category'],
                                    "sub_categories": channel['ai_sub_categories'],
                                    "content_themes": channel['ai_content_themes'],
                                    "target_age": channel['ai_target_age'],
                                    "confidence": channel['ai_analysis_confidence'],
                                    "safety_score": channel['ai_brand_safety_score'],
                                    "top_product": channel['ai_top_product_category'],
                                    "match_score": channel['ai_top_product_match_score'],
                                    "audience_size": channel['ai_audience_size'],
                                    "demographics": channel['ai_target_demographics'],
                                    "enhanced_at": channel['ai_enhanced_at']
                                },
                                
                                # å®Œå…¨ãªåˆ†æçµæœ
                                "full_analysis": channel['advanced_ai_analysis']
                            }
                        }
                        
                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
                        doc_ref = collection_ref.document(channel['channel_id'])
                        doc_ref.set(firestore_data, merge=True)
                        updated_count += 1
                        
                    except Exception as e:
                        print(f"âŒ Firestoreæ›´æ–°ã‚¨ãƒ©ãƒ¼ ({channel['channel_title']}): {e}")
                        continue
            
            print(f"âœ… Firestoreã« {updated_count} ä»¶ã®AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
            return updated_count
            
        except Exception as e:
            print(f"âŒ Firestoreæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    async def enhance_existing_data_with_advanced_ai(self, limit=5):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’é«˜åº¦AIåˆ†æã§å¼·åŒ–"""
        print("ğŸš€ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®é«˜åº¦AIåˆ†æå¼·åŒ–é–‹å§‹")
        print("=" * 80)
        
        print("ğŸ”§ å®Ÿè¡Œå†…å®¹:")
        print("  1. BigQueryã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
        print("  2. å„ãƒãƒ£ãƒ³ãƒãƒ«ã«é«˜åº¦AIåˆ†æã‚’å®Ÿè¡Œ")
        print("  3. BigQueryã®ai_analysisåˆ—ã‚’æ›´æ–°")
        print("  4. Firestoreã®ai_analysisæƒ…å ±ã‚’æ›´æ–°")
        print("  5. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒã—ã€AIåˆ†æã®ã¿å¼·åŒ–")
        print()
        
        # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å–å¾—
        existing_channels = self.fetch_existing_channels(limit)
        if not existing_channels:
            print("âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ“Š {len(existing_channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®é«˜åº¦AIåˆ†æã‚’é–‹å§‹...")
        print()
        
        # 2. é«˜åº¦AIåˆ†æå®Ÿè¡Œ
        enhanced_channels = []
        for i, channel in enumerate(existing_channels, 1):
            print(f"â³ é€²æ—: {i}/{len(existing_channels)} ({i/len(existing_channels)*100:.1f}%)")
            
            try:
                enhanced_channel = await self.enhance_channel_with_advanced_ai(channel)
                if enhanced_channel:
                    enhanced_channels.append(enhanced_channel)
                    self.enhanced_count += 1
                else:
                    self.failed_count += 1
                    
                print()
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
                if i < len(existing_channels):
                    print("â¸ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚3ç§’å¾…æ©Ÿ...")
                    await asyncio.sleep(3)
                    
            except Exception as e:
                print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                self.failed_count += 1
                continue
        
        print(f"\nğŸ“Š é«˜åº¦AIåˆ†æå®Œäº†:")
        print(f"  - æˆåŠŸ: {self.enhanced_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - å¤±æ•—: {self.failed_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print()
        
        if enhanced_channels:
            # 3. BigQueryæ›´æ–°
            bigquery_count = self.update_bigquery_with_enhanced_ai(enhanced_channels)
            
            # 4. Firestoreæ›´æ–°
            firestore_count = self.update_firestore_with_enhanced_ai(enhanced_channels)
            
            # 5. çµæœä¿å­˜
            self.save_enhancement_report(enhanced_channels)
            
            print("\n" + "=" * 80)
            print("ğŸ‰ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®é«˜åº¦AIåˆ†æå¼·åŒ–å®Œäº†ï¼")
            print(f"ğŸ“Š BigQueryæ›´æ–°: {bigquery_count} ä»¶")
            print(f"ğŸ”¥ Firestoreæ›´æ–°: {firestore_count} ä»¶")
            print(f"ğŸ“ˆ AIåˆ†ææˆåŠŸç‡: {self.enhanced_count/(self.enhanced_count+self.failed_count)*100:.1f}%")
            print()
            print("ğŸ’¡ ç¢ºèªæ–¹æ³•:")
            print("  - BigQuery: ai_analysisåˆ—ã®'advanced_analysis'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰")
            print("  - Firestore: ai_analysis.advanced ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰")
            
        else:
            print("âŒ AIå¼·åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def save_enhancement_report(self, enhanced_channels):
        """AIå¼·åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        try:
            # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            enhancement_data = []
            for channel in enhanced_channels:
                if channel:
                    enhancement_data.append({
                        "channel_id": channel["channel_id"],
                        "channel_title": channel["channel_title"],
                        "original_category": channel["category"],
                        "ai_category": channel["ai_category"],
                        "ai_sub_categories": channel["ai_sub_categories"],
                        "ai_content_themes": channel["ai_content_themes"],
                        "ai_target_age": channel["ai_target_age"],
                        "ai_confidence": channel["ai_analysis_confidence"],
                        "ai_safety_score": channel["ai_brand_safety_score"],
                        "ai_top_product": channel["ai_top_product_category"],
                        "ai_match_score": channel["ai_top_product_match_score"],
                        "ai_enhanced_at": channel["ai_enhanced_at"]
                    })
            
            with open('existing_data_enhanced.json', 'w', encoding='utf-8') as f:
                json.dump(enhancement_data, f, ensure_ascii=False, indent=2)
            
            # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                "enhancement_timestamp": datetime.now().isoformat(),
                "total_processed": len(enhanced_channels),
                "successful_enhancements": self.enhanced_count,
                "failed_enhancements": self.failed_count,
                "success_rate": self.enhanced_count/(self.enhanced_count+self.failed_count)*100 if (self.enhanced_count+self.failed_count) > 0 else 0,
                "enhancement_summary": {
                    "categories_enhanced": len(set(ch["ai_category"] for ch in enhanced_channels if ch)),
                    "avg_confidence": sum(ch["ai_analysis_confidence"] for ch in enhanced_channels if ch) / len(enhanced_channels),
                    "avg_safety_score": sum(ch["ai_brand_safety_score"] for ch in enhanced_channels if ch) / len(enhanced_channels),
                    "product_categories": list(set(ch["ai_top_product_category"] for ch in enhanced_channels if ch))
                },
                "enhanced_channels": enhancement_data
            }
            
            with open('ai_enhancement_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ AIå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ existing_data_enhanced.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"ğŸ“Š å¼·åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ ai_enhancement_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    enhancer = ExistingDataAIEnhancer()
    
    try:
        # ãƒ†ã‚¹ãƒˆç”¨ã«5ä»¶ã‹ã‚‰é–‹å§‹
        await enhancer.enhance_existing_data_with_advanced_ai(limit=5)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    asyncio.run(main())