#!/usr/bin/env python3
"""
å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†ææ›´æ–°

@description BigQuery/Firestoreã®å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ï¼ˆ24ä»¶ï¼‰ã«AIåˆ†æã‚’è¿½åŠ 
å®‰å…¨ã«æ®µéšçš„ã«å®Ÿè¡Œ

@author InfuMatch Development Team
@version 1.0.0
"""

import asyncio
import json
from datetime import datetime
from google.cloud import firestore, bigquery
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
PROJECT_ID = "hackathon-462905"

class AllDataAIUpdater:
    """å…¨ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†ææ›´æ–°å™¨"""
    
    def __init__(self):
        self.project_id = PROJECT_ID
        self.bigquery_client = bigquery.Client(project=self.project_id)
        self.firestore_client = firestore.Client(project=self.project_id)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        self.updated_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        
    def get_all_channels_status(self):
        """å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®æ›´æ–°çŠ¶æ³ã‚’ç¢ºèª"""
        try:
            print("ğŸ“Š å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®æ›´æ–°çŠ¶æ³ã‚’ç¢ºèªä¸­...")
            
            query = """
            SELECT 
                channel_id,
                channel_title,
                ai_analysis,
                updated_at
            FROM `hackathon-462905.infumatch_data.influencers`
            WHERE is_active = true
            ORDER BY subscriber_count DESC
            """
            
            query_job = self.bigquery_client.query(query)
            results = query_job.result()
            
            all_channels = []
            updated_channels = []
            old_format_channels = []
            
            for row in results:
                channel_info = {
                    'channel_id': row.channel_id,
                    'channel_title': row.channel_title,
                    'updated_at': row.updated_at
                }
                
                all_channels.append(channel_info)
                
                # AIåˆ†æã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
                if row.ai_analysis:
                    try:
                        ai_data = json.loads(row.ai_analysis)
                        if 'advanced_analysis' in ai_data:
                            updated_channels.append(channel_info)
                        else:
                            old_format_channels.append(channel_info)
                    except:
                        old_format_channels.append(channel_info)
                else:
                    old_format_channels.append(channel_info)
            
            print(f"ğŸ“Š å…¨ä½“çŠ¶æ³:")
            print(f"  - ç·ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {len(all_channels)}")
            print(f"  - æ–°å½¢å¼AIåˆ†ææ¸ˆã¿: {len(updated_channels)}")
            print(f"  - æ—§å½¢å¼/æœªåˆ†æ: {len(old_format_channels)}")
            print()
            
            if updated_channels:
                print("âœ… æ–°å½¢å¼AIåˆ†ææ¸ˆã¿ãƒãƒ£ãƒ³ãƒãƒ«:")
                for ch in updated_channels:
                    print(f"  - {ch['channel_title']}")
                print()
            
            print(f"ğŸ”„ æ›´æ–°ãŒå¿…è¦ãªãƒãƒ£ãƒ³ãƒãƒ« ({len(old_format_channels)}ä»¶):")
            for i, ch in enumerate(old_format_channels[:10], 1):  # æœ€åˆã®10ä»¶è¡¨ç¤º
                print(f"  {i:2d}. {ch['channel_title']}")
            if len(old_format_channels) > 10:
                print(f"     ... ä»– {len(old_format_channels) - 10} ä»¶")
            print()
            
            return old_format_channels
            
        except Exception as e:
            print(f"âŒ çŠ¶æ³ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_channels_for_update(self, channel_ids):
        """æ›´æ–°å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            print(f"ğŸ“Š {len(channel_ids)} ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            
            # ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’ã‚¯ã‚¨ãƒªç”¨ã«æº–å‚™
            channel_ids_str = "', '".join(channel_ids)
            
            query = f"""
            SELECT 
                influencer_id,
                channel_id,
                channel_title,
                description,
                subscriber_count,
                video_count,
                view_count,
                category,
                country,
                language,
                contact_email,
                social_links,
                ai_analysis,
                created_at,
                updated_at,
                is_active
            FROM `hackathon-462905.infumatch_data.influencers`
            WHERE channel_id IN ('{channel_ids_str}')
            AND is_active = true
            ORDER BY subscriber_count DESC
            """
            
            query_job = self.bigquery_client.query(query)
            results = query_job.result()
            
            channels = []
            for row in results:
                # social_linksã‹ã‚‰emailsã‚’å–å¾—
                social_links = {}
                try:
                    if row.social_links:
                        social_links = json.loads(row.social_links)
                except:
                    social_links = {}
                
                emails = social_links.get('emails', [])
                if row.contact_email:
                    emails.append(row.contact_email)
                
                channel_data = {
                    'influencer_id': row.influencer_id,
                    'channel_id': row.channel_id,
                    'channel_title': row.channel_title,
                    'description': row.description or '',
                    'subscriber_count': int(row.subscriber_count),
                    'video_count': int(row.video_count),
                    'view_count': int(row.view_count),
                    'category': row.category,
                    'country': row.country or 'JP',
                    'language': row.language or 'ja',
                    'contact_email': row.contact_email or '',
                    'emails': emails,
                    'has_contact': len(emails) > 0 or bool(row.contact_email),
                    'social_links': social_links,
                    'old_ai_analysis': row.ai_analysis,
                    'created_at': row.created_at,
                    'updated_at': row.updated_at,
                    'is_active': row.is_active
                }
                
                # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨å®šå€¤è¨ˆç®—
                if channel_data['video_count'] > 0 and channel_data['subscriber_count'] > 0:
                    channel_data['engagement_estimate'] = round(
                        (channel_data['view_count'] / channel_data['video_count'] / channel_data['subscriber_count']) * 100, 2
                    )
                else:
                    channel_data['engagement_estimate'] = 0.0
                
                channels.append(channel_data)
            
            print(f"âœ… {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return channels
            
        except Exception as e:
            print(f"âŒ è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def process_channels_in_batches(self, channels, batch_size=3):
        """ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã§æ›´æ–°"""
        total = len(channels)
        
        print(f"ğŸš€ {total} ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒãƒƒãƒå‡¦ç†é–‹å§‹ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ï¼‰")
        print("=" * 80)
        
        for i in range(0, total, batch_size):
            batch = channels[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (total + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches} å‡¦ç†ä¸­...")
            print(f"   ãƒãƒ£ãƒ³ãƒãƒ« {i+1}-{min(i+batch_size, total)} / {total}")
            
            # ãƒãƒƒãƒå†…ã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
            tasks = []
            for channel in batch:
                task = self.enhance_channel_with_ai(channel)
                tasks.append(task)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            enhanced_channels = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœå‡¦ç†
            successful_channels = []
            for j, result in enumerate(enhanced_channels):
                if isinstance(result, Exception):
                    print(f"âŒ {batch[j]['channel_title']}: {result}")
                    self.failed_count += 1
                elif result:
                    successful_channels.append(result)
                    self.updated_count += 1
                    print(f"âœ… {result['channel_title']}: AIåˆ†æå®Œäº†")
                else:
                    self.failed_count += 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            if successful_channels:
                await self.update_databases_batch(successful_channels)
            
            # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
            if i + batch_size < total:
                wait_time = 5
                print(f"â¸ï¸ æ¬¡ã®ãƒãƒƒãƒã¾ã§ {wait_time} ç§’å¾…æ©Ÿ...")
                await asyncio.sleep(wait_time)
        
        print(f"\nğŸ“Š å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†:")
        print(f"  - æˆåŠŸ: {self.updated_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - å¤±æ•—: {self.failed_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - æˆåŠŸç‡: {self.updated_count/(self.updated_count+self.failed_count)*100:.1f}%")
    
    async def enhance_channel_with_ai(self, channel_data):
        """1ã¤ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«AIåˆ†æã‚’è¿½åŠ """
        try:
            # AIåˆ†æå®Ÿè¡Œ
            advanced_ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
            
            # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
            enhanced_data = {
                **channel_data,
                'advanced_ai_analysis': advanced_ai_analysis,
                'ai_category': advanced_ai_analysis.get('category_tags', {}).get('primary_category', channel_data['category']),
                'ai_sub_categories': advanced_ai_analysis.get('category_tags', {}).get('sub_categories', []),
                'ai_content_themes': advanced_ai_analysis.get('category_tags', {}).get('content_themes', []),
                'ai_target_age': advanced_ai_analysis.get('category_tags', {}).get('target_age_group', 'ä¸æ˜'),
                'ai_confidence_score': advanced_ai_analysis.get('category_tags', {}).get('confidence_score', 0.5),
                'ai_brand_safety_score': advanced_ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                'ai_analysis_confidence': advanced_ai_analysis.get('analysis_confidence', 0.5),
                'ai_enhanced_at': datetime.now().isoformat()
            }
            
            # å•†æãƒãƒƒãƒãƒ³ã‚°æƒ…å ±
            product_matching = advanced_ai_analysis.get('product_matching', {})
            recommended_products = product_matching.get('recommended_products', [])
            
            if recommended_products:
                enhanced_data['ai_top_product_category'] = recommended_products[0].get('category', 'ä¸æ˜')
                enhanced_data['ai_top_product_match_score'] = recommended_products[0].get('match_score', 0.0)
                enhanced_data['ai_collaboration_formats'] = product_matching.get('collaboration_formats', [])
            else:
                enhanced_data['ai_top_product_category'] = 'ä¸æ˜'
                enhanced_data['ai_top_product_match_score'] = 0.0
                enhanced_data['ai_collaboration_formats'] = []
            
            return enhanced_data
            
        except Exception as e:
            return None
    
    async def update_databases_batch(self, enhanced_channels):
        """ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°"""
        # BigQueryæ›´æ–°
        bigquery_count = await self.update_bigquery_batch(enhanced_channels)
        
        # Firestoreæ›´æ–°
        firestore_count = await self.update_firestore_batch(enhanced_channels)
        
        print(f"    ğŸ“Š BigQueryæ›´æ–°: {bigquery_count} ä»¶")
        print(f"    ğŸ”¥ Firestoreæ›´æ–°: {firestore_count} ä»¶")
    
    async def update_bigquery_batch(self, enhanced_channels):
        """BigQueryã‚’ãƒãƒƒãƒæ›´æ–°"""
        try:
            table_id = f"{self.project_id}.infumatch_data.influencers"
            updated_count = 0
            
            for channel in enhanced_channels:
                if channel:
                    # æ–°ã—ã„ai_analysisä½œæˆ
                    new_ai_analysis = {
                        "engagement_rate": channel['engagement_estimate'],
                        "content_quality_score": 0.8,
                        "brand_safety_score": channel['ai_brand_safety_score'],
                        "growth_potential": 0.7,
                        "advanced_analysis": {
                            "category_tags": {
                                "primary_category": channel['ai_category'],
                                "sub_categories": channel['ai_sub_categories'],
                                "content_themes": channel['ai_content_themes'],
                                "target_age_group": channel['ai_target_age'],
                                "confidence_score": channel['ai_confidence_score']
                            },
                            "product_matching": {
                                "top_category": channel['ai_top_product_category'],
                                "match_score": channel['ai_top_product_match_score'],
                                "collaboration_formats": channel['ai_collaboration_formats']
                            },
                            "analysis_meta": {
                                "confidence": channel['ai_analysis_confidence'],
                                "enhanced_at": channel['ai_enhanced_at'],
                                "version": "2.0"
                            }
                        },
                        "full_analysis": channel['advanced_ai_analysis']
                    }
                    
                    update_query = f"""
                    UPDATE `{table_id}`
                    SET 
                        ai_analysis = @ai_analysis,
                        updated_at = CURRENT_TIMESTAMP()
                    WHERE channel_id = @channel_id
                    """
                    
                    job_config = bigquery.QueryJobConfig(
                        query_parameters=[
                            bigquery.ScalarQueryParameter("ai_analysis", "STRING", json.dumps(new_ai_analysis, ensure_ascii=False)),
                            bigquery.ScalarQueryParameter("channel_id", "STRING", channel['channel_id'])
                        ]
                    )
                    
                    query_job = self.bigquery_client.query(update_query, job_config=job_config)
                    query_job.result()
                    updated_count += 1
            
            return updated_count
            
        except Exception as e:
            print(f"âŒ BigQueryãƒãƒƒãƒæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    async def update_firestore_batch(self, enhanced_channels):
        """Firestoreã‚’ãƒãƒƒãƒæ›´æ–°"""
        try:
            collection_ref = self.firestore_client.collection('influencers')
            updated_count = 0
            
            # ãƒãƒƒãƒæ›¸ãè¾¼ã¿
            batch = self.firestore_client.batch()
            
            for channel in enhanced_channels:
                if channel:
                    firestore_data = {
                        'ai_analysis': {
                            "engagement_rate": channel['engagement_estimate'],
                            "content_quality_score": 0.8,
                            "brand_safety_score": channel['ai_brand_safety_score'],
                            "growth_potential": 0.7,
                            "advanced": {
                                "category": channel['ai_category'],
                                "sub_categories": channel['ai_sub_categories'],
                                "content_themes": channel['ai_content_themes'],
                                "target_age": channel['ai_target_age'],
                                "confidence": channel['ai_analysis_confidence'],
                                "safety_score": channel['ai_brand_safety_score'],
                                "top_product": channel['ai_top_product_category'],
                                "match_score": channel['ai_top_product_match_score'],
                                "enhanced_at": channel['ai_enhanced_at']
                            },
                            "full_analysis": channel['advanced_ai_analysis']
                        },
                        'updated_at': datetime.now()
                    }
                    
                    doc_ref = collection_ref.document(channel['channel_id'])
                    batch.update(doc_ref, firestore_data)
                    updated_count += 1
            
            # ãƒãƒƒãƒå®Ÿè¡Œ
            if updated_count > 0:
                batch.commit()
            
            return updated_count
            
        except Exception as e:
            print(f"âŒ Firestoreãƒãƒƒãƒæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def save_final_report(self):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
        try:
            report = {
                "full_update_timestamp": datetime.now().isoformat(),
                "total_processed": self.updated_count + self.failed_count,
                "successful_updates": self.updated_count,
                "failed_updates": self.failed_count,
                "skipped_updates": self.skipped_count,
                "success_rate": self.updated_count/(self.updated_count+self.failed_count)*100 if (self.updated_count+self.failed_count) > 0 else 0,
                "update_method": "batch_processing",
                "version": "2.0"
            }
            
            with open('full_ai_update_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ full_ai_update_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def update_all_existing_data(self):
        """å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        print("ğŸš€ å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†ææ›´æ–°é–‹å§‹")
        print("=" * 80)
        
        # 1. ç¾åœ¨ã®çŠ¶æ³ç¢ºèª
        channels_to_update = self.get_all_channels_status()
        
        if not channels_to_update:
            print("âœ… å…¨ã¦ã®ãƒãƒ£ãƒ³ãƒãƒ«ãŒæ—¢ã«æœ€æ–°ã®AIåˆ†ææ¸ˆã¿ã§ã™")
            return
        
        print(f"ğŸ”„ {len(channels_to_update)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ›´æ–°ã—ã¾ã™")
        
        # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        response = input("\nç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() != 'y':
            print("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return
        
        # 2. è©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—
        channel_ids = [ch['channel_id'] for ch in channels_to_update]
        detailed_channels = self.fetch_channels_for_update(channel_ids)
        
        if not detailed_channels:
            print("âŒ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 3. ãƒãƒƒãƒå‡¦ç†ã§æ›´æ–°
        await self.process_channels_in_batches(detailed_channels, batch_size=3)
        
        # 4. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        self.save_final_report()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ å…¨æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†ææ›´æ–°å®Œäº†ï¼")
        print(f"ğŸ“Š æ›´æ–°æ¸ˆã¿: {self.updated_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"âŒ å¤±æ•—: {self.failed_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.updated_count/(self.updated_count+self.failed_count)*100:.1f}%")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    updater = AllDataAIUpdater()
    
    try:
        await updater.update_all_existing_data()
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    asyncio.run(main())