#!/usr/bin/env python3
"""
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªAIåˆ†æãƒªãƒã‚¤ã‚º

@description æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«æ–°ã—ã„åˆ—ã‚’è¿½åŠ ã—ã¦AIåˆ†æçµæœã‚’å®‰å…¨ã«æ›´æ–°
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å–ã‚ŠãªãŒã‚‰æ®µéšçš„ã«å®Ÿè¡Œ

@author InfuMatch Development Team
@version 1.0.0
"""

import asyncio
import json
from datetime import datetime
from google.cloud import firestore, bigquery
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
PROJECT_ID = "hackathon-462905"

class SafeExistingDataReviser:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªAIåˆ†æãƒªãƒã‚¤ã‚¶ãƒ¼"""
    
    def __init__(self):
        self.project_id = PROJECT_ID
        self.bigquery_client = bigquery.Client(project=self.project_id)
        self.firestore_client = firestore.Client(project=self.project_id)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        self.revised_count = 0
        self.failed_count = 0
        
    def fetch_existing_channels(self, limit=5):
        """æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°ï¼‰"""
        try:
            print(f"ğŸ“Š BigQueryã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ï¼ˆæœ€å¤§{limit}ä»¶ï¼‰...")
            
            query = f"""
            SELECT 
                channel_id,
                channel_title,
                description,
                subscriber_count,
                video_count,
                view_count,
                country,
                category,
                emails,
                has_contact,
                engagement_estimate,
                collected_at
            FROM `hackathon-462905.infumatch_data.influencers`
            ORDER BY subscriber_count DESC
            LIMIT {limit}
            """
            
            query_job = self.bigquery_client.query(query)
            results = query_job.result()
            
            channels = []
            for row in results:
                channel_data = {
                    'channel_id': row.channel_id,
                    'channel_title': row.channel_title,
                    'description': row.description or '',
                    'subscriber_count': int(row.subscriber_count),
                    'video_count': int(row.video_count),
                    'view_count': int(row.view_count),
                    'country': row.country,
                    'category': row.category,
                    'emails': row.emails or [],
                    'has_contact': bool(row.has_contact),
                    'engagement_estimate': float(row.engagement_estimate),
                    'collected_at': row.collected_at
                }
                channels.append(channel_data)
            
            print(f"âœ… {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            return channels
            
        except Exception as e:
            print(f"âŒ BigQueryãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def add_ai_analysis_to_channel(self, channel_data):
        """1ã¤ã®ãƒãƒ£ãƒ³ãƒãƒ«ã«AIåˆ†æã‚’è¿½åŠ """
        try:
            print(f"ğŸ¤– AIåˆ†æä¸­: {channel_data['channel_title']}")
            
            # AIåˆ†æå®Ÿè¡Œ
            ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
            
            # AIåˆ†æçµæœã‚’çµ±åˆ
            enhanced_data = {
                **channel_data,
                'ai_analysis': ai_analysis,
                'ai_category': ai_analysis.get('category_tags', {}).get('primary_category', channel_data['category']),
                'ai_sub_categories': ai_analysis.get('category_tags', {}).get('sub_categories', []),
                'ai_content_themes': ai_analysis.get('category_tags', {}).get('content_themes', []),
                'ai_target_age': ai_analysis.get('category_tags', {}).get('target_age_group', 'ä¸æ˜'),
                'ai_brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                'ai_analysis_confidence': ai_analysis.get('analysis_confidence', 0.5),
                'ai_revised_at': datetime.now().isoformat()
            }
            
            # æ¨å¥¨å•†ææƒ…å ±ã‚’ç°¡æ½”ã«
            recommended_products = ai_analysis.get('product_matching', {}).get('recommended_products', [])
            if recommended_products:
                enhanced_data['ai_top_product_category'] = recommended_products[0].get('category', 'ä¸æ˜')
                enhanced_data['ai_top_product_match_score'] = recommended_products[0].get('match_score', 0.0)
            else:
                enhanced_data['ai_top_product_category'] = 'ä¸æ˜'
                enhanced_data['ai_top_product_match_score'] = 0.0
            
            print(f"âœ… AIåˆ†æå®Œäº†: {channel_data['channel_title']}")
            print(f"   ğŸ¯ AIã‚«ãƒ†ã‚´ãƒª: {enhanced_data['ai_category']}")
            print(f"   ğŸ›¡ï¸ å®‰å…¨æ€§: {enhanced_data['ai_brand_safety_score']:.2f}")
            print(f"   ğŸ“ˆ ä¿¡é ¼åº¦: {enhanced_data['ai_analysis_confidence']:.2f}")
            print(f"   ğŸ’¼ æ¨å¥¨å•†æ: {enhanced_data['ai_top_product_category']}")
            
            return enhanced_data
            
        except Exception as e:
            print(f"âŒ AIåˆ†æã‚¨ãƒ©ãƒ¼ ({channel_data['channel_title']}): {e}")
            return None
    
    def create_ai_enhanced_table(self, enhanced_channels):
        """AIåˆ†æçµæœã‚’å«ã‚€æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
        try:
            print("ğŸ—ï¸ AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆä¸­...")
            
            # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ID
            table_id = f"{self.project_id}.infumatch_data.influencers_ai_enhanced"
            
            # ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
            schema = [
                # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                bigquery.SchemaField("channel_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("channel_title", "STRING"),
                bigquery.SchemaField("description", "STRING"),
                bigquery.SchemaField("subscriber_count", "INTEGER"),
                bigquery.SchemaField("video_count", "INTEGER"),
                bigquery.SchemaField("view_count", "INTEGER"),
                bigquery.SchemaField("country", "STRING"),
                bigquery.SchemaField("category", "STRING"),
                bigquery.SchemaField("emails", "STRING", mode="REPEATED"),
                bigquery.SchemaField("has_contact", "BOOLEAN"),
                bigquery.SchemaField("engagement_estimate", "FLOAT"),
                bigquery.SchemaField("collected_at", "STRING"),
                # AIåˆ†æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                bigquery.SchemaField("ai_category", "STRING"),
                bigquery.SchemaField("ai_sub_categories", "STRING", mode="REPEATED"),
                bigquery.SchemaField("ai_content_themes", "STRING", mode="REPEATED"),
                bigquery.SchemaField("ai_target_age", "STRING"),
                bigquery.SchemaField("ai_top_product_category", "STRING"),
                bigquery.SchemaField("ai_top_product_match_score", "FLOAT"),
                bigquery.SchemaField("ai_brand_safety_score", "FLOAT"),
                bigquery.SchemaField("ai_analysis_confidence", "FLOAT"),
                bigquery.SchemaField("ai_revised_at", "STRING"),
                bigquery.SchemaField("ai_analysis_json", "STRING"),  # å®Œå…¨ãªAIåˆ†æçµæœ
            ]
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            table = bigquery.Table(table_id, schema=schema)
            
            # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤
            try:
                self.bigquery_client.delete_table(table_id)
                print(f"ğŸ—‘ï¸ æ—¢å­˜ã®AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            except:
                pass
            
            table = self.bigquery_client.create_table(table)
            print(f"âœ… AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†: {table_id}")
            
            # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
            rows_to_insert = []
            for channel in enhanced_channels:
                if channel:
                    row = {
                        'channel_id': channel['channel_id'],
                        'channel_title': channel['channel_title'],
                        'description': channel['description'],
                        'subscriber_count': channel['subscriber_count'],
                        'video_count': channel['video_count'],
                        'view_count': channel['view_count'],
                        'country': channel['country'],
                        'category': channel['category'],
                        'emails': channel['emails'],
                        'has_contact': channel['has_contact'],
                        'engagement_estimate': channel['engagement_estimate'],
                        'collected_at': channel['collected_at'],
                        'ai_category': channel['ai_category'],
                        'ai_sub_categories': channel['ai_sub_categories'],
                        'ai_content_themes': channel['ai_content_themes'],
                        'ai_target_age': channel['ai_target_age'],
                        'ai_top_product_category': channel['ai_top_product_category'],
                        'ai_top_product_match_score': channel['ai_top_product_match_score'],
                        'ai_brand_safety_score': channel['ai_brand_safety_score'],
                        'ai_analysis_confidence': channel['ai_analysis_confidence'],
                        'ai_revised_at': channel['ai_revised_at'],
                        'ai_analysis_json': json.dumps(channel['ai_analysis'], ensure_ascii=False)
                    }
                    rows_to_insert.append(row)
            
            if rows_to_insert:
                errors = self.bigquery_client.insert_rows_json(table, rows_to_insert)
                if errors:
                    print(f"âŒ BigQueryæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {errors}")
                    return 0
                else:
                    print(f"âœ… AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ã« {len(rows_to_insert)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã—ã¾ã—ãŸ")
                    return len(rows_to_insert)
            
            return 0
            
        except Exception as e:
            print(f"âŒ AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def update_firestore_with_ai(self, enhanced_channels):
        """Firestoreã«AIåˆ†æçµæœã‚’è¿½åŠ """
        try:
            print("ğŸ”¥ Firestoreã«AIåˆ†æçµæœã‚’è¿½åŠ ä¸­...")
            
            collection_ref = self.firestore_client.collection('influencers_ai_enhanced')
            updated_count = 0
            
            for channel in enhanced_channels:
                if channel:
                    try:
                        # Firestoreç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                        firestore_data = {
                            'channel_id': channel['channel_id'],
                            'channel_title': channel['channel_title'],
                            'description': channel['description'],
                            'subscriber_count': channel['subscriber_count'],
                            'video_count': channel['video_count'],
                            'view_count': channel['view_count'],
                            'country': channel['country'],
                            'category': channel['category'],
                            'emails': channel['emails'],
                            'has_contact': channel['has_contact'],
                            'engagement_estimate': channel['engagement_estimate'],
                            'collected_at': channel['collected_at'],
                            # AIåˆ†æçµæœ
                            'ai_category': channel['ai_category'],
                            'ai_sub_categories': channel['ai_sub_categories'],
                            'ai_content_themes': channel['ai_content_themes'],
                            'ai_target_age': channel['ai_target_age'],
                            'ai_top_product_category': channel['ai_top_product_category'],
                            'ai_top_product_match_score': channel['ai_top_product_match_score'],
                            'ai_brand_safety_score': channel['ai_brand_safety_score'],
                            'ai_analysis_confidence': channel['ai_analysis_confidence'],
                            'ai_revised_at': channel['ai_revised_at'],
                            'ai_analysis': channel['ai_analysis']  # å®Œå…¨ãªAIåˆ†æçµæœ
                        }
                        
                        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ 
                        doc_ref = collection_ref.document(channel['channel_id'])
                        doc_ref.set(firestore_data)
                        updated_count += 1
                        
                    except Exception as e:
                        print(f"âŒ Firestoreè¿½åŠ ã‚¨ãƒ©ãƒ¼ ({channel['channel_title']}): {e}")
                        continue
            
            print(f"âœ… Firestoreã« {updated_count} ä»¶ã®AIæ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            return updated_count
            
        except Exception as e:
            print(f"âŒ Firestoreè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    async def safe_revise_existing_data(self, limit=5):
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«AIåˆ†ææ‹¡å¼µ"""
        print("ğŸš€ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªAIåˆ†ææ‹¡å¼µé–‹å§‹")
        print("=" * 80)
        
        print("ğŸ”§ å®Ÿè¡Œå†…å®¹:")
        print("  1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å°‘æ•°å–å¾—ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰")
        print("  2. å„ãƒãƒ£ãƒ³ãƒãƒ«ã«AIåˆ†æã‚’å®Ÿè¡Œ")
        print("  3. AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ–°è¦ä½œæˆ")
        print("  4. AIæ‹¡å¼µFirestoreã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ")
        print("  5. å…ƒãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰")
        print()
        
        # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å–å¾—
        existing_channels = self.fetch_existing_channels(limit)
        if not existing_channels:
            print("âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ“Š {len(existing_channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®AIåˆ†æã‚’é–‹å§‹...")
        print()
        
        # 2. AIåˆ†æå®Ÿè¡Œ
        enhanced_channels = []
        for i, channel in enumerate(existing_channels, 1):
            print(f"â³ é€²æ—: {i}/{len(existing_channels)} ({i/len(existing_channels)*100:.1f}%)")
            
            try:
                enhanced_channel = await self.add_ai_analysis_to_channel(channel)
                if enhanced_channel:
                    enhanced_channels.append(enhanced_channel)
                    self.revised_count += 1
                else:
                    self.failed_count += 1
                    
                print()
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
                if i < len(existing_channels):
                    print("â¸ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚3ç§’å¾…æ©Ÿ...")
                    await asyncio.sleep(3)
                    
            except Exception as e:
                print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                self.failed_count += 1
                continue
        
        print(f"\nğŸ“Š AIåˆ†æå®Œäº†:")
        print(f"  - æˆåŠŸ: {self.revised_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - å¤±æ•—: {self.failed_count} ãƒãƒ£ãƒ³ãƒãƒ«")
        print()
        
        if enhanced_channels:
            # 3. AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            bigquery_count = self.create_ai_enhanced_table(enhanced_channels)
            
            # 4. AIæ‹¡å¼µFirestoreã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
            firestore_count = self.update_firestore_with_ai(enhanced_channels)
            
            # 5. çµæœä¿å­˜
            self.save_enhanced_data(enhanced_channels)
            
            print("\n" + "=" * 80)
            print("ğŸ‰ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®AIåˆ†ææ‹¡å¼µå®Œäº†ï¼")
            print(f"ğŸ“Š BigQuery AIæ‹¡å¼µãƒ†ãƒ¼ãƒ–ãƒ«: {bigquery_count} ä»¶")
            print(f"ğŸ”¥ Firestore AIæ‹¡å¼µã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {firestore_count} ä»¶")
            print(f"ğŸ“ˆ AIåˆ†ææˆåŠŸç‡: {self.revised_count/(self.revised_count+self.failed_count)*100:.1f}%")
            print()
            print("ğŸ’¡ ç¢ºèªæ–¹æ³•:")
            print("  - BigQuery: `hackathon-462905.infumatch_data.influencers_ai_enhanced`")
            print("  - Firestore: `influencers_ai_enhanced` ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
            
        else:
            print("âŒ AIåˆ†ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def save_enhanced_data(self, enhanced_channels):
        """AIæ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ä¿å­˜
            with open('existing_data_ai_enhanced.json', 'w', encoding='utf-8') as f:
                json.dump(enhanced_channels, f, ensure_ascii=False, indent=2)
            
            # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                "enhancement_timestamp": datetime.now().isoformat(),
                "total_processed": len(enhanced_channels),
                "successful_enhancements": self.revised_count,
                "failed_enhancements": self.failed_count,
                "success_rate": self.revised_count/(self.revised_count+self.failed_count)*100 if (self.revised_count+self.failed_count) > 0 else 0,
                "enhanced_channels_summary": [
                    {
                        "channel_id": ch["channel_id"],
                        "channel_title": ch["channel_title"],
                        "original_category": ch["category"],
                        "ai_category": ch["ai_category"],
                        "ai_confidence": ch["ai_analysis_confidence"],
                        "ai_safety_score": ch["ai_brand_safety_score"],
                        "ai_top_product": ch["ai_top_product_category"]
                    }
                    for ch in enhanced_channels if ch
                ]
            }
            
            with open('ai_enhancement_report.json', 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“„ AIæ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’ existing_data_ai_enhanced.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"ğŸ“Š æ‹¡å¼µãƒ¬ãƒãƒ¼ãƒˆã‚’ ai_enhancement_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    reviser = SafeExistingDataReviser()
    
    try:
        # ãƒ†ã‚¹ãƒˆç”¨ã«5ä»¶ã‹ã‚‰é–‹å§‹
        await reviser.safe_revise_existing_data(limit=5)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    asyncio.run(main())