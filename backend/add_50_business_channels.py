#!/usr/bin/env python3
"""
50ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

@description ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆãƒ»èµ·æ¥­ç³»ãƒãƒ£ãƒ³ãƒãƒ«ã‚’50ä»¶è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
@author InfuMatch Development Team
@version 1.0.0
"""

import os
import json
import asyncio
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Set
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.cloud import firestore
from google.cloud import bigquery

# AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
YOUTUBE_API_KEY = "AIzaSyDtPl5WSRdxk744ha5Tlwno4iTBZVO96r4"
PROJECT_ID = "hackathon-462905"

class BusinessChannelAdder:
    """
    50ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    """
    
    def __init__(self, api_key: str = YOUTUBE_API_KEY):
        self.api_key = api_key
        self.service = build('youtube', 'v3', developerKey=api_key)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.firestore_db = firestore.Client(project=PROJECT_ID)
        self.bigquery_client = bigquery.Client(project=PROJECT_ID)
        
        # æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDè¿½è·¡
        self.existing_channel_ids: Set[str] = set()
        
        # åé›†ãƒ‡ãƒ¼ã‚¿
        self.added_channels = []
        self.stats = {
            'searched': 0,
            'duplicates_skipped': 0,
            'analyzed': 0,
            'saved_firestore': 0,
            'saved_bigquery': 0,
            'errors': 0
        }
    
    async def load_existing_channel_ids(self):
        """æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿"""
        print("ğŸ” æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            collection_ref = self.firestore_db.collection('influencers')
            docs = collection_ref.get()
            
            for doc in docs:
                data = doc.to_dict()
                channel_id = data.get('channel_id')
                if channel_id:
                    self.existing_channel_ids.add(channel_id)
            
            print(f"âœ… æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿å®Œäº†: {len(self.existing_channel_ids)} ä»¶")
            
        except Exception as e:
            print(f"âŒ æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_emails_from_description(self, description: str) -> List[str]:
        """æ¦‚è¦æ¬„ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º"""
        if not description:
            return []
        
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, description)
        return list(set(emails))
    
    async def search_business_channels(self, target_count: int = 80) -> List[Dict[str, Any]]:
        """ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        print(f"\nğŸ” ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢é–‹å§‹ï¼ˆç›®æ¨™: {target_count} ä»¶ç™ºè¦‹ï¼‰")
        
        # ãƒ“ã‚¸ãƒã‚¹ç³»æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
        business_queries = [
            # èµ·æ¥­ãƒ»çµŒå–¶ç³»
            "èµ·æ¥­ çµŒå–¶", "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ— ãƒ™ãƒ³ãƒãƒ£ãƒ¼", "çµŒå–¶è€… CEO", "ãƒ“ã‚¸ãƒã‚¹æˆ¦ç•¥",
            "ä¼šç¤¾çµŒå–¶ ç¤¾é•·", "èµ·æ¥­å®¶ å‰µæ¥­", "çµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ", "ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«",
            
            # æŠ•è³‡ãƒ»è³‡ç”£é‹ç”¨ç³»
            "æŠ•è³‡ æ ªå¼", "è³‡ç”£é‹ç”¨ æŠ•è³‡ä¿¡è¨—", "ä¸å‹•ç”£æŠ•è³‡", "FX ç‚ºæ›¿", 
            "ä»®æƒ³é€šè²¨ ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³", "ç©ç«‹æŠ•è³‡ NISA", "ç±³å›½æ ªæŠ•è³‡", "å€‹åˆ¥æ ªåˆ†æ",
            
            # å‰¯æ¥­ãƒ»ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ç³»
            "å‰¯æ¥­ åœ¨å®…ãƒ¯ãƒ¼ã‚¯", "ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ ç‹¬ç«‹", "ãƒãƒƒãƒˆãƒ“ã‚¸ãƒã‚¹", "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
            "YouTubeåç›ŠåŒ–", "ãƒ–ãƒ­ã‚°åç›Š", "ã›ã©ã‚Š è»¢å£²", "ã‚¦ã‚§ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³",
            
            # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ»ã‚»ãƒ¼ãƒ«ã‚¹ç³»
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥", "ãƒ‡ã‚¸ã‚¿ãƒ«ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "ã‚»ãƒ¼ãƒ«ã‚¹å–¶æ¥­",
            "ãƒ–ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°", "åºƒå‘Šé‹ç”¨", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°", "é¡§å®¢ç²å¾—",
            
            # è‡ªå·±å•“ç™ºãƒ»ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ç³»
            "è‡ªå·±å•“ç™º æˆåŠŸæ³•å‰‡", "æ™‚é–“ç®¡ç† ç”Ÿç”£æ€§", "èª­æ›¸ ãƒ“ã‚¸ãƒã‚¹æ›¸", "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³",
            "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³", "ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—", "ç›®æ¨™é”æˆ", "ç¿’æ…£åŒ–",
            
            # ITãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ç³»
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° ãƒ“ã‚¸ãƒã‚¹", "AI ãƒ“ã‚¸ãƒã‚¹æ´»ç”¨", "DX ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©", "ITã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ",
            "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º", "ã‚¢ãƒ—ãƒªé–‹ç™º", "ã‚¦ã‚§ãƒ–é–‹ç™º", "ãƒ‡ãƒ¼ã‚¿åˆ†æ",
            
            # çµŒæ¸ˆãƒ»é‡‘èç³»
            "çµŒæ¸ˆè§£èª¬ ãƒ‹ãƒ¥ãƒ¼ã‚¹", "é‡‘èå¸‚å ´åˆ†æ", "ãƒã‚¯ãƒ­çµŒæ¸ˆ", "ä¼æ¥­åˆ†æ",
            "æ¥­ç•Œå‹•å‘", "çµŒæ¸ˆæ”¿ç­–", "é‡‘èãƒªãƒ†ãƒ©ã‚·ãƒ¼", "ä¿é™º ãƒ©ã‚¤ãƒ•ãƒ—ãƒ©ãƒ³",
            
            # è»¢è·ãƒ»ã‚­ãƒ£ãƒªã‚¢ç³»
            "è»¢è· ã‚­ãƒ£ãƒªã‚¢", "å°±è·æ´»å‹• é¢æ¥", "ã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒƒãƒ—", "ã‚¹ã‚­ãƒ«è»¢è·",
            "å¤–è³‡ç³»è»¢è·", "ITè»¢è·", "ç®¡ç†è· æ˜‡é€²", "ã‚­ãƒ£ãƒªã‚¢ãƒã‚§ãƒ³ã‚¸",
            
            # ä¼šè¨ˆãƒ»ç¨å‹™ç³»
            "ä¼šè¨ˆ ç°¿è¨˜", "ç¨å‹™ ç¢ºå®šç”³å‘Š", "æ³•äººç¨", "ç›¸ç¶šç¨", "ç¯€ç¨å¯¾ç­–",
            "è²¡å‹™ç®¡ç†", "è³‡é‡‘èª¿é”", "äº‹æ¥­è¨ˆç”»",
            
            # è‹±èªãƒ“ã‚¸ãƒã‚¹ç³»
            "ãƒ“ã‚¸ãƒã‚¹è‹±èª", "è‹±ä¼šè©± ä»•äº‹", "TOEIC ãƒ“ã‚¸ãƒã‚¹", "å›½éš›ãƒ“ã‚¸ãƒã‚¹",
            "æµ·å¤–é€²å‡º", "ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ“ã‚¸ãƒã‚¹", "å¤–å›½äºº ãƒ“ã‚¸ãƒã‚¹"
        ]
        
        found_channels = []
        seen_in_search = set()
        
        for i, query in enumerate(business_queries, 1):
            try:
                print(f"  {i:3d}. '{query}' æ¤œç´¢ä¸­...")
                
                search_request = self.service.search().list(
                    part='snippet',
                    q=query,
                    type='channel',
                    regionCode='JP',
                    relevanceLanguage='ja',
                    order='relevance',
                    maxResults=15  # ã‚ˆã‚Šå¤šãã®å€™è£œã‚’å–å¾—
                )
                
                search_response = search_request.execute()
                new_found = 0
                
                for item in search_response.get('items', []):
                    channel_id = item['id']['channelId']
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ + ä»Šå›æ¤œç´¢åˆ†ï¼‰
                    if channel_id not in self.existing_channel_ids and channel_id not in seen_in_search:
                        seen_in_search.add(channel_id)
                        found_channels.append({
                            'channel_id': channel_id,
                            'channel_title': item['snippet']['title'],
                            'description': item['snippet']['description'],
                            'query_source': query
                        })
                        new_found += 1
                
                print(f"       âœ… æ–°è¦ç™ºè¦‹: {new_found} ä»¶")
                self.stats['searched'] += len(search_response.get('items', []))
                
                # ç›®æ¨™æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
                if len(found_channels) >= target_count:
                    print(f"ğŸ¯ ç›®æ¨™æ•° {target_count} ä»¶ã«åˆ°é”ï¼æ¤œç´¢ã‚’çµ‚äº†ã—ã¾ã™")
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(0.3)
                
            except HttpError as e:
                if "quotaExceeded" in str(e):
                    print(f"âš ï¸ YouTube APIã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã«åˆ°é”ã€‚ç¾åœ¨ {len(found_channels)} ä»¶å–å¾—æ¸ˆã¿")
                    break
                else:
                    print(f"     âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                    self.stats['errors'] += 1
                    continue
        
        print(f"\nğŸ“Š æ¤œç´¢çµæœ: {len(found_channels)} ä»¶ã®æ–°è¦ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«ç™ºè¦‹")
        return found_channels[:target_count]
    
    async def get_channel_details_with_ai(self, channels: List[Dict[str, Any]], target_final: int = 50) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ãƒãƒ«è©³ç´°å–å¾— + AIåˆ†æï¼ˆãƒ“ã‚¸ãƒã‚¹ç³»ç‰¹åŒ–ï¼‰"""
        print(f"\nğŸ¤– {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°å–å¾— + AIåˆ†æä¸­ï¼ˆç›®æ¨™: {target_final} ä»¶ï¼‰...")
        
        enhanced_channels = []
        processed = 0
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ï¼ˆAPIåˆ¶é™å¯¾å¿œï¼‰
        batch_size = 50
        
        for batch_start in range(0, len(channels), batch_size):
            batch_channels = channels[batch_start:batch_start + batch_size]
            channel_ids = [ch['channel_id'] for ch in batch_channels]
            
            try:
                # ãƒãƒƒãƒã§è©³ç´°å–å¾—
                details_request = self.service.channels().list(
                    part='snippet,statistics',
                    id=','.join(channel_ids)
                )
                details_response = details_request.execute()
                
                for item in details_response.get('items', []):
                    try:
                        channel_id = item['id']
                        
                        # æœ€çµ‚é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if channel_id in self.existing_channel_ids:
                            print(f"âš ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {item['snippet']['title']}")
                            self.stats['duplicates_skipped'] += 1
                            continue
                        
                        snippet = item['snippet']
                        statistics = item['statistics']
                        
                        subscriber_count = int(statistics.get('subscriberCount', 0))
                        video_count = int(statistics.get('videoCount', 0))
                        view_count = int(statistics.get('viewCount', 0))
                        
                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼: 10K-500Kï¼‰
                        if not (10000 <= subscriber_count <= 500000):
                            print(f"ğŸ“Š ç¯„å›²å¤–ã‚¹ã‚­ãƒƒãƒ—: {snippet['title']} (ç™»éŒ²è€…: {subscriber_count:,})")
                            continue
                        
                        description = snippet.get('description', '')
                        emails = self.extract_emails_from_description(description)
                        engagement_estimate = (subscriber_count / video_count * 100) if video_count > 0 else 0
                        
                        # åŸºæœ¬ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
                        channel_data = {
                            'channel_id': channel_id,
                            'channel_title': snippet.get('title', ''),
                            'description': description,
                            'subscriber_count': subscriber_count,
                            'video_count': video_count,
                            'view_count': view_count,
                            'country': snippet.get('country', 'JP'),
                            'emails': emails,
                            'has_contact': len(emails) > 0,
                            'engagement_estimate': round(engagement_estimate, 2),
                            'collected_at': datetime.now(timezone.utc).isoformat()
                        }
                        
                        # ğŸ¤– AIåˆ†æå®Ÿè¡Œ
                        print(f"ğŸ¤– AIåˆ†æä¸­: {channel_data['channel_title']}")
                        ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
                        
                        # ãƒ“ã‚¸ãƒã‚¹ç³»ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        detected_category = ai_analysis.get('category_tags', {}).get('primary_category', '').lower()
                        business_keywords = [
                            'ãƒ“ã‚¸ãƒã‚¹', 'business', 'çµŒæ¸ˆ', 'æŠ•è³‡', 'èµ·æ¥­', 'çµŒå–¶', 
                            'æ•™è‚²', 'education', 'howto', 'finance', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°',
                            'ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°', 'ã‚­ãƒ£ãƒªã‚¢', 'è‡ªå·±å•“ç™º'
                        ]
                        
                        is_business_related = any(keyword in detected_category for keyword in business_keywords)
                        
                        if not is_business_related:
                            # ãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜ã‹ã‚‰ã‚‚ãƒ“ã‚¸ãƒã‚¹é–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                            description_lower = description.lower()
                            title_lower = channel_data['channel_title'].lower()
                            combined_text = f"{description_lower} {title_lower}"
                            
                            business_check_keywords = [
                                'ãƒ“ã‚¸ãƒã‚¹', 'business', 'èµ·æ¥­', 'çµŒå–¶', 'æŠ•è³‡', 'å‰¯æ¥­', 
                                'ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹', 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'çµŒæ¸ˆ', 'ã‚³ãƒ³ã‚µãƒ«',
                                'è³‡ç”£é‹ç”¨', 'è»¢è·', 'ã‚­ãƒ£ãƒªã‚¢', 'è‡ªå·±å•“ç™º', 'æˆåŠŸ'
                            ]
                            
                            is_business_related = any(keyword in combined_text for keyword in business_check_keywords)
                        
                        if not is_business_related:
                            print(f"ğŸ“ éãƒ“ã‚¸ãƒã‚¹ç³»ã‚¹ã‚­ãƒƒãƒ—: {channel_data['channel_title']} (ã‚«ãƒ†ã‚´ãƒª: {detected_category})")
                            continue
                        
                        # AIåˆ†æçµæœã‚’çµ±åˆ
                        enhanced_channel = {
                            **channel_data,
                            'ai_analysis': ai_analysis,
                            'category': ai_analysis.get('category_tags', {}).get('primary_category', 'æœªåˆ†é¡'),
                            'sub_categories': ai_analysis.get('category_tags', {}).get('sub_categories', []),
                            'content_themes': ai_analysis.get('category_tags', {}).get('content_themes', []),
                            'recommended_products': ai_analysis.get('product_matching', {}).get('recommended_products', []),
                            'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                            'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5)
                        }
                        
                        enhanced_channels.append(enhanced_channel)
                        self.existing_channel_ids.add(channel_id)
                        self.stats['analyzed'] += 1
                        processed += 1
                        
                        # çµæœè¡¨ç¤º
                        print(f"âœ… å®Œäº† {len(enhanced_channels):2d}: {channel_data['channel_title']}")
                        print(f"     ğŸ“Š ç™»éŒ²è€…: {subscriber_count:,}")
                        print(f"     ğŸ¯ ã‚«ãƒ†ã‚´ãƒª: {enhanced_channel['category']}")
                        print(f"     ğŸ›¡ï¸ å®‰å…¨æ€§: {enhanced_channel['brand_safety_score']:.2f}")
                        if enhanced_channel['recommended_products']:
                            top_product = enhanced_channel['recommended_products'][0]
                            print(f"     ğŸ’¼ æ¨å¥¨å•†æ: {top_product.get('category', 'N/A')}")
                        print()
                        
                        # ç›®æ¨™ã®50ãƒãƒ£ãƒ³ãƒãƒ«ã«é”ã—ãŸã‚‰çµ‚äº†
                        if len(enhanced_channels) >= target_final:
                            print(f"ğŸ¯ ç›®æ¨™ {target_final} ãƒãƒ£ãƒ³ãƒãƒ«ã«åˆ°é”ï¼åˆ†æã‚’çµ‚äº†ã—ã¾ã™")
                            return enhanced_channels
                        
                    except Exception as e:
                        print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({item.get('id', 'Unknown')}): {e}")
                        self.stats['errors'] += 1
                        continue
                
                # ãƒãƒƒãƒé–“ã®ä¼‘æ†©
                if len(enhanced_channels) < target_final and batch_start + batch_size < len(channels):
                    print(f"â±ï¸ ãƒãƒƒãƒå‡¦ç†å®Œäº†ã€‚3ç§’ä¼‘æ†©...")
                    await asyncio.sleep(3)
                
            except HttpError as e:
                if "quotaExceeded" in str(e):
                    print(f"âš ï¸ YouTube APIã‚¯ã‚©ãƒ¼ã‚¿åˆ¶é™ã€‚ç¾åœ¨ {len(enhanced_channels)} ä»¶åˆ†ææ¸ˆã¿")
                    break
                else:
                    print(f"âŒ YouTube API ã‚¨ãƒ©ãƒ¼: {e}")
                    self.stats['errors'] += 1
                    continue
        
        print(f"\nğŸ“Š æœ€çµ‚åˆ†æçµæœ: {len(enhanced_channels)} ä»¶ã®ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«")
        return enhanced_channels
    
    def convert_to_firestore_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """Firestoreå½¢å¼ã«å¤‰æ›"""
        ai_analysis = channel_data.get('ai_analysis', {})
        brand_safety = ai_analysis.get('brand_safety', {})
        category_tags = ai_analysis.get('category_tags', {})
        product_matching = ai_analysis.get('product_matching', {})
        
        return {
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'],
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_info': {
                'emails': channel_data.get('emails', []),
                'primary_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None
            },
            'engagement_metrics': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'avg_views_per_video': channel_data['view_count'] / channel_data['video_count'] if channel_data['video_count'] > 0 else 0,
                'has_contact': channel_data.get('has_contact', False)
            },
            'ai_analysis': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'content_quality_score': 0.8,
                'brand_safety_score': brand_safety.get('overall_safety_score', 0.8),
                'growth_potential': 0.7,
                'full_analysis': ai_analysis,
                'advanced': {
                    'enhanced_at': datetime.now(timezone.utc).isoformat(),
                    'category': category_tags.get('primary_category', 'æœªåˆ†é¡'),
                    'sub_categories': category_tags.get('sub_categories', []),
                    'content_themes': category_tags.get('content_themes', []),
                    'safety_score': brand_safety.get('overall_safety_score', 0.8),
                    'confidence': ai_analysis.get('analysis_confidence', 0.5),
                    'target_age': category_tags.get('target_age_group', 'ä¸æ˜'),
                    'top_product': product_matching.get('recommended_products', [{}])[0].get('category', 'æœªå®š') if product_matching.get('recommended_products') else 'æœªå®š',
                    'match_score': product_matching.get('recommended_products', [{}])[0].get('match_score', 0.5) if product_matching.get('recommended_products') else 0.5
                }
            },
            'status': 'active',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'last_analyzed': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'fetched_at': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'data_source': 'youtube_api',
            'collection_method': 'business_50_channels'
        }
    
    async def save_to_firestore(self, channels: List[Dict[str, Any]]) -> bool:
        """Firestoreã«ä¿å­˜"""
        print(f"\nğŸ”¥ Firestoreã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        success_count = 0
        
        for i, channel_data in enumerate(channels, 1):
            try:
                firestore_doc = self.convert_to_firestore_format(channel_data)
                
                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                doc_ref = self.firestore_db.collection('influencers').document(firestore_doc['channel_id'])
                existing_doc = doc_ref.get()
                
                if existing_doc.exists:
                    print(f"âš ï¸  {i:2d}. {firestore_doc['channel_title']} (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—)")
                    self.stats['duplicates_skipped'] += 1
                    continue
                
                # Firestoreã«ä¿å­˜
                doc_ref.set(firestore_doc)
                
                print(f"âœ… {i:2d}. {firestore_doc['channel_title']} (ç™»éŒ²è€…: {firestore_doc['subscriber_count']:,})")
                success_count += 1
                
            except Exception as e:
                print(f"âŒ {i:2d}. Firestoreä¿å­˜å¤±æ•— ({channel_data.get('channel_title', 'Unknown')}): {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['saved_firestore'] = success_count
        print(f"\nğŸ“Š Firestoreä¿å­˜çµæœ: {success_count}/{len(channels)} ä»¶æˆåŠŸ")
        return success_count > 0
    
    def convert_to_bigquery_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """BigQueryå½¢å¼ã«å¤‰æ›"""
        ai_analysis = channel_data.get('ai_analysis', {})
        
        # AIåˆ†ææƒ…å ±ã‚’social_linksãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã‚ã‚‹
        extended_social_links = {
            'emails': channel_data.get('emails', []),
            'ai_analysis': ai_analysis,
            'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
            'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5)
        }
        
        return {
            'influencer_id': channel_data['channel_id'],
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'][:1000] if channel_data.get('description') else '',
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None,
            'social_links': json.dumps(extended_social_links),
            'ai_analysis': json.dumps(ai_analysis),
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'is_active': True
        }
    
    async def save_to_bigquery(self, channels: List[Dict[str, Any]]) -> bool:
        """BigQueryã«ä¿å­˜"""
        print(f"\nğŸ—ï¸ BigQueryã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        try:
            dataset_id = "infumatch_data"
            table_id = "influencers"
            
            table_ref = self.bigquery_client.dataset(dataset_id).table(table_id)
            
            try:
                table = self.bigquery_client.get_table(table_ref)
                print(f"âœ… BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª: {dataset_id}.{table_id}")
            except Exception:
                print(f"âš ï¸ BigQueryãƒ†ãƒ¼ãƒ–ãƒ« {dataset_id}.{table_id} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            rows_to_insert = []
            for channel_data in channels:
                bq_row = self.convert_to_bigquery_format(channel_data)
                rows_to_insert.append(bq_row)
            
            # BigQueryã«æŒ¿å…¥
            errors = self.bigquery_client.insert_rows_json(table, rows_to_insert)
            
            if not errors:
                self.stats['saved_bigquery'] = len(rows_to_insert)
                print(f"âœ… BigQueryä¿å­˜æˆåŠŸ: {len(rows_to_insert)} ä»¶")
                return True
            else:
                print(f"âŒ BigQueryä¿å­˜ã‚¨ãƒ©ãƒ¼: {errors}")
                self.stats['errors'] += 1
                return False
                
        except Exception as e:
            print(f"âŒ BigQueryä¿å­˜å¤±æ•—: {e}")
            self.stats['errors'] += 1
            return False
    
    def save_to_json(self, channels: List[Dict[str, Any]], filename: str):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(channels, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âŒ JSONä¿å­˜å¤±æ•—: {e}")
    
    def print_results_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ 50ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ çµæœ")
        print(f"{'='*80}")
        
        print(f"ğŸ“Š çµ±è¨ˆæƒ…å ±:")
        print(f"  - æ¤œç´¢å®Ÿè¡Œ: {self.stats['searched']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {self.stats['duplicates_skipped']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - AIåˆ†æå®Œäº†: {self.stats['analyzed']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - Firestoreä¿å­˜: {self.stats['saved_firestore']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - BigQueryä¿å­˜: {self.stats['saved_bigquery']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors']}")
        
        if self.added_channels:
            # ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ
            categories = {}
            total_subscribers = 0
            avg_safety = 0
            avg_confidence = 0
            contact_count = 0
            
            for channel in self.added_channels:
                cat = channel.get('category', 'æœªåˆ†é¡')
                categories[cat] = categories.get(cat, 0) + 1
                total_subscribers += channel.get('subscriber_count', 0)
                avg_safety += channel.get('brand_safety_score', 0)
                avg_confidence += channel.get('analysis_confidence', 0)
                if channel.get('has_contact', False):
                    contact_count += 1
            
            total_count = len(self.added_channels)
            avg_safety = avg_safety / total_count if total_count > 0 else 0
            avg_confidence = avg_confidence / total_count if total_count > 0 else 0
            
            print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:")
            for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {category}: {count} ãƒãƒ£ãƒ³ãƒãƒ«")
            
            print(f"\nğŸ“ˆ å“è³ªæŒ‡æ¨™:")
            print(f"  - ç·ç™»éŒ²è€…æ•°: {total_subscribers:,} äºº")
            print(f"  - å¹³å‡å®‰å…¨æ€§ã‚¹ã‚³ã‚¢: {avg_safety:.2f}")
            print(f"  - å¹³å‡AIä¿¡é ¼åº¦: {avg_confidence:.2f}")
            print(f"  - é€£çµ¡å…ˆæœ‰ã‚Šãƒãƒ£ãƒ³ãƒãƒ«: {contact_count} / {total_count} ({contact_count/total_count*100:.1f}%)")
            
            print(f"\nğŸ“‹ è¿½åŠ ãƒãƒ£ãƒ³ãƒãƒ«:")
            for i, channel in enumerate(self.added_channels[:10], 1):  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                print(f"{i:2d}. {channel['channel_title']} (ç™»éŒ²è€…: {channel['subscriber_count']:,}, ã‚«ãƒ†ã‚´ãƒª: {channel.get('category', 'æœªåˆ†é¡')})")
            if len(self.added_channels) > 10:
                print(f"    ... ä»– {len(self.added_channels) - 10} ãƒãƒ£ãƒ³ãƒãƒ«")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    adder = BusinessChannelAdder()
    
    print("ğŸ¤– 50ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)
    print("æ©Ÿèƒ½: ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆãƒ»èµ·æ¥­ç³»ç‰¹åŒ– + AIåˆ†æ + è‡ªå‹•ç™»éŒ²")
    print()
    
    try:
        # 1. æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿
        await adder.load_existing_channel_ids()
        
        # 2. ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
        channels = await adder.search_business_channels(target_count=100)  # å¤šã‚ã«æ¤œç´¢
        
        if not channels:
            print("âŒ æ–°è¦ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 3. è©³ç´°å–å¾— + AIåˆ†æï¼ˆ50ãƒãƒ£ãƒ³ãƒãƒ«ç›®æ¨™ï¼‰
        enhanced_channels = await adder.get_channel_details_with_ai(channels, target_final=50)
        
        if not enhanced_channels:
            print("âŒ æ¡ä»¶ã«åˆã†ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        adder.added_channels = enhanced_channels
        
        # 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        await adder.save_to_firestore(enhanced_channels)
        await adder.save_to_bigquery(enhanced_channels)
        
        # 5. JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"business_50_channels_{timestamp}.json"
        adder.save_to_json(enhanced_channels, filename)
        
        # 6. çµæœè¡¨ç¤º
        adder.print_results_summary()
        
        print(f"\nğŸ‰ ãƒ“ã‚¸ãƒã‚¹ç³»50ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ˆ åˆè¨ˆç™»éŒ²ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {102 + len(enhanced_channels)} ä»¶")
        
    except Exception as e:
        print(f"âŒ è¿½åŠ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        adder.print_results_summary()

if __name__ == "__main__":
    asyncio.run(main())