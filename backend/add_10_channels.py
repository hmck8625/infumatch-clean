#!/usr/bin/env python3
"""
10ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰

@description channel_idã«ã‚ˆã‚‹é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ãªãŒã‚‰10ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¿½åŠ 
@author InfuMatch Development Team
@version 1.0.0
"""

import os
import json
import asyncio
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Set
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from google.cloud import firestore
from google.cloud import bigquery

# AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.ai_channel_analyzer import AdvancedChannelAnalyzer

# è¨­å®š
YOUTUBE_API_KEY = "AIzaSyDtPl5WSRdxk744ha5Tlwno4iTBZVO96r4"
PROJECT_ID = "hackathon-462905"

class TenChannelAdder:
    """
    10ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    """
    
    def __init__(self, api_key: str = YOUTUBE_API_KEY):
        self.api_key = api_key
        self.service = build('youtube', 'v3', developerKey=api_key)
        self.ai_analyzer = AdvancedChannelAnalyzer()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        self.firestore_db = firestore.Client(project=PROJECT_ID)
        self.bigquery_client = bigquery.Client(project=PROJECT_ID)
        
        # æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDè¿½è·¡
        self.existing_channel_ids: Set[str] = set()
        
        # åé›†ãƒ‡ãƒ¼ã‚¿
        self.added_channels = []
        self.stats = {
            'searched': 0,
            'duplicates_skipped': 0,
            'analyzed': 0,
            'saved_firestore': 0,
            'saved_bigquery': 0,
            'errors': 0
        }
    
    async def load_existing_channel_ids(self):
        """æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ” æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            # Firestoreã‹ã‚‰æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDå–å¾—
            collection_ref = self.firestore_db.collection('influencers')
            docs = collection_ref.get()
            
            for doc in docs:
                data = doc.to_dict()
                channel_id = data.get('channel_id')
                if channel_id:
                    self.existing_channel_ids.add(channel_id)
            
            print(f"âœ… æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿å®Œäº†: {len(self.existing_channel_ids)} ä»¶")
            
        except Exception as e:
            print(f"âŒ æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def extract_emails_from_description(self, description: str) -> List[str]:
        """æ¦‚è¦æ¬„ã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŠ½å‡º"""
        if not description:
            return []
        
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        emails = re.findall(email_pattern, description)
        return list(set(emails))  # é‡è¤‡é™¤å»
    
    async def search_additional_channels(self, target_count: int = 20) -> List[Dict[str, Any]]:
        """è¿½åŠ ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        print(f"\nğŸ” è¿½åŠ ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢é–‹å§‹ï¼ˆç›®æ¨™: {target_count} ä»¶ç™ºè¦‹ï¼‰")
        
        # è¿½åŠ æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆæ—¢å­˜ã¨ã¯ç•°ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        additional_queries = [
            "å¥åº· ãƒ€ã‚¤ã‚¨ãƒƒãƒˆ", "ç­‹ãƒˆãƒ¬ ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹", "ãƒ¨ã‚¬ ã‚¨ã‚¯ã‚µã‚µã‚¤ã‚º",
            "DIY æ‰‹ä½œã‚Š", "ã‚¬ãƒ¼ãƒ‡ãƒ‹ãƒ³ã‚° åœ’èŠ¸", "ãƒšãƒƒãƒˆ å‹•ç‰©",
            "ã‚¢ãƒ‹ãƒ¡ ãƒ¬ãƒ“ãƒ¥ãƒ¼", "æ˜ ç”» æ„Ÿæƒ³", "æœ¬ èª­æ›¸",
            "æ—…è¡Œ è¦³å…‰", "æ¸©æ³‰ ã‚°ãƒ«ãƒ¡", "ã‚«ãƒ•ã‚§ ã‚¹ã‚¤ãƒ¼ãƒ„",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚° æŠ€è¡“", "è‹±èª å­¦ç¿’", "è³‡æ ¼ å‹‰å¼·"
        ]
        
        found_channels = []
        seen_in_search = set()
        
        for i, query in enumerate(additional_queries, 1):
            try:
                print(f"  {i:2d}. '{query}' æ¤œç´¢ä¸­...")
                
                search_request = self.service.search().list(
                    part='snippet',
                    q=query,
                    type='channel',
                    regionCode='JP',
                    relevanceLanguage='ja',
                    order='relevance',
                    maxResults=10
                )
                
                search_response = search_request.execute()
                new_found = 0
                
                for item in search_response.get('items', []):
                    channel_id = item['id']['channelId']
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢å­˜ + ä»Šå›æ¤œç´¢åˆ†ï¼‰
                    if channel_id not in self.existing_channel_ids and channel_id not in seen_in_search:
                        seen_in_search.add(channel_id)
                        found_channels.append({
                            'channel_id': channel_id,
                            'channel_title': item['snippet']['title'],
                            'description': item['snippet']['description'],
                            'query_source': query
                        })
                        new_found += 1
                
                print(f"     âœ… æ–°è¦ç™ºè¦‹: {new_found} ä»¶")
                self.stats['searched'] += len(search_response.get('items', []))
                
                # ç›®æ¨™æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
                if len(found_channels) >= target_count:
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                await asyncio.sleep(0.5)
                
            except HttpError as e:
                print(f"     âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                self.stats['errors'] += 1
                continue
        
        print(f"\nğŸ“Š æ¤œç´¢çµæœ: {len(found_channels)} ä»¶ã®æ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«ç™ºè¦‹")
        return found_channels[:target_count]
    
    async def get_channel_details_with_ai(self, channels: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ãƒãƒ«è©³ç´°å–å¾— + AIåˆ†æï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        print(f"\nğŸ¤– {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã®è©³ç´°å–å¾— + AIåˆ†æä¸­...")
        
        enhanced_channels = []
        channel_ids = [ch['channel_id'] for ch in channels]
        
        try:
            # ãƒãƒƒãƒã§è©³ç´°å–å¾—
            details_request = self.service.channels().list(
                part='snippet,statistics',
                id=','.join(channel_ids[:50])
            )
            details_response = details_request.execute()
            
            for item in details_response.get('items', []):
                try:
                    channel_id = item['id']
                    
                    # æœ€çµ‚é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if channel_id in self.existing_channel_ids:
                        print(f"âš ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {item['snippet']['title']}")
                        self.stats['duplicates_skipped'] += 1
                        continue
                    
                    snippet = item['snippet']
                    statistics = item['statistics']
                    
                    subscriber_count = int(statistics.get('subscriberCount', 0))
                    video_count = int(statistics.get('videoCount', 0))
                    view_count = int(statistics.get('viewCount', 0))
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒã‚¤ã‚¯ãƒ­ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼: 10K-500Kï¼‰
                    if not (10000 <= subscriber_count <= 500000):
                        print(f"ğŸ“Š ç¯„å›²å¤–ã‚¹ã‚­ãƒƒãƒ—: {snippet['title']} (ç™»éŒ²è€…: {subscriber_count:,})")
                        continue
                    
                    description = snippet.get('description', '')
                    emails = self.extract_emails_from_description(description)
                    engagement_estimate = (subscriber_count / video_count * 100) if video_count > 0 else 0
                    
                    # åŸºæœ¬ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿
                    channel_data = {
                        'channel_id': channel_id,
                        'channel_title': snippet.get('title', ''),
                        'description': description,
                        'subscriber_count': subscriber_count,
                        'video_count': video_count,
                        'view_count': view_count,
                        'country': snippet.get('country', 'JP'),
                        'emails': emails,
                        'has_contact': len(emails) > 0,
                        'engagement_estimate': round(engagement_estimate, 2),
                        'collected_at': datetime.now(timezone.utc).isoformat()
                    }
                    
                    # ğŸ¤– AIåˆ†æå®Ÿè¡Œ
                    print(f"ğŸ¤– AIåˆ†æä¸­: {channel_data['channel_title']}")
                    ai_analysis = await self.ai_analyzer.analyze_channel_comprehensive(channel_data)
                    
                    # AIåˆ†æçµæœã‚’çµ±åˆ
                    enhanced_channel = {
                        **channel_data,
                        'ai_analysis': ai_analysis,
                        'category': ai_analysis.get('category_tags', {}).get('primary_category', 'æœªåˆ†é¡'),
                        'sub_categories': ai_analysis.get('category_tags', {}).get('sub_categories', []),
                        'content_themes': ai_analysis.get('category_tags', {}).get('content_themes', []),
                        'recommended_products': ai_analysis.get('product_matching', {}).get('recommended_products', []),
                        'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
                        'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5)
                    }
                    
                    enhanced_channels.append(enhanced_channel)
                    self.existing_channel_ids.add(channel_id)  # è¿½åŠ æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
                    self.stats['analyzed'] += 1
                    
                    # çµæœè¡¨ç¤º
                    print(f"âœ… å®Œäº†: {channel_data['channel_title']}")
                    print(f"   ğŸ“Š ç™»éŒ²è€…: {subscriber_count:,}")
                    print(f"   ğŸ¯ ã‚«ãƒ†ã‚´ãƒª: {enhanced_channel['category']}")
                    print(f"   ğŸ›¡ï¸ å®‰å…¨æ€§: {enhanced_channel['brand_safety_score']:.2f}")
                    print()
                    
                    # ç›®æ¨™ã®10ãƒãƒ£ãƒ³ãƒãƒ«ã«é”ã—ãŸã‚‰çµ‚äº†
                    if len(enhanced_channels) >= 10:
                        break
                    
                except Exception as e:
                    print(f"âŒ ãƒãƒ£ãƒ³ãƒãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({item.get('id', 'Unknown')}): {e}")
                    self.stats['errors'] += 1
                    continue
            
            return enhanced_channels
            
        except HttpError as e:
            print(f"âŒ YouTube API ã‚¨ãƒ©ãƒ¼: {e}")
            self.stats['errors'] += 1
            return []
    
    def convert_to_firestore_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """Firestoreå½¢å¼ã«å¤‰æ›"""
        ai_analysis = channel_data.get('ai_analysis', {})
        brand_safety = ai_analysis.get('brand_safety', {})
        category_tags = ai_analysis.get('category_tags', {})
        product_matching = ai_analysis.get('product_matching', {})
        
        return {
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'],
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_info': {
                'emails': channel_data.get('emails', []),
                'primary_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None
            },
            'engagement_metrics': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'avg_views_per_video': channel_data['view_count'] / channel_data['video_count'] if channel_data['video_count'] > 0 else 0,
                'has_contact': channel_data.get('has_contact', False)
            },
            'ai_analysis': {
                'engagement_rate': channel_data.get('engagement_estimate', 0) / 100,
                'content_quality_score': 0.8,
                'brand_safety_score': brand_safety.get('overall_safety_score', 0.8),
                'growth_potential': 0.7,
                'full_analysis': ai_analysis,
                'advanced': {
                    'enhanced_at': datetime.now(timezone.utc).isoformat(),
                    'category': category_tags.get('primary_category', 'æœªåˆ†é¡'),
                    'sub_categories': category_tags.get('sub_categories', []),
                    'content_themes': category_tags.get('content_themes', []),
                    'safety_score': brand_safety.get('overall_safety_score', 0.8),
                    'confidence': ai_analysis.get('analysis_confidence', 0.5),
                    'target_age': category_tags.get('target_age_group', 'ä¸æ˜'),
                    'top_product': product_matching.get('recommended_products', [{}])[0].get('category', 'æœªå®š') if product_matching.get('recommended_products') else 'æœªå®š',
                    'match_score': product_matching.get('recommended_products', [{}])[0].get('match_score', 0.5) if product_matching.get('recommended_products') else 0.5
                }
            },
            'status': 'active',
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'last_analyzed': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'fetched_at': channel_data.get('collected_at', datetime.now(timezone.utc).isoformat()),
            'data_source': 'youtube_api',
            'collection_method': 'additional_10_channels'
        }
    
    async def save_to_firestore(self, channels: List[Dict[str, Any]]) -> bool:
        """Firestoreã«ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        print(f"\nğŸ”¥ Firestoreã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        success_count = 0
        
        for i, channel_data in enumerate(channels, 1):
            try:
                firestore_doc = self.convert_to_firestore_format(channel_data)
                
                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€çµ‚ç¢ºèªï¼‰
                doc_ref = self.firestore_db.collection('influencers').document(firestore_doc['channel_id'])
                existing_doc = doc_ref.get()
                
                if existing_doc.exists:
                    print(f"âš ï¸  {i:2d}. {firestore_doc['channel_title']} (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—)")
                    self.stats['duplicates_skipped'] += 1
                    continue
                
                # Firestoreã«ä¿å­˜
                doc_ref.set(firestore_doc)
                
                print(f"âœ… {i:2d}. {firestore_doc['channel_title']} (ç™»éŒ²è€…: {firestore_doc['subscriber_count']:,})")
                success_count += 1
                
            except Exception as e:
                print(f"âŒ {i:2d}. Firestoreä¿å­˜å¤±æ•— ({channel_data.get('channel_title', 'Unknown')}): {e}")
                self.stats['errors'] += 1
                continue
        
        self.stats['saved_firestore'] = success_count
        print(f"\nğŸ“Š Firestoreä¿å­˜çµæœ: {success_count}/{len(channels)} ä»¶æˆåŠŸ")
        return success_count > 0
    
    def convert_to_bigquery_format(self, channel_data: Dict[str, Any]) -> Dict[str, Any]:
        """BigQueryå½¢å¼ã«å¤‰æ›ï¼ˆç¾åœ¨ã®ã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ã¦ï¼‰"""
        ai_analysis = channel_data.get('ai_analysis', {})
        
        # AIåˆ†ææƒ…å ±ã‚’social_linksãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å«ã‚ã‚‹ï¼ˆJSONå½¢å¼ï¼‰
        extended_social_links = {
            'emails': channel_data.get('emails', []),
            'ai_analysis': ai_analysis,
            'brand_safety_score': ai_analysis.get('brand_safety', {}).get('overall_safety_score', 0.8),
            'analysis_confidence': ai_analysis.get('analysis_confidence', 0.5)
        }
        
        return {
            'influencer_id': channel_data['channel_id'],
            'channel_id': channel_data['channel_id'],
            'channel_title': channel_data['channel_title'],
            'description': channel_data['description'][:1000] if channel_data.get('description') else '',
            'subscriber_count': channel_data['subscriber_count'],
            'video_count': channel_data['video_count'],
            'view_count': channel_data['view_count'],
            'category': channel_data.get('category', 'æœªåˆ†é¡'),
            'country': channel_data.get('country', 'JP'),
            'language': 'ja',
            'contact_email': channel_data.get('emails', [None])[0] if channel_data.get('emails') else None,
            'social_links': json.dumps(extended_social_links),
            'ai_analysis': json.dumps(ai_analysis),  # å°‚ç”¨ã®ai_analysisãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            'created_at': datetime.now(timezone.utc).isoformat(),
            'updated_at': datetime.now(timezone.utc).isoformat(),
            'is_active': True
        }
    
    async def save_to_bigquery(self, channels: List[Dict[str, Any]]) -> bool:
        """BigQueryã«ä¿å­˜"""
        print(f"\nğŸ—ï¸ BigQueryã« {len(channels)} ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä¿å­˜ä¸­...")
        
        try:
            dataset_id = "infumatch_data"
            table_id = "influencers"
            
            table_ref = self.bigquery_client.dataset(dataset_id).table(table_id)
            
            try:
                table = self.bigquery_client.get_table(table_ref)
                print(f"âœ… BigQueryãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª: {dataset_id}.{table_id}")
            except Exception:
                print(f"âš ï¸ BigQueryãƒ†ãƒ¼ãƒ–ãƒ« {dataset_id}.{table_id} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return False
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            rows_to_insert = []
            for channel_data in channels:
                bq_row = self.convert_to_bigquery_format(channel_data)
                rows_to_insert.append(bq_row)
            
            # BigQueryã«æŒ¿å…¥
            errors = self.bigquery_client.insert_rows_json(table, rows_to_insert)
            
            if not errors:
                self.stats['saved_bigquery'] = len(rows_to_insert)
                print(f"âœ… BigQueryä¿å­˜æˆåŠŸ: {len(rows_to_insert)} ä»¶")
                return True
            else:
                print(f"âŒ BigQueryä¿å­˜ã‚¨ãƒ©ãƒ¼: {errors}")
                self.stats['errors'] += 1
                return False
                
        except Exception as e:
            print(f"âŒ BigQueryä¿å­˜å¤±æ•—: {e}")
            self.stats['errors'] += 1
            return False
    
    def save_to_json(self, channels: List[Dict[str, Any]], filename: str):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(channels, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âŒ JSONä¿å­˜å¤±æ•—: {e}")
    
    def print_results_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"\n{'='*80}")
        print(f"ğŸ¯ 10ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ çµæœ")
        print(f"{'='*80}")
        
        print(f"ğŸ“Š çµ±è¨ˆæƒ…å ±:")
        print(f"  - æ¤œç´¢å®Ÿè¡Œ: {self.stats['searched']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {self.stats['duplicates_skipped']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - AIåˆ†æå®Œäº†: {self.stats['analyzed']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - Firestoreä¿å­˜: {self.stats['saved_firestore']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - BigQueryä¿å­˜: {self.stats['saved_bigquery']} ãƒãƒ£ãƒ³ãƒãƒ«")
        print(f"  - ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors']}")
        
        if self.added_channels:
            print(f"\nğŸ“‹ è¿½åŠ ãƒãƒ£ãƒ³ãƒãƒ«:")
            for i, channel in enumerate(self.added_channels, 1):
                print(f"{i:2d}. {channel['channel_title']} (ç™»éŒ²è€…: {channel['subscriber_count']:,}, ã‚«ãƒ†ã‚´ãƒª: {channel.get('category', 'æœªåˆ†é¡')})")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    adder = TenChannelAdder()
    
    print("ğŸ¤– 10ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰")
    print("=" * 60)
    print("æ©Ÿèƒ½: channel_idé‡è¤‡ãƒã‚§ãƒƒã‚¯ + AIåˆ†æ + è‡ªå‹•ç™»éŒ²")
    print()
    
    try:
        # 1. æ—¢å­˜ãƒãƒ£ãƒ³ãƒãƒ«IDèª­ã¿è¾¼ã¿
        await adder.load_existing_channel_ids()
        
        # 2. è¿½åŠ ãƒãƒ£ãƒ³ãƒãƒ«æ¤œç´¢
        channels = await adder.search_additional_channels(target_count=20)
        
        if not channels:
            print("âŒ æ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 3. è©³ç´°å–å¾— + AIåˆ†æï¼ˆ10ãƒãƒ£ãƒ³ãƒãƒ«ã¾ã§ï¼‰
        enhanced_channels = await adder.get_channel_details_with_ai(channels)
        
        if not enhanced_channels:
            print("âŒ æ¡ä»¶ã«åˆã†ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        adder.added_channels = enhanced_channels
        
        # 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
        await adder.save_to_firestore(enhanced_channels)
        await adder.save_to_bigquery(enhanced_channels)
        
        # 5. JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"additional_10_channels_{timestamp}.json"
        adder.save_to_json(enhanced_channels, filename)
        
        # 6. çµæœè¡¨ç¤º
        adder.print_results_summary()
        
        print(f"\nğŸ‰ 10ãƒãƒ£ãƒ³ãƒãƒ«è¿½åŠ ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        print(f"âŒ è¿½åŠ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        adder.print_results_summary()

if __name__ == "__main__":
    asyncio.run(main())