# ğŸ—ï¸ InfuMatch ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½](#å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½)
4. [ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †](#ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †)
5. [ãƒ‡ãƒ¼ã‚¿ç™»éŒ²æ–¹æ³•](#ãƒ‡ãƒ¼ã‚¿ç™»éŒ²æ–¹æ³•)
6. [ãƒ‡ãƒ¼ã‚¿åŒæœŸæ–¹æ³•](#ãƒ‡ãƒ¼ã‚¿åŒæœŸæ–¹æ³•)
7. [API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ](#api-ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ)
8. [åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½](#åˆ†æãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½)
9. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)
10. [ä¿å®ˆãƒ»é‹ç”¨](#ä¿å®ˆé‹ç”¨)

---

## ğŸ“Š æ¦‚è¦

InfuMatch ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¯ã€YouTube ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã«ç‰¹åŒ–ã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿åŸºç›¤ã§ã™ã€‚Google Cloud ã® Firestoreï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ï¼‰ã¨ BigQueryï¼ˆåˆ†æãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸã€é«˜æ€§èƒ½ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

### ğŸ¯ ä¸»è¦ç‰¹å¾´

- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§**: Firestore ã«ã‚ˆã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: BigQuery ã«ã‚ˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åˆ†æ
- **è‡ªå‹•åŒæœŸ**: Firestore â†” BigQuery é–“ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿åŒæœŸ
- **AI çµ±åˆ**: Gemini API ã¨ã®çµ±åˆã«ã‚ˆã‚‹é«˜åº¦ãªåˆ†æ
- **RESTful API**: æ¨™æº–çš„ãª API ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

---

## ğŸ›ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Backend API   â”‚    â”‚  Google Cloud   â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚â—„â”€â”€â–ºâ”‚   Services      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Data Integrationâ”‚    â”‚   Firestore     â”‚
                       â”‚    Service      â”‚â—„â”€â”€â–ºâ”‚ (Real-time DB)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    BigQuery     â”‚    â”‚  Analytics &    â”‚
                       â”‚ (Data Warehouse)â”‚â—„â”€â”€â–ºâ”‚   Reporting     â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ | å½¹å‰² |
|----------|-------------|------|
| **Frontend** | Next.js 14, TypeScript | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ |
| **Backend** | FastAPI, Python 3.11+ | API ã‚µãƒ¼ãƒãƒ¼ã€ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ |
| **Real-time DB** | Google Firestore | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã€CRUDæ“ä½œ |
| **Data Warehouse** | Google BigQuery | å¤§è¦æ¨¡åˆ†æã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ |
| **Integration** | è‡ªä½œ ETL Service | ãƒ‡ãƒ¼ã‚¿åŒæœŸã€å¤‰æ›å‡¦ç† |
| **AI/ML** | Vertex AI, Gemini API | è‡ªç„¶è¨€èªå‡¦ç†ã€åˆ†æå¼·åŒ– |

---

## âš¡ å®Ÿè£…æ¸ˆã¿æ©Ÿèƒ½

### 1. ğŸ”¥ Firestore æ©Ÿèƒ½ (`backend/core/database.py`)

#### **FirestoreClient ã‚¯ãƒ©ã‚¹**
- ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªæ¥ç¶šç®¡ç†
- èªè¨¼æƒ…å ±ã®è‡ªå‹•èª­ã¿è¾¼ã¿
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°æ©Ÿèƒ½

#### **DatabaseHelper ã‚¯ãƒ©ã‚¹**
```python
from core.database import DatabaseHelper, DatabaseCollections

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
db_helper = DatabaseHelper()

# åŸºæœ¬çš„ãªCRUDæ“ä½œ
await db_helper.create_document(collection, doc_id, data)
await db_helper.get_document(collection, doc_id)
await db_helper.update_document(collection, doc_id, updates)
await db_helper.delete_document(collection, doc_id)

# è¤‡åˆã‚¯ã‚¨ãƒª
conditions = [('status', '==', 'active'), ('category', '==', 'beauty')]
results = await db_helper.query_documents(collection, conditions)
```

#### **å®šç¾©æ¸ˆã¿ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³**
- `INFLUENCERS`: ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æƒ…å ±
- `COMPANIES`: ä¼æ¥­æƒ…å ±
- `CAMPAIGNS`: ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æƒ…å ±  
- `NEGOTIATIONS`: äº¤æ¸‰å±¥æ­´
- `MESSAGES`: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´
- `ANALYTICS`: åˆ†æãƒ‡ãƒ¼ã‚¿

### 2. ğŸ—ï¸ BigQuery æ©Ÿèƒ½ (`backend/core/bigquery_client.py`)

#### **BigQueryClient ã‚¯ãƒ©ã‚¹**
```python
from core.bigquery_client import get_bigquery_client

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå–å¾—
bq_client = get_bigquery_client()

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
bq_client.setup_all_tables()

# ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
rows = [{'influencer_id': 'test123', 'channel_title': 'Test Channel'}]
bq_client.insert_rows('influencers', rows)

# ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
sql = "SELECT COUNT(*) as total FROM `project.dataset.influencers`"
results = bq_client.query_to_dataframe(sql)
```

#### **å®šç¾©æ¸ˆã¿ãƒ†ãƒ¼ãƒ–ãƒ«**
- `influencers`: ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åŸºæœ¬æƒ…å ±
- `influencer_analytics`: åˆ†æãƒ¡ãƒˆãƒªã‚¯ã‚¹
- `video_performance`: å‹•ç”»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- `campaigns`: ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æƒ…å ±
- `negotiations`: äº¤æ¸‰ãƒ‡ãƒ¼ã‚¿
- `daily_metrics`: æ—¥æ¬¡é›†è¨ˆãƒ‡ãƒ¼ã‚¿

### 3. ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆæ©Ÿèƒ½ (`backend/services/data_integration.py`)

#### **DataIntegrationService ã‚¯ãƒ©ã‚¹**
```python
from services.data_integration import get_data_integration_service

# ã‚µãƒ¼ãƒ“ã‚¹å–å¾—
integration = get_data_integration_service()

# å®Œå…¨åŒæœŸ
result = await integration.full_sync()

# å€‹åˆ¥åŒæœŸ
await integration.sync_influencers_to_bigquery()
await integration.sync_campaigns_to_bigquery()

# æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
await integration.generate_daily_metrics()
```

---

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### 1. ç’°å¢ƒè¨­å®š

#### **å¿…è¦ãªç’°å¢ƒå¤‰æ•°** (`.env.local`)
```bash
# Google Cloud è¨­å®š
GOOGLE_CLOUD_PROJECT_ID=hackathon-462905
GOOGLE_APPLICATION_CREDENTIALS=./hackathon-462905-fd4f661125e5.json

# Firestore è¨­å®š  
FIRESTORE_DATABASE_ID=local

# BigQuery è¨­å®š
BIGQUERY_DATASET=infumatch_data

# API ã‚­ãƒ¼
YOUTUBE_API_KEY=your_youtube_api_key
GEMINI_API_KEY=your_gemini_api_key
```

#### **Google Cloud èªè¨¼**
```bash
# Google Cloud CLI ã«ãƒ­ã‚°ã‚¤ãƒ³
gcloud auth login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
gcloud config set project hackathon-462905

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼
gcloud auth application-default login
```

### 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–

#### **è‡ªå‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**
```bash
# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd backend

# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
python scripts/test_bigquery_setup.py

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆåˆå›èµ·å‹•æ™‚ã«è‡ªå‹•åˆæœŸåŒ–ï¼‰
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

#### **æ‰‹å‹•ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**
```python
# BigQuery ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
from core.bigquery_client import initialize_bigquery
await initialize_bigquery()

# Firestore åˆæœŸåŒ–
from core.database import get_firestore_client
firestore_client = get_firestore_client()
```

### 3. æ¥ç¶šç¢ºèª

#### **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ API**
```bash
# ãƒ‡ãƒ¼ã‚¿åŸºç›¤ã®çŠ¶æ…‹ç¢ºèª
curl http://localhost:8000/api/v1/data/health

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹
{
  "status": "healthy",
  "timestamp": "2024-06-14T10:30:00Z",
  "services": {
    "firestore": "connected",
    "bigquery": "connected"
  }
}
```

---

## ğŸ“ ãƒ‡ãƒ¼ã‚¿ç™»éŒ²æ–¹æ³•

### 1. ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ç™»éŒ²

#### **API çµŒç”±ã§ã®ç™»éŒ²**
```bash
curl -X POST "http://localhost:8000/api/v2/influencers" \
  -H "Content-Type: application/json" \
  -d '{
    "channel_id": "UC1234567890",
    "channel_title": "ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ãƒãƒ«",
    "description": "ãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜",
    "subscriber_count": 50000,
    "view_count": 1000000,
    "video_count": 100,
    "category": "beauty",
    "country": "JP",
    "language": "ja",
    "contact_info": {
      "email": "contact@example.com"
    }
  }'
```

#### **ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµŒç”±ã§ã®ç™»éŒ²**
```python
from core.database import DatabaseHelper, DatabaseCollections
from datetime import datetime, timezone

# ãƒ‡ãƒ¼ã‚¿ãƒ˜ãƒ«ãƒ‘ãƒ¼å–å¾—
db_helper = DatabaseHelper()

# ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿æº–å‚™
influencer_data = {
    'channel_id': 'UC1234567890',
    'channel_title': 'ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ãƒãƒ«',
    'description': 'ãƒãƒ£ãƒ³ãƒãƒ«èª¬æ˜æ–‡',
    'subscriber_count': 50000,
    'view_count': 1000000,
    'video_count': 100,
    'category': 'beauty',
    'country': 'JP',
    'language': 'ja',
    'contact_info': {
        'email': 'contact@example.com',
        'social_links': {
            'twitter': '@example',
            'instagram': '@example'
        }
    },
    'ai_analysis': {
        'engagement_rate': 0.045,
        'content_quality_score': 0.8,
        'brand_safety_score': 0.9
    },
    'status': 'active',
    'created_at': datetime.now(timezone.utc).isoformat(),
    'updated_at': datetime.now(timezone.utc).isoformat()
}

# Firestore ã«ä¿å­˜
await db_helper.create_document(
    collection=DatabaseCollections.INFLUENCERS,
    document_id=influencer_data['channel_id'],
    data=influencer_data
)

print(f"âœ… ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ {influencer_data['channel_title']} ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
```

### 2. ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ç™»éŒ²

#### **API çµŒç”±ã§ã®ç™»éŒ²**
```bash
curl -X POST "http://localhost:8000/api/v1/campaigns" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "å¤ã®ã‚³ã‚¹ãƒ¡ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
    "description": "æ–°ä½œã‚³ã‚¹ãƒ¡ã® PR ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
    "company_id": "company_123",
    "budget": 500000,
    "target_category": "beauty",
    "target_demographics": {
      "age_range": "20-35",
      "gender": "female",
      "interests": ["beauty", "fashion"]
    },
    "requirements": {
      "min_subscribers": 10000,
      "min_engagement_rate": 0.03,
      "content_type": "video_review"
    },
    "start_date": "2024-07-01",
    "end_date": "2024-08-31"
  }'
```

#### **ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµŒç”±ã§ã®ç™»éŒ²**
```python
import shortuuid
from datetime import datetime, timezone

# ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿æº–å‚™
campaign_data = {
    'campaign_id': f"camp_{shortuuid.uuid()}",
    'title': 'å¤ã®ã‚³ã‚¹ãƒ¡ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
    'description': 'æ–°ä½œã‚³ã‚¹ãƒ¡ã® PR ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³',
    'company_id': 'company_123',
    'budget': 500000,
    'target_category': 'beauty',
    'target_demographics': {
        'age_range': '20-35',
        'gender': 'female',
        'interests': ['beauty', 'fashion']
    },
    'requirements': {
        'min_subscribers': 10000,
        'min_engagement_rate': 0.03,
        'content_type': 'video_review'
    },
    'status': 'active',
    'start_date': '2024-07-01',
    'end_date': '2024-08-31',
    'created_at': datetime.now(timezone.utc).isoformat(),
    'updated_at': datetime.now(timezone.utc).isoformat()
}

# Firestore ã«ä¿å­˜
await db_helper.create_document(
    collection=DatabaseCollections.CAMPAIGNS,
    document_id=campaign_data['campaign_id'],
    data=campaign_data
)

print(f"âœ… ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ {campaign_data['title']} ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
```

### 3. ãƒãƒƒãƒã§ã®ãƒ‡ãƒ¼ã‚¿ç™»éŒ²

#### **YouTube API çµŒç”±ã§ã®ä¸€æ‹¬ç™»éŒ²**
```python
from services.batch_processor import YouTubeBatchProcessor

# ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–
processor = YouTubeBatchProcessor()

# ã‚«ãƒ†ã‚´ãƒªãƒ™ãƒ¼ã‚¹ã§ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’ä¸€æ‹¬ç™ºè¦‹ãƒ»ç™»éŒ²
result = await processor.discover_influencers_batch(
    categories=['beauty', 'gaming', 'cooking', 'tech'],
    max_per_category=50  # ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šæœ€å¤§50ä»¶
)

print(f"âœ… {result['total_discovered']} äººã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’ç™ºè¦‹ãƒ»ç™»éŒ²ã—ã¾ã—ãŸ")
```

#### **CSV ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ä¸€æ‹¬ç™»éŒ²**
```python
import pandas as pd

# CSV ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
df = pd.read_csv('influencers.csv')

# å„è¡Œã‚’Firestoreã«ç™»éŒ²
for _, row in df.iterrows():
    influencer_data = {
        'channel_id': row['channel_id'],
        'channel_title': row['channel_title'],
        'subscriber_count': int(row['subscriber_count']),
        'category': row['category'],
        'status': 'active',
        'created_at': datetime.now(timezone.utc).isoformat(),
        'updated_at': datetime.now(timezone.utc).isoformat()
    }
    
    await db_helper.create_document(
        collection=DatabaseCollections.INFLUENCERS,
        document_id=row['channel_id'],
        data=influencer_data
    )

print(f"âœ… {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ç™»éŒ²ã—ã¾ã—ãŸ")
```

---

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿åŒæœŸæ–¹æ³•

### 1. è‡ªå‹•åŒæœŸï¼ˆæ¨å¥¨ï¼‰

#### **æ—¥æ¬¡è‡ªå‹•åŒæœŸã®è¨­å®š**
```python
# Cloud Scheduler ã¾ãŸã¯ Cron ã§ã®å®Ÿè¡Œ
from services.data_integration import run_daily_sync

# æ—¥æ¬¡åŒæœŸå®Ÿè¡Œï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼‰
result = await run_daily_sync()

print(f"ğŸ“Š åŒæœŸå®Œäº†:")
print(f"  - åŒæœŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {result['discovery']['total_discovered']}")
print(f"  - æ›´æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {result['updates']['successfully_updated']}")
print(f"  - å‡¦ç†æ™‚é–“: {result['duration_seconds']:.2f}ç§’")
```

#### **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åŒæœŸãƒˆãƒªã‚¬ãƒ¼**
```python
# Firestore æ›´æ–°æ™‚ã®ãƒˆãƒªã‚¬ãƒ¼è¨­å®š
from google.cloud import firestore

def on_influencer_update(doc_snapshot, changes, read_time):
    """Firestore ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°æ™‚ã®åŒæœŸå‡¦ç†"""
    for change in changes:
        if change.type.name in ('ADDED', 'MODIFIED'):
            # BigQuery ã¸ã®åŒæœŸã‚’ãƒˆãƒªã‚¬ãƒ¼
            asyncio.create_task(sync_single_document(change.document))

# Firestore ãƒªã‚¹ãƒŠãƒ¼è¨­å®š
firestore_client.collection('influencers').on_snapshot(on_influencer_update)
```

### 2. æ‰‹å‹•åŒæœŸ

#### **API çµŒç”±ã§ã®æ‰‹å‹•åŒæœŸ**
```bash
# å®Œå…¨åŒæœŸï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
curl -X POST "http://localhost:8000/api/v1/data/sync/full"

# ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ã¿åŒæœŸ
curl -X POST "http://localhost:8000/api/v1/data/sync/influencers?batch_size=100"

# ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿åŒæœŸ
curl -X POST "http://localhost:8000/api/v1/data/sync/campaigns?batch_size=50"

# æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
curl -X POST "http://localhost:8000/api/v1/data/metrics/generate"
```

#### **ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµŒç”±ã§ã®æ‰‹å‹•åŒæœŸ**
```python
from services.data_integration import get_data_integration_service

# ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚µãƒ¼ãƒ“ã‚¹å–å¾—
integration = get_data_integration_service()

# ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿åŒæœŸ
result = await integration.sync_influencers_to_bigquery(batch_size=100)
print(f"âœ… {result['synced_count']} ä»¶ã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸ")

# ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿åŒæœŸ
result = await integration.sync_campaigns_to_bigquery(batch_size=50)
print(f"âœ… {result['synced_count']} ä»¶ã®ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åŒæœŸ")

# æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
from datetime import datetime
result = await integration.generate_daily_metrics(
    target_date=datetime.now()
)
print(f"âœ… {result['metrics_generated']} ä»¶ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç”Ÿæˆ")
```

### 3. å·®åˆ†åŒæœŸ

#### **æœ€çµ‚æ›´æ–°æ—¥æ™‚ã«ã‚ˆã‚‹å·®åˆ†åŒæœŸ**
```python
from datetime import datetime, timedelta

# éå»24æ™‚é–“ã®æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿åŒæœŸ
cutoff_time = datetime.now() - timedelta(hours=24)

# æ›´æ–°ã•ã‚ŒãŸã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’å–å¾—
updated_influencers = await db_helper.query_documents(
    collection=DatabaseCollections.INFLUENCERS,
    conditions=[('updated_at', '>=', cutoff_time.isoformat())],
    limit=1000
)

print(f"ğŸ“Š éå»24æ™‚é–“ã§ {len(updated_influencers)} ä»¶ã®æ›´æ–°ã‚’æ¤œå‡º")

# BigQuery ã«å·®åˆ†åŒæœŸ
if updated_influencers:
    # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
    bigquery_rows = []
    for influencer in updated_influencers:
        row = integration._convert_influencer_to_bigquery_format(influencer)
        if row:
            bigquery_rows.append(row)
    
    # BigQuery ã«æŒ¿å…¥ï¼ˆMERGE æ–‡ã§é‡è¤‡å‡¦ç†ï¼‰
    bq_client.insert_rows('influencers', bigquery_rows)
    print(f"âœ… {len(bigquery_rows)} ä»¶ã®å·®åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’ BigQuery ã«åŒæœŸ")
```

### 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§

#### **Firestore ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**
```bash
# gcloud CLI ã§ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
gcloud firestore export gs://your-backup-bucket/firestore-backup \
  --project=hackathon-462905 \
  --database=local
```

#### **BigQuery ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**
```python
# BigQuery ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
from google.cloud import bigquery

client = bigquery.Client()

# ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ Cloud Storage ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
extract_job = client.extract_table(
    'hackathon-462905.infumatch_data.influencers',
    'gs://your-backup-bucket/bigquery-backup/influencers.csv'
)
extract_job.result()  # å®Œäº†ã¾ã§å¾…æ©Ÿ
```

---

## ğŸ”Œ API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### ãƒ‡ãƒ¼ã‚¿åŒæœŸ API

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|---------------|---------|------|----------|
| `/api/v1/data/sync/full` | POST | å®Œå…¨ãƒ‡ãƒ¼ã‚¿åŒæœŸ | - |
| `/api/v1/data/sync/influencers` | POST | ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åŒæœŸ | `batch_size` |
| `/api/v1/data/sync/campaigns` | POST | ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åŒæœŸ | `batch_size` |
| `/api/v1/data/metrics/generate` | POST | æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ | `target_date` |
| `/api/v1/data/sync/status` | GET | åŒæœŸçŠ¶æ³ç¢ºèª | - |
| `/api/v1/data/health` | GET | ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ | - |

### åˆ†æ API

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|---------------|---------|------|----------|
| `/api/v1/data/analytics/overview` | GET | åˆ†ææ¦‚è¦å–å¾— | `days` |
| `/api/v1/data/analytics/categories` | GET | ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ | - |
| `/api/v1/data/analytics/growth-trends` | GET | æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ | `days` |

### ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ API (v2)

| ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ | ãƒ¡ã‚½ãƒƒãƒ‰ | èª¬æ˜ | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ |
|---------------|---------|------|----------|
| `/api/v2/influencers/search` | GET | é«˜åº¦ãªæ¤œç´¢ | å„ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ |
| `/api/v2/influencers/discover` | POST | æ–°è¦ç™ºè¦‹ | `query`, `category` |
| `/api/v2/influencers/batch-discovery` | POST | ä¸€æ‹¬ç™ºè¦‹ | `categories`, `max_results` |
| `/api/v2/influencers/{id}/analyze` | POST | AIåˆ†æå®Ÿè¡Œ | - |

### ä½¿ç”¨ä¾‹

#### **å®Œå…¨åŒæœŸã®å®Ÿè¡Œ**
```bash
curl -X POST "http://localhost:8000/api/v1/data/sync/full" \
  -H "Content-Type: application/json"

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹
{
  "success": true,
  "synced_count": 1250,
  "failed_count": 0,
  "duration_seconds": 45.3,
  "errors": [],
  "completed_at": "2024-06-14T10:30:00Z"
}
```

#### **åˆ†æãƒ‡ãƒ¼ã‚¿ã®å–å¾—**
```bash
curl "http://localhost:8000/api/v1/data/analytics/overview?days=30"

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹
[
  {
    "date": "2024-06-14",
    "total_influencers": 1250,
    "active_campaigns": 15,
    "completed_negotiations": 8,
    "total_revenue": 2500000.0,
    "avg_engagement_rate": 0.045
  }
]
```

---

## ğŸ“Š åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½

### 1. BigQuery Analytics ã‚¯ãƒ©ã‚¹

#### **æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ**
```python
from core.bigquery_client import get_bigquery_analytics

analytics = get_bigquery_analytics()

# ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆéå»30æ—¥ï¼‰
df = analytics.get_influencer_growth_trends(days=30)

# çµæœã®å¯è¦–åŒ–
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['subscriber_growth'])
plt.title('ç™»éŒ²è€…æ•°æˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰')
plt.xlabel('æ—¥ä»˜')
plt.ylabel('æˆé•·æ•°')
plt.show()
```

#### **ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
```python
# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
df = analytics.get_category_performance()

print("ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
for _, row in df.iterrows():
    print(f"  {row['category']}: {row['influencer_count']}äºº, "
          f"å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {row['avg_engagement']:.3f}")
```

#### **ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ ROI åˆ†æ**
```python
# ROIåˆ†æãƒ‡ãƒ¼ã‚¿
df = analytics.get_campaign_roi_analysis()

# ROIä¸Šä½ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®è¡¨ç¤º
top_campaigns = df.head(10)
print("ROIä¸Šä½ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³:")
for _, row in top_campaigns.iterrows():
    roi = (row['total_spent'] / row['budget']) * 100 if row['budget'] > 0 else 0
    print(f"  {row['title']}: ROI {roi:.1f}%")
```

### 2. ã‚«ã‚¹ã‚¿ãƒ åˆ†æã‚¯ã‚¨ãƒª

#### **æœˆæ¬¡æˆé•·ç‡ã®è¨ˆç®—**
```sql
WITH monthly_growth AS (
  SELECT 
    DATE_TRUNC(created_at, MONTH) as month,
    COUNT(*) as new_influencers,
    AVG(subscriber_count) as avg_subscribers
  FROM `hackathon-462905.infumatch_data.influencers`
  WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 12 MONTH)
  GROUP BY month
  ORDER BY month
)
SELECT 
  month,
  new_influencers,
  avg_subscribers,
  LAG(new_influencers) OVER (ORDER BY month) as prev_month_count,
  SAFE_DIVIDE(
    new_influencers - LAG(new_influencers) OVER (ORDER BY month),
    LAG(new_influencers) OVER (ORDER BY month)
  ) * 100 as growth_rate_percent
FROM monthly_growth
```

#### **ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡åˆ†å¸ƒã®åˆ†æ**
```sql
SELECT 
  CASE 
    WHEN JSON_EXTRACT_SCALAR(ai_analysis, '$.engagement_rate') < 0.01 THEN 'Low (< 1%)'
    WHEN JSON_EXTRACT_SCALAR(ai_analysis, '$.engagement_rate') < 0.03 THEN 'Medium (1-3%)'
    WHEN JSON_EXTRACT_SCALAR(ai_analysis, '$.engagement_rate') < 0.05 THEN 'High (3-5%)'
    ELSE 'Very High (> 5%)'
  END as engagement_tier,
  COUNT(*) as influencer_count,
  AVG(subscriber_count) as avg_subscribers
FROM `hackathon-462905.infumatch_data.influencers`
WHERE is_active = true
  AND JSON_EXTRACT_SCALAR(ai_analysis, '$.engagement_rate') IS NOT NULL
GROUP BY engagement_tier
ORDER BY 
  CASE engagement_tier
    WHEN 'Low (< 1%)' THEN 1
    WHEN 'Medium (1-3%)' THEN 2
    WHEN 'High (3-5%)' THEN 3
    WHEN 'Very High (> 5%)' THEN 4
  END
```

### 3. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰é€£æº

#### **Looker Studioï¼ˆæ—§ Google Data Studioï¼‰é€£æº**
```python
# BigQuery ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆï¼ˆLooker Studio ç”¨ï¼‰
create_view_sql = """
CREATE OR REPLACE VIEW `hackathon-462905.infumatch_data.influencer_dashboard` AS
SELECT 
  influencer_id,
  channel_title,
  category,
  subscriber_count,
  CAST(JSON_EXTRACT_SCALAR(ai_analysis, '$.engagement_rate') AS FLOAT64) as engagement_rate,
  CAST(JSON_EXTRACT_SCALAR(ai_analysis, '$.content_quality_score') AS FLOAT64) as quality_score,
  created_at,
  updated_at
FROM `hackathon-462905.infumatch_data.influencers`
WHERE is_active = true
"""

bq_client.query(create_view_sql).result()
```

#### **ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆ**
```python
from datetime import datetime, timedelta
import pandas as pd

async def generate_weekly_report():
    """é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®è‡ªå‹•ç”Ÿæˆ"""
    
    # éå»1é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)
    
    analytics = get_bigquery_analytics()
    
    # å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    daily_metrics = analytics.get_daily_metrics_summary(days=7)
    growth_trends = analytics.get_influencer_growth_trends(days=7)
    category_performance = analytics.get_category_performance()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    report = {
        'period': f"{start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}",
        'summary': {
            'total_influencers': daily_metrics['total_influencers'].sum(),
            'new_registrations': len(growth_trends[growth_trends['date'] >= start_date.date()]),
            'avg_engagement': daily_metrics['platform_engagement_rate'].mean(),
            'total_revenue': daily_metrics['daily_revenue'].sum()
        },
        'top_categories': category_performance.head(5).to_dict('records')
    }
    
    return report

# é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Ÿè¡Œ
report = await generate_weekly_report()
print("ğŸ“Š é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
```

---

## ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### **1. èªè¨¼ã‚¨ãƒ©ãƒ¼**
```
Error: google.auth.exceptions.DefaultCredentialsError
```

**è§£æ±ºæ–¹æ³•:**
```bash
# èªè¨¼æƒ…å ±ã®å†è¨­å®š
gcloud auth application-default login

# ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
echo $GOOGLE_APPLICATION_CREDENTIALS

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®æ¨©é™ç¢ºèª
gcloud iam service-accounts describe admin-654@hackathon-462905.iam.gserviceaccount.com
```

#### **2. BigQuery æ¥ç¶šã‚¨ãƒ©ãƒ¼**
```
Error: 403 Access Denied: BigQuery BigQuery: Permission denied
```

**è§£æ±ºæ–¹æ³•:**
```bash
# BigQuery API ã®æœ‰åŠ¹åŒ–
gcloud services enable bigquery.googleapis.com

# IAM æ¨©é™ã®ç¢ºèªãƒ»è¿½åŠ 
gcloud projects add-iam-policy-binding hackathon-462905 \
  --member="serviceAccount:admin-654@hackathon-462905.iam.gserviceaccount.com" \
  --role="roles/bigquery.admin"
```

#### **3. Firestore æ¥ç¶šã‚¨ãƒ©ãƒ¼**
```
Error: google.cloud.exceptions.NotFound: 404 Database not found
```

**è§£æ±ºæ–¹æ³•:**
```bash
# Firestore ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ
gcloud firestore databases create --database=local --location=us-central1

# Firestore API ã®æœ‰åŠ¹åŒ–
gcloud services enable firestore.googleapis.com
```

#### **4. åŒæœŸå‡¦ç†ãŒé…ã„**

**åŸå› ã¨å¯¾ç­–:**
- **ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´**: `batch_size=50` ãªã©å°ã•ã„å€¤ã«è¨­å®š
- **ä¸¦è¡Œå‡¦ç†æ•°ã‚’åˆ¶é™**: `max_workers=5` ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
- **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¿½åŠ **: Firestore ã«ã‚¯ã‚¨ãƒªç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 

```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æœ€é©åŒ–
result = await integration.sync_influencers_to_bigquery(batch_size=50)

# ä¸¦è¡Œå‡¦ç†æ•°ã®åˆ¶é™
import asyncio
semaphore = asyncio.Semaphore(5)  # æœ€å¤§5ä¸¦è¡Œ

async def limited_sync():
    async with semaphore:
        return await integration.sync_influencers_to_bigquery()
```

#### **5. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼**

**è§£æ±ºæ–¹æ³•:**
```python
# å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²å‡¦ç†
async def chunked_sync(total_records, chunk_size=1000):
    """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã®åˆ†å‰²åŒæœŸ"""
    for offset in range(0, total_records, chunk_size):
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»åŒæœŸ
        chunk_data = await get_chunk_data(offset, chunk_size)
        await sync_chunk_to_bigquery(chunk_data)
        
        # ãƒ¡ãƒ¢ãƒªè§£æ”¾ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
        await asyncio.sleep(1)
        
        print(f"âœ… {offset + len(chunk_data)}/{total_records} ä»¶å‡¦ç†å®Œäº†")
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

#### **åŒæœŸå‡¦ç†ã®ç›£è¦–**
```python
import time
from datetime import datetime

class SyncMonitor:
    """åŒæœŸå‡¦ç†ã®ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.metrics = {
            'start_time': None,
            'records_processed': 0,
            'errors': [],
            'performance': []
        }
    
    def start_monitoring(self):
        self.metrics['start_time'] = time.time()
    
    def record_batch(self, batch_size, duration):
        self.metrics['records_processed'] += batch_size
        self.metrics['performance'].append({
            'batch_size': batch_size,
            'duration': duration,
            'rate': batch_size / duration if duration > 0 else 0
        })
    
    def get_summary(self):
        total_time = time.time() - self.metrics['start_time']
        avg_rate = self.metrics['records_processed'] / total_time
        
        return {
            'total_records': self.metrics['records_processed'],
            'total_time': total_time,
            'average_rate': avg_rate,
            'errors': len(self.metrics['errors'])
        }

# ä½¿ç”¨ä¾‹
monitor = SyncMonitor()
monitor.start_monitoring()

# åŒæœŸå‡¦ç†ä¸­ã«è¨˜éŒ²
monitor.record_batch(100, 5.2)

# çµæœè¡¨ç¤º
summary = monitor.get_summary()
print(f"ğŸ“Š åŒæœŸå®Œäº†: {summary['total_records']}ä»¶, {summary['average_rate']:.1f}ä»¶/ç§’")
```

---

## ğŸ”§ ä¿å®ˆãƒ»é‹ç”¨

### å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

#### **1. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—**
```python
# å¤ã„ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å‰Šé™¤ï¼ˆ90æ—¥ä»¥ä¸Šå‰ï¼‰
from services.batch_processor import YouTubeBatchProcessor

processor = YouTubeBatchProcessor()
cleanup_result = await processor.cleanup_old_data(days_to_keep=90)

print(f"ğŸ§¹ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {cleanup_result['deleted_count']}ä»¶å‰Šé™¤")
```

#### **2. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–**
```bash
# Firestore è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
gcloud firestore indexes composite create \
  --collection-group=influencers \
  --field-config field-path=category,order=ascending \
  --field-config field-path=subscriber_count,order=descending
```

#### **3. BigQuery ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†**
```sql
-- å¤ã„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å‰Šé™¤ï¼ˆéå»1å¹´ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿æŒï¼‰
DELETE FROM `hackathon-462905.infumatch_data.daily_metrics`
WHERE date < DATE_SUB(CURRENT_DATE(), INTERVAL 365 DAY)
```

### ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

#### **Cloud Monitoring ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**
```python
# ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é€ä¿¡
from google.cloud import monitoring_v3

def send_sync_metrics(records_synced, duration):
    """åŒæœŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ Cloud Monitoring ã«é€ä¿¡"""
    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/{settings.GOOGLE_CLOUD_PROJECT_ID}"
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
    series = monitoring_v3.TimeSeries()
    series.metric.type = "custom.googleapis.com/infumatch/sync_rate"
    series.resource.type = "global"
    
    # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ
    point = series.points.add()
    point.value.double_value = records_synced / duration
    point.interval.end_time.seconds = int(time.time())
    
    # é€ä¿¡
    client.create_time_series(name=project_name, time_series=[series])
```

#### **ã‚¨ãƒ©ãƒ¼é€šçŸ¥è¨­å®š**
```python
import logging
from google.cloud import error_reporting

# ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
error_client = error_reporting.Client()

def report_sync_error(error_message, context=None):
    """åŒæœŸã‚¨ãƒ©ãƒ¼ã‚’ Cloud Error Reporting ã«é€ä¿¡"""
    try:
        error_client.report_exception(
            message=error_message,
            context=context or {}
        )
    except Exception as e:
        logging.error(f"Failed to report error: {e}")

# ä½¿ç”¨ä¾‹
try:
    await integration.sync_influencers_to_bigquery()
except Exception as e:
    report_sync_error(
        f"Influencer sync failed: {str(e)}",
        context={"batch_size": 100, "timestamp": datetime.now().isoformat()}
    )
```

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

#### **è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ**
```python
async def automated_backup():
    """è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ"""
    from google.cloud import storage
    import json
    
    # Firestore ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    firestore_client = get_firestore_client()
    collections = ['influencers', 'campaigns', 'negotiations']
    
    backup_data = {}
    for collection in collections:
        docs = await db_helper.get_all_documents(collection, limit=10000)
        backup_data[collection] = docs
    
    # Cloud Storage ã«ä¿å­˜
    storage_client = storage.Client()
    bucket = storage_client.bucket('infumatch-backups')
    
    backup_filename = f"firestore_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    blob = bucket.blob(backup_filename)
    blob.upload_from_string(json.dumps(backup_data, ensure_ascii=False, indent=2))
    
    print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_filename}")

# æ—¥æ¬¡ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ
await automated_backup()
```

#### **ç½å®³å¾©æ—§è¨ˆç”»**
```python
async def disaster_recovery(backup_file):
    """ç½å®³å¾©æ—§ã®å®Ÿè¡Œ"""
    import json
    from google.cloud import storage
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    storage_client = storage.Client()
    bucket = storage_client.bucket('infumatch-backups')
    blob = bucket.blob(backup_file)
    backup_data = json.loads(blob.download_as_text())
    
    # ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
    db_helper = DatabaseHelper()
    
    for collection, documents in backup_data.items():
        print(f"ğŸ“¥ {collection} ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¾©å…ƒä¸­...")
        
        for doc in documents:
            doc_id = doc.get('channel_id') or doc.get('campaign_id') or doc.get('id')
            if doc_id:
                await db_helper.create_document(collection, doc_id, doc)
        
        print(f"âœ… {collection}: {len(documents)}ä»¶å¾©å…ƒå®Œäº†")
    
    print("ğŸ‰ ç½å®³å¾©æ—§å®Œäº†")

# å¾©æ—§å®Ÿè¡Œä¾‹
# await disaster_recovery('firestore_backup_20240614_103000.json')
```

---

## ğŸ“š å‚è€ƒè³‡æ–™

### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [Google Cloud Firestore](https://cloud.google.com/firestore/docs)
- [Google Cloud BigQuery](https://cloud.google.com/bigquery/docs)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [YouTube Data API v3](https://developers.google.com/youtube/v3)

### ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰
- [BigQuery Python Client](https://github.com/googleapis/python-bigquery)
- [Firestore Python Client](https://github.com/googleapis/python-firestore)

### é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
- `backend/core/database.py` - Firestore æ“ä½œ
- `backend/core/bigquery_client.py` - BigQuery æ“ä½œ
- `backend/services/data_integration.py` - ãƒ‡ãƒ¼ã‚¿çµ±åˆ
- `backend/api/v1/data_sync.py` - åŒæœŸ API
- `backend/scripts/test_bigquery_setup.py` - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ

---

**ğŸ“… æœ€çµ‚æ›´æ–°**: 2024å¹´6æœˆ14æ—¥  
**ğŸ“ ä½œæˆè€…**: InfuMatch Development Team  
**ğŸ“§ ã‚µãƒãƒ¼ãƒˆ**: dev@infumatch.com