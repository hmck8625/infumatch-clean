"""
Cloud Functions å®šæœŸå®Ÿè¡Œå‡¦ç†

@description YouTube ãƒ‡ãƒ¼ã‚¿ã®å®šæœŸåé›†ã€AIåˆ†æã€ãƒ‡ãƒ¼ã‚¿æ›´æ–°
Google Cloud Scheduler ã¨é€£æºã—ãŸè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ 

@author InfuMatch Development Team  
@version 1.0.0
"""

import logging
import json
import asyncio
from typing import Dict, Any, List
from datetime import datetime, timedelta
import os

import functions_framework
from google.cloud import firestore
from google.cloud import scheduler_v1
import vertexai

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'hackathon-462905')
REGION = os.environ.get('GOOGLE_CLOUD_REGION', 'asia-northeast1')

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CloudFunctionService:
    """Cloud Functions ã‚µãƒ¼ãƒ“ã‚¹åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.db = firestore.Client()
        self.project_id = PROJECT_ID
        self.region = REGION
        
        # Vertex AI åˆæœŸåŒ–
        vertexai.init(project=self.project_id, location=self.region)
        
        logger.info(f"ğŸš€ CloudFunctionService initialized for project: {self.project_id}")
    
    async def log_execution(self, function_name: str, status: str, details: Dict[str, Any]):
        """å®Ÿè¡Œãƒ­ã‚°ã®è¨˜éŒ²"""
        try:
            log_data = {
                'function_name': function_name,
                'status': status,
                'details': details,
                'timestamp': datetime.utcnow(),
                'project_id': self.project_id
            }
            
            await self.db.collection('function_logs').add(log_data)
            logger.info(f"ğŸ“ Logged execution: {function_name} - {status}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to log execution: {e}")


class YouTubeDataCollector(CloudFunctionService):
    """YouTube ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    async def collect_new_influencers(self, search_queries: List[str]) -> Dict[str, Any]:
        """æ–°ã—ã„ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®åé›†"""
        try:
            logger.info(f"ğŸ” Starting influencer collection for {len(search_queries)} queries")
            
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€YouTube API ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
            # ã“ã“ã§ã¯æ§‹é€ ã®ã¿ã‚’ç¤ºã™
            
            results = {
                'queries_processed': len(search_queries),
                'new_influencers_found': 0,
                'updated_influencers': 0,
                'errors': []
            }
            
            for query in search_queries:
                try:
                    # YouTube API æ¤œç´¢å®Ÿè¡Œï¼ˆä»®å®Ÿè£…ï¼‰
                    # channels = await youtube_service.search_channels(query)
                    # new_count = await self.process_new_channels(channels)
                    # results['new_influencers_found'] += new_count
                    
                    logger.info(f"âœ… Processed query: {query}")
                    
                except Exception as e:
                    error_msg = f"Query '{query}' failed: {str(e)}"
                    results['errors'].append(error_msg)
                    logger.error(f"âŒ {error_msg}")
            
            await self.log_execution('collect_new_influencers', 'completed', results)
            return results
            
        except Exception as e:
            logger.error(f"âŒ Influencer collection failed: {e}")
            await self.log_execution('collect_new_influencers', 'failed', {'error': str(e)})
            raise
    
    async def update_existing_analytics(self) -> Dict[str, Any]:
        """æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®åˆ†æãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        try:
            logger.info("ğŸ“Š Starting analytics update for existing influencers")
            
            # æ›´æ–°å¯¾è±¡ã®é¸å®šï¼ˆ7æ—¥ä»¥ä¸Šæ›´æ–°ã•ã‚Œã¦ã„ãªã„ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ï¼‰
            cutoff_date = datetime.utcnow() - timedelta(days=7)
            
            # Firestore ã‹ã‚‰æ›´æ–°å¯¾è±¡ã‚’å–å¾—
            influencers_ref = self.db.collection('influencers')
            query = influencers_ref.where('last_analyzed', '<', cutoff_date).limit(50)
            docs = query.stream()
            
            update_results = {
                'total_candidates': 0,
                'successful_updates': 0,
                'failed_updates': 0,
                'errors': []
            }
            
            for doc in docs:
                update_results['total_candidates'] += 1
                
                try:
                    influencer_data = doc.to_dict()
                    channel_id = influencer_data.get('channel_id')
                    
                    # åˆ†æãƒ‡ãƒ¼ã‚¿æ›´æ–°ï¼ˆä»®å®Ÿè£…ï¼‰
                    # updated_data = await youtube_service.update_influencer_analytics(channel_id)
                    # await self.save_updated_analytics(doc.id, updated_data)
                    
                    update_results['successful_updates'] += 1
                    logger.info(f"âœ… Updated analytics for: {channel_id}")
                    
                except Exception as e:
                    error_msg = f"Failed to update {doc.id}: {str(e)}"
                    update_results['errors'].append(error_msg)
                    update_results['failed_updates'] += 1
                    logger.error(f"âŒ {error_msg}")
            
            await self.log_execution('update_existing_analytics', 'completed', update_results)
            return update_results
            
        except Exception as e:
            logger.error(f"âŒ Analytics update failed: {e}")
            await self.log_execution('update_existing_analytics', 'failed', {'error': str(e)})
            raise


class AIAnalysisProcessor(CloudFunctionService):
    """AI åˆ†æå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    async def batch_analyze_influencers(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®ãƒãƒƒãƒåˆ†æå‡¦ç†"""
        try:
            logger.info("ğŸ¤– Starting batch AI analysis for influencers")
            
            # åˆ†æå¯¾è±¡ã®é¸å®šï¼ˆAIåˆ†æãŒæœªå®Ÿè¡Œã¾ãŸã¯ã‚¹ã‚³ã‚¢ãŒä½ã„ã‚‚ã®ï¼‰
            influencers_ref = self.db.collection('influencers')
            query = influencers_ref.where('ai_analysis_completed', '==', False).limit(20)
            docs = query.stream()
            
            analysis_results = {
                'total_analyzed': 0,
                'successful_analyses': 0,
                'failed_analyses': 0,
                'average_quality_score': 0.0,
                'errors': []
            }
            
            quality_scores = []
            
            for doc in docs:
                try:
                    influencer_data = doc.to_dict()
                    channel_id = influencer_data.get('channel_id')
                    
                    # AIåˆ†æå®Ÿè¡Œï¼ˆä»®å®Ÿè£…ï¼‰
                    # analysis_result = await ai_agent.analyze_channel(influencer_data)
                    # await self.save_ai_analysis(doc.id, analysis_result)
                    
                    # ä»®ã®å“è³ªã‚¹ã‚³ã‚¢
                    quality_score = 0.7  # å®Ÿéš›ã«ã¯AIåˆ†æçµæœã‹ã‚‰å–å¾—
                    quality_scores.append(quality_score)
                    
                    analysis_results['total_analyzed'] += 1
                    analysis_results['successful_analyses'] += 1
                    
                    logger.info(f"âœ… Analyzed: {channel_id} (score: {quality_score})")
                    
                except Exception as e:
                    error_msg = f"Failed to analyze {doc.id}: {str(e)}"
                    analysis_results['errors'].append(error_msg)
                    analysis_results['failed_analyses'] += 1
                    logger.error(f"âŒ {error_msg}")
            
            if quality_scores:
                analysis_results['average_quality_score'] = sum(quality_scores) / len(quality_scores)
            
            await self.log_execution('batch_analyze_influencers', 'completed', analysis_results)
            return analysis_results
            
        except Exception as e:
            logger.error(f"âŒ Batch AI analysis failed: {e}")
            await self.log_execution('batch_analyze_influencers', 'failed', {'error': str(e)})
            raise
    
    async def update_recommendation_models(self) -> Dict[str, Any]:
        """æ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°"""
        try:
            logger.info("ğŸ”„ Starting recommendation model update")
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¨è–¦ãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ML Pipelineã‚’ä½¿ç”¨
            
            model_update_results = {
                'model_version': datetime.utcnow().strftime('%Y%m%d_%H%M%S'),
                'training_data_size': 0,
                'model_accuracy': 0.0,
                'deployment_status': 'pending'
            }
            
            # ãƒ‡ãƒ¼ã‚¿åé›†
            # training_data = await self.collect_training_data()
            # model_update_results['training_data_size'] = len(training_data)
            
            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            # model_accuracy = await self.train_recommendation_model(training_data)
            # model_update_results['model_accuracy'] = model_accuracy
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤
            # deployment_success = await self.deploy_updated_model()
            # model_update_results['deployment_status'] = 'success' if deployment_success else 'failed'
            
            model_update_results['training_data_size'] = 1000  # ä»®ã®å€¤
            model_update_results['model_accuracy'] = 0.85  # ä»®ã®å€¤
            model_update_results['deployment_status'] = 'success'
            
            await self.log_execution('update_recommendation_models', 'completed', model_update_results)
            return model_update_results
            
        except Exception as e:
            logger.error(f"âŒ Model update failed: {e}")
            await self.log_execution('update_recommendation_models', 'failed', {'error': str(e)})
            raise


class DataMaintenanceService(CloudFunctionService):
    """ãƒ‡ãƒ¼ã‚¿ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚µãƒ¼ãƒ“ã‚¹"""
    
    async def cleanup_old_data(self) -> Dict[str, Any]:
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            logger.info("ğŸ§¹ Starting data cleanup process")
            
            cleanup_results = {
                'deleted_logs': 0,
                'archived_campaigns': 0,
                'cleaned_temp_data': 0,
                'errors': []
            }
            
            # å¤ã„ãƒ­ã‚°ã®å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šå‰ï¼‰
            cutoff_date = datetime.utcnow() - timedelta(days=30)
            
            try:
                logs_ref = self.db.collection('function_logs')
                old_logs_query = logs_ref.where('timestamp', '<', cutoff_date)
                old_logs = old_logs_query.stream()
                
                for log_doc in old_logs:
                    log_doc.reference.delete()
                    cleanup_results['deleted_logs'] += 1
                
                logger.info(f"âœ… Deleted {cleanup_results['deleted_logs']} old logs")
                
            except Exception as e:
                cleanup_results['errors'].append(f"Log cleanup failed: {str(e)}")
                logger.error(f"âŒ Log cleanup failed: {e}")
            
            # å®Œäº†ã—ãŸã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
            try:
                campaigns_ref = self.db.collection('campaigns')
                completed_campaigns = campaigns_ref.where('status', '==', 'completed').stream()
                
                for campaign_doc in completed_campaigns:
                    campaign_data = campaign_doc.to_dict()
                    
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»å‹•
                    await self.db.collection('archived_campaigns').add(campaign_data)
                    campaign_doc.reference.delete()
                    cleanup_results['archived_campaigns'] += 1
                
                logger.info(f"âœ… Archived {cleanup_results['archived_campaigns']} completed campaigns")
                
            except Exception as e:
                cleanup_results['errors'].append(f"Campaign archiving failed: {str(e)}")
                logger.error(f"âŒ Campaign archiving failed: {e}")
            
            # ä¸€æ™‚ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            try:
                temp_data_ref = self.db.collection('temp_data')
                old_temp_data = temp_data_ref.where('created_at', '<', cutoff_date).stream()
                
                for temp_doc in old_temp_data:
                    temp_doc.reference.delete()
                    cleanup_results['cleaned_temp_data'] += 1
                
                logger.info(f"âœ… Cleaned {cleanup_results['cleaned_temp_data']} temporary data entries")
                
            except Exception as e:
                cleanup_results['errors'].append(f"Temp data cleanup failed: {str(e)}")
                logger.error(f"âŒ Temp data cleanup failed: {e}")
            
            await self.log_execution('cleanup_old_data', 'completed', cleanup_results)
            return cleanup_results
            
        except Exception as e:
            logger.error(f"âŒ Data cleanup failed: {e}")
            await self.log_execution('cleanup_old_data', 'failed', {'error': str(e)})
            raise
    
    async def generate_analytics_reports(self) -> Dict[str, Any]:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("ğŸ“Š Starting analytics report generation")
            
            report_results = {
                'daily_report_generated': False,
                'weekly_report_generated': False,
                'influencer_stats': {},
                'campaign_stats': {},
                'errors': []
            }
            
            # æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            try:
                daily_stats = await self.generate_daily_stats()
                await self.save_daily_report(daily_stats)
                report_results['daily_report_generated'] = True
                report_results['influencer_stats'] = daily_stats
                
            except Exception as e:
                report_results['errors'].append(f"Daily report failed: {str(e)}")
                logger.error(f"âŒ Daily report generation failed: {e}")
            
            # é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æ›œæ—¥ã®ã¿ï¼‰
            if datetime.utcnow().weekday() == 6:  # Sunday
                try:
                    weekly_stats = await self.generate_weekly_stats()
                    await self.save_weekly_report(weekly_stats)
                    report_results['weekly_report_generated'] = True
                    report_results['campaign_stats'] = weekly_stats
                    
                except Exception as e:
                    report_results['errors'].append(f"Weekly report failed: {str(e)}")
                    logger.error(f"âŒ Weekly report generation failed: {e}")
            
            await self.log_execution('generate_analytics_reports', 'completed', report_results)
            return report_results
            
        except Exception as e:
            logger.error(f"âŒ Analytics report generation failed: {e}")
            await self.log_execution('generate_analytics_reports', 'failed', {'error': str(e)})
            raise
    
    async def generate_daily_stats(self) -> Dict[str, Any]:
        """æ—¥æ¬¡çµ±è¨ˆã®ç”Ÿæˆ"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€Firestoreã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ
        return {
            'total_influencers': 150,  # ä»®ã®å€¤
            'new_influencers_today': 5,
            'active_campaigns': 3,
            'total_engagements': 15000,
            'date': datetime.utcnow().date().isoformat()
        }
    
    async def generate_weekly_stats(self) -> Dict[str, Any]:
        """é€±æ¬¡çµ±è¨ˆã®ç”Ÿæˆ"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€é€±é–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ
        return {
            'weekly_new_influencers': 25,
            'campaigns_completed': 2,
            'total_reach': 500000,
            'avg_engagement_rate': 3.2,
            'week_start': (datetime.utcnow() - timedelta(days=7)).date().isoformat()
        }
    
    async def save_daily_report(self, stats: Dict[str, Any]):
        """æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        await self.db.collection('daily_reports').add(stats)
    
    async def save_weekly_report(self, stats: Dict[str, Any]):
        """é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        await self.db.collection('weekly_reports').add(stats)


# =============================================================================
# Cloud Functions ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# =============================================================================

@functions_framework.http
def http_trigger_main(request):
    """HTTP ãƒˆãƒªã‚¬ãƒ¼ç”¨ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—
        request_json = request.get_json(silent=True)
        action = request_json.get('action', 'health_check') if request_json else 'health_check'
        
        if action == 'health_check':
            return {'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()}
        elif action == 'manual_trigger':
            # æ‰‹å‹•å®Ÿè¡Œç”¨
            function_name = request_json.get('function_name', 'collect_new_influencers')
            result = asyncio.run(manual_execution(function_name))
            return result
        else:
            return {'error': f'Unknown action: {action}'}, 400
            
    except Exception as e:
        logger.error(f"âŒ HTTP trigger failed: {e}")
        return {'error': str(e)}, 500


@functions_framework.cloud_event
def scheduled_youtube_collection(cloud_event):
    """YouTube ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰"""
    try:
        logger.info("â° Scheduled YouTube data collection triggered")
        
        # éåŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
        result = asyncio.run(execute_youtube_collection())
        
        logger.info(f"âœ… YouTube collection completed: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Scheduled YouTube collection failed: {e}")
        raise


@functions_framework.cloud_event
def scheduled_ai_analysis(cloud_event):
    """AI åˆ†æå‡¦ç†ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰"""
    try:
        logger.info("â° Scheduled AI analysis triggered")
        
        result = asyncio.run(execute_ai_analysis())
        
        logger.info(f"âœ… AI analysis completed: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Scheduled AI analysis failed: {e}")
        raise


@functions_framework.cloud_event  
def scheduled_data_maintenance(cloud_event):
    """ãƒ‡ãƒ¼ã‚¿ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼ˆå®šæœŸå®Ÿè¡Œï¼‰"""
    try:
        logger.info("â° Scheduled data maintenance triggered")
        
        result = asyncio.run(execute_data_maintenance())
        
        logger.info(f"âœ… Data maintenance completed: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Scheduled data maintenance failed: {e}")
        raise


# =============================================================================
# å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
# =============================================================================

async def execute_youtube_collection() -> Dict[str, Any]:
    """YouTube ãƒ‡ãƒ¼ã‚¿åé›†ã®å®Ÿè¡Œ"""
    collector = YouTubeDataCollector()
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¨­å®š
    search_queries = [
        "ç¾å®¹ ãƒ¡ã‚¤ã‚¯ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«",
        "æ–™ç† ãƒ¬ã‚·ãƒ” ç°¡å˜",
        "ã‚²ãƒ¼ãƒ  å®Ÿæ³ é¢ç™½ã„",
        "ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ« vlog",
        "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆ"
    ]
    
    # æ–°è¦ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åé›†
    new_influencers_result = await collector.collect_new_influencers(search_queries)
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    analytics_update_result = await collector.update_existing_analytics()
    
    return {
        'new_influencers': new_influencers_result,
        'analytics_update': analytics_update_result,
        'execution_time': datetime.utcnow().isoformat()
    }


async def execute_ai_analysis() -> Dict[str, Any]:
    """AI åˆ†æå‡¦ç†ã®å®Ÿè¡Œ"""
    processor = AIAnalysisProcessor()
    
    # ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ
    batch_analysis_result = await processor.batch_analyze_influencers()
    
    # æ¨è–¦ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆé€±1å›ï¼‰
    model_update_result = None
    if datetime.utcnow().weekday() == 0:  # Monday
        model_update_result = await processor.update_recommendation_models()
    
    return {
        'batch_analysis': batch_analysis_result,
        'model_update': model_update_result,
        'execution_time': datetime.utcnow().isoformat()
    }


async def execute_data_maintenance() -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã®å®Ÿè¡Œ"""
    maintenance = DataMaintenanceService()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_result = await maintenance.cleanup_old_data()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_result = await maintenance.generate_analytics_reports()
    
    return {
        'cleanup': cleanup_result,
        'reports': report_result,
        'execution_time': datetime.utcnow().isoformat()
    }


async def manual_execution(function_name: str) -> Dict[str, Any]:
    """æ‰‹å‹•å®Ÿè¡Œç”¨ã®ãƒ­ã‚¸ãƒƒã‚¯"""
    if function_name == 'collect_new_influencers':
        return await execute_youtube_collection()
    elif function_name == 'ai_analysis':
        return await execute_ai_analysis()
    elif function_name == 'data_maintenance':
        return await execute_data_maintenance()
    else:
        return {'error': f'Unknown function: {function_name}'}


if __name__ == "__main__":
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ã‚¹ãƒˆç”¨
    print("ğŸ§ª Running local test...")
    result = asyncio.run(execute_youtube_collection())
    print(f"Test result: {json.dumps(result, indent=2, ensure_ascii=False)}")