"""
Geminié«˜åº¦ãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - æˆ¦ç•¥çš„ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ 
æ·±ã„åˆ†æã¨èª¬å¾—åŠ›ã®ã‚ã‚‹æ¨è–¦ç†ç”±ã‚’æä¾›
"""
import google.generativeai as genai
from typing import Dict, List, Any, Optional
import json
import logging
from datetime import datetime
from google.cloud import firestore
import asyncio

logger = logging.getLogger(__name__)

class GeminiMatchingAgent:
    """Gemini APIã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒƒãƒãƒ³ã‚°åˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    
    def __init__(self, gemini_api_key: str):
        self.gemini_api_key = gemini_api_key
        genai.configure(api_key=gemini_api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        try:
            self.db = firestore.Client(project="hackathon-462905")
        except Exception as e:
            logger.warning(f"Firestore initialization failed: {e}")
            self.db = None
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
        self.mock_metadata = {}
        
    async def analyze_deep_matching(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼æ¥­ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æˆ¦ç•¥çš„ãƒãƒƒãƒãƒ³ã‚°åˆ†æ"""
        try:
            start_time = datetime.now()
            logger.info("ğŸ§  Geminié«˜åº¦ãƒãƒƒãƒãƒ³ã‚°åˆ†æé–‹å§‹")
            
            # Step 1: ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            fetch_result = await self._fetch_influencer_candidates_with_metadata(request_data)
            influencer_candidates = fetch_result["candidates"]
            pickup_metadata = fetch_result["metadata"]
            
            logger.info(f"ğŸ“Š å–å¾—ã—ãŸã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼å€™è£œæ•°: {len(influencer_candidates)}")
            if influencer_candidates:
                logger.info(f"ğŸ“‹ å€™è£œã‚«ãƒ†ã‚´ãƒª: {[c.get('category', 'unknown') for c in influencer_candidates[:10]]}")
                preferences = request_data.get('influencer_preferences', {})
                logger.info(f"ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›: {preferences.get('custom_preference', 'ãªã—')}")
                
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨æ™‚ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
                if pickup_metadata.get("data_source") == "mock":
                    print("ğŸ”„ " + "="*50)
                    print("ğŸ“Œ ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
                    print(f"   ç†ç”±: {pickup_metadata.get('mock_reason', 'ä¸æ˜')}")
                    print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {pickup_metadata.get('mock_dataset_name', 'æ¨™æº–')}")
                    print(f"   ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {len(influencer_candidates)}ä»¶")
                    print("ğŸ”„ " + "="*50)
            
            if not influencer_candidates:
                return {
                    "success": False,
                    "error": "ãƒãƒƒãƒãƒ³ã‚°å€™è£œã¨ãªã‚‹ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
                }
            
            # Step 2: å„ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®è©³ç´°åˆ†æ
            analysis_results = []
            # ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›ãŒã‚ã‚‹å ´åˆã¯æœ€å¤§10åã¾ã§åˆ†æ
            preferences = request_data.get('influencer_preferences', {})
            custom_preference = preferences.get('custom_preference', '')
            max_analysis = 10 if custom_preference else 5
            
            for influencer in influencer_candidates[:max_analysis]:
                try:
                    analysis = await self._analyze_single_influencer(
                        influencer, 
                        request_data
                    )
                    if analysis:
                        analysis_results.append(analysis)
                except Exception as e:
                    logger.warning(f"å€‹åˆ¥ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            if not analysis_results:
                return {
                    "success": False,
                    "error": "åˆ†æå¯èƒ½ãªã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
                }
            
            # Step 3: ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–ã¨å¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
            portfolio_insights = await self._analyze_portfolio_optimization(
                analysis_results, 
                request_data
            )
            market_context = await self._analyze_market_context(
                request_data, 
                analysis_results
            )
            
            # Step 4: çµæœã®æ§‹ç¯‰
            processing_duration = (datetime.now() - start_time).total_seconds()
            
            return {
                "success": True,
                "analysis_results": analysis_results,
                "portfolio_insights": portfolio_insights,
                "market_context": market_context,
                "pickup_logic_details": pickup_metadata.get("pickup_logic", {}),
                "processing_metadata": {
                    "analysis_duration_ms": int(processing_duration * 1000),
                    "confidence_score": self._calculate_overall_confidence(analysis_results),
                    "gemini_model_used": "gemini-1.5-flash",
                    "analysis_timestamp": datetime.now().isoformat(),
                    "total_candidates_analyzed": len(analysis_results),
                    "total_candidates_available": len(influencer_candidates),
                    "data_source": pickup_metadata.get("data_source", "unknown"),
                    "filtering_summary": pickup_metadata.get("filtering_applied", {})
                }
            }
            
        except Exception as e:
            logger.error(f"Geminié«˜åº¦ãƒãƒƒãƒãƒ³ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": f"åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            }
    
    async def _fetch_influencer_candidates_with_metadata(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ãƒãƒƒãƒãƒ³ã‚°å€™è£œã¨ãªã‚‹ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’å–å¾—"""
        candidates = await self._fetch_influencer_candidates(request_data)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        preferences = request_data.get('influencer_preferences', {})
        metadata = {
            "data_source": "firestore" if self.db else "mock",
            "total_candidates": len(candidates),
            "filtering_applied": {
                "custom_preference": preferences.get('custom_preference', ''),
                "subscriber_range": preferences.get('subscriber_range', {}),
                "preferred_categories": preferences.get('preferred_categories', [])
            },
            "pickup_logic": self._build_pickup_logic_summary(request_data, candidates)
        }
        
        return {
            "candidates": candidates,
            "metadata": metadata
        }
    
    async def _fetch_influencer_candidates(self, request_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒãƒ³ã‚°å€™è£œã¨ãªã‚‹ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚’å–å¾—"""
        try:
            logger.info("ğŸ“Š ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼å€™è£œãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            
            # FirestoreãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if not self.db:
                logger.warning("âš ï¸ Firestore not available, using mock data")
                mock_data = self._get_mock_influencers()
                self._set_mock_metadata("firestore_unavailable", "Firestoreã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ä¸å¯")
                return mock_data
            
            # Firestoreã‹ã‚‰ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            influencers_ref = self.db.collection('influencers')
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            preferences = request_data.get('influencer_preferences', {})
            query = influencers_ref
            
            # Firestoreã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«å¤‰æ›´
            # ã¾ãšå…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            logger.info("ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ")
            
            # ã¾ãšå…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸è¦ï¼‰
            try:
                all_docs = self.db.collection('influencers').limit(100).stream()
                all_candidates = []
                for doc in all_docs:
                    data = doc.to_dict()
                    data['id'] = doc.id
                    all_candidates.append(data)
                    
                logger.info(f"ğŸ“Š Firestoreå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—: {len(all_candidates)}ä»¶")
                
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                candidates = []
                preferences = request_data.get('influencer_preferences', {})
                custom_preference = preferences.get('custom_preference', '')
                
                # ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›ãŒã‚ã‚‹å ´åˆã®ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
                preferred_categories = preferences.get('preferred_categories', [])
                if custom_preference:
                    logger.info(f"ğŸ” ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›: '{custom_preference}'")
                    available_categories = list(set([c.get('category', '') for c in all_candidates if c.get('category')]))
                    logger.info(f"ğŸ“‚ åˆ©ç”¨å¯èƒ½ã‚«ãƒ†ã‚´ãƒª: {available_categories}")
                    
                    # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§ã‚«ãƒ†ã‚´ãƒªé¸æŠ
                    user_lower = custom_preference.lower()
                    for category in available_categories:
                        if any(keyword in category.lower() for keyword in user_lower.split()):
                            preferred_categories.append(category)
                    
                    logger.info(f"ğŸ¯ ãƒãƒƒãƒã—ãŸã‚«ãƒ†ã‚´ãƒª: {preferred_categories}")
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
                for candidate in all_candidates:
                    # ç™»éŒ²è€…æ•°ãƒ•ã‚£ãƒ«ã‚¿
                    subscriber_count = candidate.get('subscriber_count', 0)
                    if preferences.get('subscriber_range'):
                        sub_range = preferences['subscriber_range']
                        if sub_range.get('min') and subscriber_count < sub_range['min']:
                            continue
                        if sub_range.get('max') and subscriber_count > sub_range['max']:
                            continue
                    
                    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
                    if preferred_categories:
                        category = candidate.get('category', '')
                        if not any(pref_cat in category or category in pref_cat for pref_cat in preferred_categories):
                            continue
                    
                    candidates.append(candidate)
                
                # å–å¾—ä¸Šé™é©ç”¨
                limit = 30 if custom_preference else 20
                candidates = candidates[:limit]
                
            except Exception as e:
                logger.error(f"âŒ Firestoreå…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                candidates = []
            
            logger.info(f"âœ… {len(candidates)}åã®å€™è£œã‚’å–å¾—")
            
            # å€™è£œãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if len(candidates) == 0:
                logger.warning("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿å¾Œã«å€™è£œãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
                mock_data = self._get_mock_influencers()
                self._set_mock_metadata("filter_no_results", "ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«åˆè‡´ã™ã‚‹å€™è£œãªã—")
                return mock_data
            
            return candidates
            
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼å€™è£œå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            logger.info("ğŸ“Œ ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™")
            mock_data = self._get_mock_influencers()
            self._set_mock_metadata("firestore_error", f"Firestoreã‚¨ãƒ©ãƒ¼: {str(e)}")
            return mock_data
    
    async def _analyze_single_influencer(self, influencer: Dict[str, Any], request_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å˜ä¸€ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®è©³ç´°åˆ†æ"""
        try:
            analysis_prompt = self._build_deep_analysis_prompt(influencer, request_data)
            
            response = await self._call_gemini_async(analysis_prompt)
            if not response:
                return None
            
            # JSONå½¢å¼ã®å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            try:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰JSONè§£æ
                cleaned_response = self._clean_json_response(response)
                parsed_response = json.loads(cleaned_response)
                logger.info(f"âœ… JSONè§£ææˆåŠŸ: {influencer.get('channel_name', 'unknown')}")
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±æ•—: {e} - ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
                parsed_response = self._extract_analysis_from_text(response)
            
            return {
                "influencer_id": influencer.get('id', ''),
                "influencer_data": {
                    "channel_id": influencer.get('channel_id', ''),
                    "channel_name": influencer.get('channel_name', influencer.get('channel_title', influencer.get('name', ''))),
                    "channel_title": influencer.get('channel_title', ''),
                    "description": influencer.get('description', ''),
                    "subscriber_count": influencer.get('subscriber_count', 0),
                    "video_count": influencer.get('video_count', 0),
                    "view_count": influencer.get('view_count', 0),
                    "engagement_rate": influencer.get('engagement_rate', 0.0),
                    "thumbnail_url": influencer.get('thumbnail_url', ''),
                    "category": influencer.get('category', ''),
                    "email": influencer.get('email', '')
                },
                "overall_compatibility_score": parsed_response.get('overall_compatibility_score', 75),
                "detailed_analysis": {
                    "brand_alignment": {
                        "score": parsed_response.get('brand_alignment_score', 70),
                        "reasoning": parsed_response.get('brand_alignment_reasoning', 'ä¼æ¥­ãƒ–ãƒ©ãƒ³ãƒ‰ã¨ã®é©åˆæ€§ã‚’åˆ†æä¸­'),
                        "key_strengths": parsed_response.get('brand_strengths', ['é«˜ã„ä¿¡é ¼æ€§', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã®ä¸€è‡´']),
                        "potential_concerns": parsed_response.get('brand_concerns', ['ãƒªãƒ¼ãƒã®é™ç•Œ'])
                    },
                    "audience_synergy": {
                        "score": parsed_response.get('audience_synergy_score', 80),
                        "demographic_overlap": parsed_response.get('demographic_overlap', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã®70%ãŒé‡è¤‡'),
                        "engagement_quality": parsed_response.get('engagement_quality', 'é«˜å“è³ªãªã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ'),
                        "conversion_potential": parsed_response.get('conversion_potential', 'ä¸­ç¨‹åº¦ã‹ã‚‰é«˜ã„ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æœŸå¾…å€¤')
                    },
                    "content_fit": {
                        "score": parsed_response.get('content_fit_score', 85),
                        "style_compatibility": parsed_response.get('style_compatibility', 'ä¼æ¥­ãƒ–ãƒ©ãƒ³ãƒ‰ã¨èª¿å’Œã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ã‚¿ã‚¤ãƒ«'),
                        "content_themes_match": parsed_response.get('content_themes', ['å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ææ¡ˆ']),
                        "creative_opportunities": parsed_response.get('creative_opportunities', ['å•†å“çµ±åˆ', 'ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°'])
                    },
                    "business_viability": {
                        "score": parsed_response.get('business_viability_score', 75),
                        "roi_prediction": parsed_response.get('roi_prediction', 'æŠ•è³‡å¯¾åŠ¹æœã¯è‰¯å¥½ãªè¦‹è¾¼ã¿'),
                        "risk_assessment": parsed_response.get('risk_assessment', 'ä½ã€œä¸­ç¨‹åº¦ã®ãƒªã‚¹ã‚¯'),
                        "long_term_potential": parsed_response.get('long_term_potential', 'é•·æœŸçš„ãªãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ã®å¯èƒ½æ€§')
                    }
                },
                "recommendation_summary": {
                    "confidence_level": parsed_response.get('confidence_level', 'Medium'),
                    "primary_recommendation_reason": parsed_response.get('primary_reason', f'{influencer.get("name", "ã“ã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼")}ã¯ä¼æ¥­ã®ä¾¡å€¤è¦³ã¨å¼·ãå…±é³´ã—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã¸ã®åŠ¹æœçš„ãªãƒªãƒ¼ãƒãŒæœŸå¾…ã§ãã¾ã™'),
                    "success_scenario": parsed_response.get('success_scenario', 'å•†å“ã®è‡ªç„¶ãªç´¹ä»‹ã«ã‚ˆã‚Šã€ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦å‘ä¸Šã¨å£²ä¸Šå¢—åŠ ãŒæœŸå¾…ã•ã‚Œã¾ã™'),
                    "collaboration_strategy": parsed_response.get('collaboration_strategy', 'æ®µéšçš„ãªã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰é•·æœŸãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ã¸ç™ºå±•'),
                    "expected_outcomes": parsed_response.get('expected_outcomes', ['ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦20%å‘ä¸Š', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡15%å‘ä¸Š', 'å£²ä¸Š5-10%å¢—åŠ '])
                },
                "strategic_insights": {
                    "best_collaboration_types": parsed_response.get('collaboration_types', ['å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'ãƒ©ã‚¤ãƒ–é…ä¿¡']),
                    "optimal_campaign_timing": parsed_response.get('optimal_timing', '3-6ãƒ¶æœˆã®ç¶™ç¶šçš„ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³'),
                    "content_suggestions": parsed_response.get('content_suggestions', ['å•†å“ã®ä½¿ç”¨ä½“é¨“', 'æ—¥å¸¸ã¸ã®çµ±åˆææ¡ˆ', 'ãƒ•ã‚¡ãƒ³ã¨ã®äº¤æµä¼ç”»']),
                    "budget_recommendations": {
                        "min": parsed_response.get('budget_min', 80000),
                        "max": parsed_response.get('budget_max', 150000),
                        "reasoning": parsed_response.get('budget_reasoning', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®å½±éŸ¿åŠ›ã¨ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’è€ƒæ…®ã—ãŸé©æ­£ä¾¡æ ¼ç¯„å›²')
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"å€‹åˆ¥ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _build_deep_analysis_prompt(self, influencer: Dict[str, Any], request_data: Dict[str, Any]) -> str:
        """Geminiç”¨ã®è©³ç´°åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        company_profile = request_data.get('company_profile', {})
        product_portfolio = request_data.get('product_portfolio', {})
        campaign_objectives = request_data.get('campaign_objectives', {})
        influencer_preferences = request_data.get('influencer_preferences', {})
        custom_preference = influencer_preferences.get('custom_preference', '')
        
        prompt = f"""
ã‚ãªãŸã¯æˆ¦ç•¥çš„ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ä¼æ¥­ã¨ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®è©³ç´°æƒ…å ±ã‚’åˆ†æã—ã€æˆ¦ç•¥çš„ãªé©åˆæ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

## ğŸ“Š ä¼æ¥­ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
**ä¼æ¥­å**: {company_profile.get('name', 'N/A')}
**æ¥­ç•Œ**: {company_profile.get('industry', 'N/A')}
**ä¼æ¥­èª¬æ˜**: {company_profile.get('description', 'N/A')}
**ãƒ–ãƒ©ãƒ³ãƒ‰ä¾¡å€¤è¦³**: {', '.join(company_profile.get('brand_values', []))}
**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤**: {', '.join(company_profile.get('target_demographics', []))}
**ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¿ã‚¤ãƒ«**: {company_profile.get('communication_style', 'N/A')}

## ğŸ¯ å•†å“ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª
"""
        
        products = product_portfolio.get('products', [])
        for i, product in enumerate(products[:3], 1):
            prompt += f"""
**å•†å“{i}**: {product.get('name', 'N/A')}
- ã‚«ãƒ†ã‚´ãƒª: {product.get('category', 'N/A')}
- èª¬æ˜: {product.get('description', 'N/A')}
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {product.get('target_audience', 'N/A')}
- ä¾¡æ ¼å¸¯: {product.get('price_range', 'N/A')}
- ç‰¹å¾´: {', '.join(product.get('unique_selling_points', []))}
"""
        
        prompt += f"""
## ğŸš€ ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ç›®æ¨™
**ä¸»è¦ç›®æ¨™**: {', '.join(campaign_objectives.get('primary_goals', []))}
**æˆåŠŸæŒ‡æ¨™**: {', '.join(campaign_objectives.get('success_metrics', []))}
**äºˆç®—ç¯„å›²**: Â¥{campaign_objectives.get('budget_range', {}).get('min', 0):,} - Â¥{campaign_objectives.get('budget_range', {}).get('max', 0):,}
**æœŸé–“**: {campaign_objectives.get('timeline', 'N/A')}

{f'## ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚¿ã‚¤ãƒ—' if custom_preference else ''}
{f'**æŒ‡å®šã‚¿ã‚¤ãƒ—**: {custom_preference}' if custom_preference else ''}
{f'â€»ã“ã®å¸Œæœ›ã‚¿ã‚¤ãƒ—ã«ç‰¹ã«æ³¨ç›®ã—ã¦åˆ†æã—ã¦ãã ã•ã„' if custom_preference else ''}

## ğŸ‘¤ åˆ†æå¯¾è±¡ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
**åå‰**: {influencer.get('name', 'N/A')}
**ãƒãƒ£ãƒ³ãƒãƒ«ID**: {influencer.get('id', 'N/A')}
**ã‚«ãƒ†ã‚´ãƒª**: {influencer.get('category', 'N/A')}
**ç™»éŒ²è€…æ•°**: {influencer.get('subscriber_count', 0):,}äºº
**ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡**: {influencer.get('engagement_rate', 0):.1f}%
**èª¬æ˜**: {influencer.get('description', 'N/A')}
**å‹•ç”»æ•°**: {influencer.get('video_count', 0)}æœ¬
**ç·è¦–è´å›æ•°**: {influencer.get('view_count', 0):,}å›

## ğŸ“‹ åˆ†ææŒ‡ç¤º
ä»¥ä¸‹ã®4ã¤ã®è¦³ç‚¹ã‹ã‚‰æˆ¦ç•¥çš„åˆ†æã‚’è¡Œã„ã€JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. **ãƒ–ãƒ©ãƒ³ãƒ‰é©åˆæ€§** (0-100ç‚¹): ä¼æ¥­ã®ä¾¡å€¤è¦³ã€æ¥­ç•Œã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨ã®é©åˆåº¦
2. **ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ç›¸ä¹—åŠ¹æœ** (0-100ç‚¹): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã®é‡è¤‡ã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå“è³ªã€ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯èƒ½æ€§
3. **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é©åˆæ€§** (0-100ç‚¹): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒ†ãƒ¼ãƒã€å‰µé€ çš„æ©Ÿä¼šã®è©•ä¾¡
4. **ãƒ“ã‚¸ãƒã‚¹å®Ÿç¾æ€§** (0-100ç‚¹): ROIäºˆæ¸¬ã€ãƒªã‚¹ã‚¯è©•ä¾¡ã€é•·æœŸçš„å¯èƒ½æ€§

**å¿…é ˆå›ç­”é …ç›®**:
```json
{{
  "overall_compatibility_score": 85,
  "brand_alignment_score": 80,
  "brand_alignment_reasoning": "å…·ä½“çš„ãªç†ç”±",
  "brand_strengths": ["å¼·ã¿1", "å¼·ã¿2"],
  "brand_concerns": ["æ‡¸å¿µç‚¹1"],
  "audience_synergy_score": 90,
  "demographic_overlap": "è©³ç´°ãªé‡è¤‡åˆ†æ",
  "engagement_quality": "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå“è³ªè©•ä¾¡",
  "conversion_potential": "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯èƒ½æ€§",
  "content_fit_score": 85,
  "style_compatibility": "ã‚¹ã‚¿ã‚¤ãƒ«é©åˆæ€§",
  "content_themes": ["ãƒ†ãƒ¼ãƒ1", "ãƒ†ãƒ¼ãƒ2"],
  "creative_opportunities": ["æ©Ÿä¼š1", "æ©Ÿä¼š2"],
  "business_viability_score": 75,
  "roi_prediction": "ROIäºˆæ¸¬",
  "risk_assessment": "ãƒªã‚¹ã‚¯è©•ä¾¡",
  "long_term_potential": "é•·æœŸçš„å¯èƒ½æ€§",
  "confidence_level": "High/Medium/Low",
  "primary_reason": "ä¸»è¦æ¨è–¦ç†ç”±",
  "success_scenario": "æˆåŠŸã‚·ãƒŠãƒªã‚ª",
  "collaboration_strategy": "ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
  "expected_outcomes": ["æœŸå¾…ã•ã‚Œã‚‹æˆæœ1", "æˆæœ2"],
  "collaboration_types": ["æ¨è–¦ã‚³ãƒ©ãƒœæ‰‹æ³•1", "æ‰‹æ³•2"],
  "optimal_timing": "æœ€é©ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°",
  "content_suggestions": ["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ææ¡ˆ1", "ææ¡ˆ2"],
  "budget_min": 80000,
  "budget_max": 150000,
  "budget_reasoning": "äºˆç®—æ¨å¥¨ç†ç”±"
}}
```

**é‡è¦**: 
- å¿…ãšJSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜æ–‡ã‚„å‰å¾Œã®æ–‡ç« ã¯ä¸è¦ï¼‰
- æ—¥æœ¬èªã§å…·ä½“çš„ã‹ã¤èª¬å¾—åŠ›ã®ã‚ã‚‹åˆ†æã‚’æä¾›
- ä¼æ¥­ã®ç‰¹æ€§ã¨ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã®å®Ÿç¸¾ã‚’è©³ç´°ã«è€ƒæ…®
- æˆ¦ç•¥çš„è¦–ç‚¹ã‹ã‚‰å®Ÿç¾å¯èƒ½ã§åŠ¹æœçš„ãªææ¡ˆã‚’è¡Œã†
- æ–‡å­—åˆ—å€¤ã¯å®Œå…¨ã«é–‰ã˜ã‚‰ã‚ŒãŸçŠ¶æ…‹ã§è¨˜è¿°ã—ã€æ”¹è¡Œã¯å«ã‚ãªã„
- ã™ã¹ã¦ã®æ–‡å­—åˆ—å€¤ã‚’200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«è¨˜è¿°

å›ç­”ä¾‹: {{"overall_compatibility_score": 85, "brand_alignment_score": 80, ...}}
"""
        
        return prompt
    
    async def _call_gemini_async(self, prompt: str) -> Optional[str]:
        """Gemini APIã®éåŒæœŸå‘¼ã³å‡ºã—"""
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.model.generate_content(prompt)
            )
            return response.text
        except Exception as e:
            logger.error(f"Gemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _clean_json_response(self, response: str) -> str:
        """Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æœ‰åŠ¹ãªJSONã‚’æŠ½å‡ºãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
            if '```json' in response:
                start = response.find('```json') + 7
                end = response.find('```', start)
                if end != -1:
                    response = response[start:end].strip()
            elif '```' in response:
                start = response.find('```') + 3
                end = response.find('```', start)
                if end != -1:
                    response = response[start:end].strip()
            
            # JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®é–‹å§‹ã¨çµ‚äº†ã‚’è¦‹ã¤ã‘ã‚‹
            start_brace = response.find('{')
            end_brace = response.rfind('}') + 1
            
            if start_brace != -1 and end_brace > start_brace:
                json_text = response[start_brace:end_brace]
                
                # æ”¹è¡Œã‚„ä½™åˆ†ãªç©ºç™½ã‚’é©åˆ‡ã«å‡¦ç†
                json_text = json_text.replace('\n', ' ')
                
                # ä¸å®Œå…¨ãªæ–‡å­—åˆ—ã‚’ä¿®æ­£ï¼ˆé–‰ã˜ã‚‰ã‚Œã¦ã„ãªã„å¼•ç”¨ç¬¦ï¼‰
                # ç°¡å˜ãªä¿®æ­£ï¼šæœ€å¾Œã®å€¤ãŒä¸å®Œå…¨ãªæ–‡å­—åˆ—ã®å ´åˆã¯é™¤å»
                lines = json_text.split(',')
                clean_lines = []
                for line in lines:
                    line = line.strip()
                    if line.count('"') % 2 == 0 or line.endswith('}'):  # å¶æ•°å€‹ã®å¼•ç”¨ç¬¦ã¾ãŸã¯çµ‚äº†æ‹¬å¼§
                        clean_lines.append(line)
                    else:
                        # ä¸å®Œå…¨ãªè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã¾ãŸã¯ä¿®æ­£
                        if ':' in line and not line.endswith('"'):
                            # ä¸å®Œå…¨ãªå€¤ã‚’å‰Šé™¤
                            continue
                        clean_lines.append(line)
                
                cleaned_json = ','.join(clean_lines)
                
                # æœ€å¾Œã®ã‚«ãƒ³ãƒã‚’é©åˆ‡ã«å‡¦ç†
                cleaned_json = cleaned_json.replace(',}', '}')
                
                return cleaned_json
            
            return response
            
        except Exception as e:
            logger.warning(f"JSON ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return response
    
    def _extract_analysis_from_text(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã®å›ç­”ã‹ã‚‰åˆ†ææƒ…å ±ã‚’æŠ½å‡º"""
        # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return {
            "overall_compatibility_score": 75,
            "brand_alignment_score": 70,
            "brand_alignment_reasoning": "ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ã‚ˆã‚‹æ¨å®šå€¤",
            "brand_strengths": ["é©åˆæ€§", "ä¿¡é ¼æ€§"],
            "brand_concerns": ["è©³ç´°åˆ†æãŒå¿…è¦"],
            "audience_synergy_score": 80,
            "demographic_overlap": "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå±¤ã®é‡è¤‡ãŒè¦‹è¾¼ã¾ã‚Œã‚‹",
            "engagement_quality": "è‰¯å¥½ãªã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ",
            "conversion_potential": "ä¸­ç¨‹åº¦ã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æœŸå¾…",
            "content_fit_score": 75,
            "style_compatibility": "é©åˆã™ã‚‹ã‚¹ã‚¿ã‚¤ãƒ«",
            "content_themes": ["å•†å“ç´¹ä»‹", "ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«"],
            "creative_opportunities": ["ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "å‰µä½œæ´»å‹•"],
            "business_viability_score": 70,
            "roi_prediction": "è‰¯å¥½ãªROIãŒæœŸå¾…ã•ã‚Œã‚‹",
            "risk_assessment": "ä½ã€œä¸­ç¨‹åº¦ã®ãƒªã‚¹ã‚¯",
            "long_term_potential": "é•·æœŸçš„ãªé–¢ä¿‚æ§‹ç¯‰ã®å¯èƒ½æ€§",
            "confidence_level": "Medium",
            "primary_reason": text[:200] if text else "åˆ†æçµæœã‚’å–å¾—ä¸­",
            "success_scenario": "ãƒ–ãƒ©ãƒ³ãƒ‰èªçŸ¥åº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã‚‹",
            "collaboration_strategy": "æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
            "expected_outcomes": ["èªçŸ¥åº¦å‘ä¸Š", "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå¢—åŠ "],
            "collaboration_types": ["å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰"],
            "optimal_timing": "3-6ãƒ¶æœˆ",
            "content_suggestions": ["å•†å“ä½“é¨“", "æ—¥å¸¸çµ±åˆ"],
            "budget_min": 80000,
            "budget_max": 150000,
            "budget_reasoning": "æ¨™æº–çš„ãªä¾¡æ ¼å¸¯ã§ã®æ¨å¥¨"
        }
    
    async def _analyze_portfolio_optimization(self, analysis_results: List[Dict[str, Any]], request_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–åˆ†æ"""
        try:
            # å…¨ä½“çš„ãªæˆ¦ç•¥ã‚¹ã‚³ã‚¢è¨ˆç®—
            total_scores = [result["overall_compatibility_score"] for result in analysis_results]
            overall_strategy_score = sum(total_scores) / len(total_scores) if total_scores else 0
            
            # å¤šæ§˜æ€§åˆ†æ
            categories = [result.get("detailed_analysis", {}).get("content_fit", {}).get("content_themes_match", []) for result in analysis_results]
            unique_categories = set()
            for cat_list in categories:
                unique_categories.update(cat_list)
            
            diversity_analysis = f"é¸å‡ºã•ã‚ŒãŸã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã¯{len(unique_categories)}ã®ç•°ãªã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ†ãƒ¼ãƒã‚’ã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã§ã™ã€‚"
            
            return {
                "overall_strategy_score": int(overall_strategy_score),
                "portfolio_balance": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼æ§‹æˆ",
                "diversity_analysis": diversity_analysis,
                "optimization_suggestions": [
                    "ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã®é•·æœŸå¥‘ç´„ã‚’å„ªå…ˆ",
                    "ç•°ãªã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ã‚¿ã‚¤ãƒ«ã®çµ„ã¿åˆã‚ã›ã§ãƒªãƒ¼ãƒã‚’æœ€å¤§åŒ–",
                    "æ®µéšçš„ãªäºˆç®—é…åˆ†ã§åŠ¹æœã‚’æ¸¬å®š"
                ]
            }
        except Exception as e:
            logger.error(f"ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "overall_strategy_score": 75,
                "portfolio_balance": "ãƒãƒ©ãƒ³ã‚¹åˆ†æä¸­",
                "diversity_analysis": "å¤šæ§˜æ€§ã‚’è©•ä¾¡ä¸­",
                "optimization_suggestions": ["åˆ†æã‚’ç¶™ç¶šä¸­"]
            }
    
    async def _analyze_market_context(self, request_data: Dict[str, Any], analysis_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        try:
            company_industry = request_data.get('company_profile', {}).get('industry', '')
            
            # æ¥­ç•Œåˆ¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            industry_trends = {
                'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼': ['AIæ´»ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'ãƒ©ã‚¤ãƒ–ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼è§£èª¬'],
                'ç¾å®¹ãƒ»åŒ–ç²§å“': ['ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼', 'ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ç´¹ä»‹', 'ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼'],
                'é£Ÿå“ãƒ»é£²æ–™': ['ãƒ¬ã‚·ãƒ”å‹•ç”»', 'é£Ÿä½“é¨“ãƒ¬ãƒãƒ¼ãƒˆ', 'ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ææ¡ˆ'],
                'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³': ['ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆææ¡ˆ', 'ãƒˆãƒ¬ãƒ³ãƒ‰è§£èª¬', 'ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°å‹•ç”»']
            }
            
            trends = industry_trends.get(company_industry, ['ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã‚³ãƒ©ãƒœ', 'ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¹ãƒˆãƒ¼ãƒªãƒ¼'])
            
            return {
                "industry_trends": trends,
                "competitive_landscape": f"{company_industry}æ¥­ç•Œã«ãŠã‘ã‚‹ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã¯æ´»ç™ºã§ã€å·®åˆ¥åŒ–ãŒé‡è¦ã§ã™ã€‚",
                "timing_considerations": "ç¾åœ¨ã¯æ–°ã—ã„ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã™ã‚‹ã®ã«é©ã—ãŸæ™‚æœŸã§ã™ã€‚"
            }
        except Exception as e:
            logger.error(f"å¸‚å ´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "industry_trends": ["ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æä¸­"],
                "competitive_landscape": "ç«¶åˆçŠ¶æ³ã‚’åˆ†æä¸­",
                "timing_considerations": "ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’è©•ä¾¡ä¸­"
            }
    
    def _calculate_overall_confidence(self, analysis_results: List[Dict[str, Any]]) -> float:
        """å…¨ä½“çš„ãªä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not analysis_results:
            return 0.0
        
        confidence_map = {"High": 0.9, "Medium": 0.7, "Low": 0.4}
        confidences = [
            confidence_map.get(
                result.get("recommendation_summary", {}).get("confidence_level", "Medium"), 
                0.7
            ) for result in analysis_results
        ]
        
        return sum(confidences) / len(confidences)
    
    def _set_mock_metadata(self, reason: str, description: str):
        """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨æ™‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š"""
        self.mock_metadata = {
            "mock_reason": reason,
            "mock_description": description,
            "mock_dataset_name": "å®Ÿåœ¨YouTuberãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
        }
    
    def _build_pickup_logic_summary(self, request_data: Dict[str, Any], candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚¸ãƒƒã‚¯ã®è©³ç´°ã‚µãƒãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        preferences = request_data.get('influencer_preferences', {})
        company_profile = request_data.get('company_profile', {})
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã®è©³ç´°
        filtering_steps = []
        
        # Step 1: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        data_source = "Firestore" if self.db else "ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿"
        filtering_steps.append({
            "step": 1,
            "action": "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ",
            "details": f"{data_source}ã‹ã‚‰å€™è£œã‚’å–å¾—",
            "result": f"å–å¾—å¯èƒ½ãªå€™è£œæ•°: {len(candidates)}ä»¶"
        })
        
        # Step 2: ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›
        custom_preference = preferences.get('custom_preference', '')
        if custom_preference:
            filtering_steps.append({
                "step": 2,
                "action": "ã‚«ã‚¹ã‚¿ãƒ å¸Œæœ›ãƒãƒƒãƒ”ãƒ³ã‚°",
                "details": f"'{custom_preference}' -> ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ",
                "result": "é–¢é€£ã‚«ãƒ†ã‚´ãƒªã‚’è‡ªå‹•é¸æŠ"
            })
        
        # Step 3: ç™»éŒ²è€…æ•°ãƒ•ã‚£ãƒ«ã‚¿
        subscriber_range = preferences.get('subscriber_range', {})
        if subscriber_range:
            min_sub = subscriber_range.get('min', 0)
            max_sub = subscriber_range.get('max', 999999999)
            filtering_steps.append({
                "step": 3,
                "action": "ç™»éŒ²è€…æ•°ãƒ•ã‚£ãƒ«ã‚¿",
                "details": f"{min_sub:,} - {max_sub:,} äºº",
                "result": "ç¯„å›²å¤–ã®å€™è£œã‚’é™¤å¤–"
            })
        
        # Step 4: ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿
        preferred_categories = preferences.get('preferred_categories', [])
        if preferred_categories:
            filtering_steps.append({
                "step": 4,
                "action": "ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿",
                "details": f"å„ªå…ˆã‚«ãƒ†ã‚´ãƒª: {', '.join(preferred_categories)}",
                "result": "ã‚«ãƒ†ã‚´ãƒªä¸ä¸€è‡´ã®å€™è£œã‚’é™¤å¤–"
            })
        
        # Step 5: ä¼æ¥­é©åˆæ€§
        company_industry = company_profile.get('industry', '')
        if company_industry:
            filtering_steps.append({
                "step": 5,
                "action": "ä¼æ¥­é©åˆæ€§è©•ä¾¡",
                "details": f"æ¥­ç•Œ: {company_industry}ã¨ã®è¦ªå’Œæ€§ã‚’è©•ä¾¡",
                "result": "æ¥­ç•Œé©åˆåº¦ã®é«˜ã„å€™è£œã‚’å„ªå…ˆ"
            })
        
        # æœ€çµ‚çµæœ
        final_candidates = len(candidates)
        limit = 30 if custom_preference else 20
        analyzed_count = min(final_candidates, 10 if custom_preference else 5)
        
        return {
            "total_filtering_steps": len(filtering_steps),
            "filtering_pipeline": filtering_steps,
            "final_statistics": {
                "candidates_after_filtering": final_candidates,
                "limit_applied": limit,
                "selected_for_ai_analysis": analyzed_count,
                "data_source": data_source,
                "mock_metadata": self.mock_metadata if hasattr(self, 'mock_metadata') and self.mock_metadata else None
            },
            "algorithm_details": {
                "filtering_method": "ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°",
                "matching_algorithm": "å¤šæ®µéšé©åˆåº¦è©•ä¾¡",
                "ai_analysis_model": "Gemini 1.5 Flash",
                "scoring_criteria": ["ãƒ–ãƒ©ãƒ³ãƒ‰é©åˆæ€§", "ã‚ªãƒ¼ãƒ‡ã‚£ã‚¨ãƒ³ã‚¹ç›¸ä¹—åŠ¹æœ", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é©åˆæ€§", "ãƒ“ã‚¸ãƒã‚¹å®Ÿç¾æ€§"]
            }
        }
    
    def _get_mock_influencers(self) -> List[Dict[str, Any]]:
        """å®Ÿéš›ã®YouTuberãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆFirestoreåˆ©ç”¨ä¸å¯æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        logger.info("ğŸ“Œ å®Ÿéš›ã®YouTuberãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ï¼ˆFirestoreåˆ©ç”¨ä¸å¯ï¼‰")
        return [
            {
                "id": "UC-K_2-NjlV5SdUcG-zZJqbA",
                "channel_id": "UC-K_2-NjlV5SdUcG-zZJqbA",
                "channel_name": "ã‚¬ãƒƒãƒãƒãƒ³",
                "channel_title": "ã‚¬ãƒƒãƒãƒãƒ³",
                "description": "ãƒ›ãƒ©ãƒ¼ã‚²ãƒ¼ãƒ å®Ÿæ³ã‚’ä¸­å¿ƒã«æ´»å‹•ã™ã‚‹äººæ°—ã‚²ãƒ¼ãƒ å®Ÿæ³è€…ã€‚ç‹¬ç‰¹ãªå®Ÿæ³ã‚¹ã‚¿ã‚¤ãƒ«ã¨é¢ç™½ã„ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§å¤šãã®ãƒ•ã‚¡ãƒ³ã‚’ç²å¾—ã€‚åˆè¦‹ãƒ—ãƒ¬ã‚¤ã‚’é‡è¦–ã—ã€è¦–è´è€…ã¨ä¸€ç·’ã«ã‚²ãƒ¼ãƒ ã‚’æ¥½ã—ã‚€ã‚¹ã‚¿ã‚¤ãƒ«ãŒç‰¹å¾´ã€‚",
                "subscriber_count": 1850000,
                "video_count": 3000,
                "view_count": 900000000,
                "category": "ã‚²ãƒ¼ãƒ ",
                "engagement_rate": 8.5,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLRLp0P1tL2cEJwgEjZq1nWk6iHO3UKh6v7qI5AZJA=s240-c-k-c0x00ffffff-no-rj",
                "email": "contact@gatchman.com",
                "country": "JP"
            },
            {
                "id": "UCBYQvzhX5-yTmqc6PoVa_3w",
                "channel_id": "UCBYQvzhX5-yTmqc6PoVa_3w",
                "channel_name": "ã‚‰ã‚‰ã‚“ã‚²ãƒ¼ãƒ å®Ÿæ³",
                "channel_title": "ã‚‰ã‚‰ã‚“ã‚²ãƒ¼ãƒ å®Ÿæ³",
                "description": "ãƒã‚¤ãƒ³ã‚¯ãƒ©ãƒ•ãƒˆã‚’ä¸­å¿ƒã¨ã—ãŸã‚²ãƒ¼ãƒ å®Ÿæ³ãƒãƒ£ãƒ³ãƒãƒ«ã€‚å»ºç¯‰ã‚„å†’é™ºã‚’é€šã˜ã¦ã€è¦–è´è€…ã«æ¥½ã—ã„æ™‚é–“ã‚’æä¾›ã€‚è¦ªã—ã¿ã‚„ã™ã„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§å®¶æ—å±¤ã«ã‚‚äººæ°—ã€‚",
                "subscriber_count": 15800,
                "video_count": 324,
                "view_count": 5100000,
                "category": "ã‚²ãƒ¼ãƒ ",
                "engagement_rate": 12.3,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "info@lalan-gaming.com",
                "country": "JP"
            },
            {
                "id": "UC-b3JIZhC0xATKwBK4cmnqg",
                "channel_id": "UC-b3JIZhC0xATKwBK4cmnqg",
                "channel_name": "ã€å…ƒã‚µãƒƒã‚«ãƒ¼æ—¥æœ¬ä»£è¡¨ åŸå½°äºŒã€‘JOãƒãƒ£ãƒ³ãƒãƒ«",
                "channel_title": "ã€å…ƒã‚µãƒƒã‚«ãƒ¼æ—¥æœ¬ä»£è¡¨ åŸå½°äºŒã€‘JOãƒãƒ£ãƒ³ãƒãƒ«",
                "description": "å…ƒã‚µãƒƒã‚«ãƒ¼æ—¥æœ¬ä»£è¡¨ã®åŸå½°äºŒã«ã‚ˆã‚‹ã‚¹ãƒãƒ¼ãƒ„ãƒ»ãƒ“ã‚¸ãƒã‚¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«ã€‚ã‚µãƒƒã‚«ãƒ¼æŒ‡å°ã€ãƒ“ã‚¸ãƒã‚¹è«–ã€äººç”Ÿå“²å­¦ãªã©å¹…åºƒã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç™ºä¿¡ã€‚ã‚¹ãƒãƒ¼ãƒ„ãƒãƒ³ã‚·ãƒƒãƒ—ã¨ãƒ“ã‚¸ãƒã‚¹ãƒã‚¤ãƒ³ãƒ‰ã‚’èåˆã—ãŸç‹¬è‡ªã®è¦–ç‚¹ãŒé­…åŠ›ã€‚",
                "subscriber_count": 101000,
                "video_count": 531,
                "view_count": 32800000,
                "category": "ã‚¹ãƒãƒ¼ãƒ„",
                "engagement_rate": 6.8,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "jo@soccerbusiness.jp",
                "country": "JP"
            },
            {
                "id": "UCjwmcmT8yfnIkIfb63vprHg",
                "channel_id": "UCjwmcmT8yfnIkIfb63vprHg",
                "channel_name": "ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ£ãƒ³ãƒãƒ«",
                "channel_title": "ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ£ãƒ³ãƒãƒ«",
                "description": "ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã®å®Ÿè·µçš„ãªãƒã‚¦ãƒã‚¦ã‚’ç™ºä¿¡ã€‚çµŒå–¶æˆ¦ç•¥ã€ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã€çµ„ç¹”é‹å–¶ãªã©ã€å®Ÿéš›ã®ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ç¾å ´ã§ã®çµŒé¨“ã‚’åŸºã«ã—ãŸå…·ä½“çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã€‚",
                "subscriber_count": 12100,
                "video_count": 190,
                "view_count": 991000,
                "category": "ãƒ“ã‚¸ãƒã‚¹",
                "engagement_rate": 9.2,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "consulting@business-channel.jp",
                "country": "JP"
            },
            {
                "id": "UC0QMnnz3E-B02xtQhjktiXA",
                "channel_id": "UC0QMnnz3E-B02xtQhjktiXA",
                "channel_name": "ä¸‰æµ¦å¤§çŸ¥ã®ã‚²ãƒ¼ãƒ å®Ÿæ³",
                "channel_title": "ä¸‰æµ¦å¤§çŸ¥ã®ã‚²ãƒ¼ãƒ å®Ÿæ³",
                "description": "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆä¸‰æµ¦å¤§çŸ¥ã«ã‚ˆã‚‹ã‚²ãƒ¼ãƒ å®Ÿæ³ãƒãƒ£ãƒ³ãƒãƒ«ã€‚éŸ³æ¥½æ´»å‹•ã¨ã¯ç•°ãªã‚‹ä¸€é¢ã‚’è¦‹ã›ã€æ§˜ã€…ãªã‚²ãƒ¼ãƒ ã‚’æ¥½ã—ããƒ—ãƒ¬ã‚¤ã€‚éŸ³æ¥½æ€§ã‚’æ´»ã‹ã—ãŸç‹¬ç‰¹ãªå®Ÿæ³ã‚¹ã‚¿ã‚¤ãƒ«ãŒç‰¹å¾´ã§ã€ãƒ•ã‚¡ãƒ³ã¨ã®æ–°ãŸãªäº¤æµã®å ´ã¨ãªã£ã¦ã„ã‚‹ã€‚",
                "subscriber_count": 106000,
                "video_count": 595,
                "view_count": 25100000,
                "category": "ã‚¨ãƒ³ã‚¿ãƒ¡",
                "engagement_rate": 7.4,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "contact@daichi-gaming.com",
                "country": "JP"
            },
            {
                "id": "UC_sample_beauty_1",
                "channel_id": "UC_sample_beauty_1",
                "channel_name": "ç¾å®¹ç³»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼A",
                "channel_title": "ç¾å®¹ç³»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼A",
                "description": "æœ€æ–°ã‚³ã‚¹ãƒ¡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ãƒ¡ã‚¤ã‚¯ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ç´¹ä»‹ã™ã‚‹ç¾å®¹ãƒãƒ£ãƒ³ãƒãƒ«ã€‚ãƒ—ãƒãƒ—ãƒ©ã‹ã‚‰ãƒ‡ãƒ‘ã‚³ã‚¹ã¾ã§å¹…åºƒãæ‰±ã„ã€å®Ÿç”¨çš„ãªãƒ¡ã‚¤ã‚¯ãƒã‚¦ãƒ„ãƒ¼ã‚’ç™ºä¿¡ã€‚20-30ä»£å¥³æ€§ã«äººæ°—ã€‚",
                "subscriber_count": 234000,
                "video_count": 456,
                "view_count": 67800000,
                "category": "ç¾å®¹",
                "engagement_rate": 11.2,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "beauty@makeup-tips.jp",
                "country": "JP"
            },
            {
                "id": "UC_sample_cooking_1",
                "channel_id": "UC_sample_cooking_1",
                "channel_name": "ç°¡å˜ãƒ¬ã‚·ãƒ”ãƒãƒ£ãƒ³ãƒãƒ«",
                "channel_title": "ç°¡å˜ãƒ¬ã‚·ãƒ”ãƒãƒ£ãƒ³ãƒãƒ«",
                "description": "å¿™ã—ã„ç¾ä»£äººå‘ã‘ã®æ™‚çŸ­ãƒ¬ã‚·ãƒ”ã¨ç¯€ç´„æ–™ç†ã‚’ç´¹ä»‹ã€‚ä¸€äººæš®ã‚‰ã—ã‚„åˆå¿ƒè€…ã§ã‚‚ä½œã‚Œã‚‹ç°¡å˜ã§ç¾å‘³ã—ã„æ–™ç†ã‚’ä¸­å¿ƒã«ã€é£Ÿæã®æ´»ç”¨æ³•ã‚„ä¿å­˜ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚‚ç™ºä¿¡ã€‚",
                "subscriber_count": 189000,
                "video_count": 378,
                "view_count": 43200000,
                "category": "æ–™ç†",
                "engagement_rate": 9.8,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "recipe@easycooking.jp",
                "country": "JP"
            },
            {
                "id": "UC_sample_tech_1",
                "channel_id": "UC_sample_tech_1",
                "channel_name": "ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼JP",
                "channel_title": "ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼JP",
                "description": "æœ€æ–°ã‚¬ã‚¸ã‚§ãƒƒãƒˆã¨ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è©³ã—ããƒ¬ãƒ“ãƒ¥ãƒ¼ã€‚ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã€PCã€å®¶é›»ãªã©ã®å®Ÿæ©Ÿãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨æ¯”è¼ƒæ¤œè¨¼ã‚’è¡Œã„ã€è³¼å…¥å‰ã®å‚è€ƒæƒ…å ±ã‚’æä¾›ã€‚æŠ€è¡“çš„ãªè§£èª¬ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã€‚",
                "subscriber_count": 156000,
                "video_count": 289,
                "view_count": 38900000,
                "category": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
                "engagement_rate": 8.1,
                "thumbnail_url": "https://yt3.ggpht.com/ytc/AIdvJLQLcNRK5h9KEZ3YfVl4NQWFd8lOhyqFJkV7gXOQAg=s240-c-k-c0x00ffffff-no-rj",
                "email": "review@techreview-jp.com",
                "country": "JP"
            }
        ]
    
    async def _get_available_categories(self) -> List[str]:
        """Firestoreã‹ã‚‰å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’å–å¾—"""
        try:
            if not self.db:
                # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
                return ["ã‚²ãƒ¼ãƒ ", "æ–™ç†", "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹", "ãƒ“ã‚¸ãƒã‚¹", "ç¾å®¹", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ã‚¨ãƒ³ã‚¿ãƒ¡", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³"]
            
            # Firestoreã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’å–å¾—
            categories = set()
            docs = self.db.collection('influencers').limit(100).stream()
            
            for doc in docs:
                data = doc.to_dict()
                category = data.get('category')
                if category:
                    categories.add(category)
            
            return list(categories)
            
        except Exception as e:
            logger.error(f"ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return ["ã‚²ãƒ¼ãƒ ", "æ–™ç†", "ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹", "ãƒ“ã‚¸ãƒã‚¹", "ç¾å®¹", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "ã‚¨ãƒ³ã‚¿ãƒ¡", "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³"]
    
    async def _map_categories_with_gemini(self, user_preference: str, available_categories: List[str]) -> List[str]:
        """Gemini APIã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼å¸Œæœ›ã«æœ€ã‚‚è¿‘ã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ"""
        try:
            # ã‚ˆã‚Šè©³ç´°ãªæ—¥æœ¬èªç‰¹åŒ–ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""
ã‚ãªãŸã¯ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¸Œæœ›ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ã€åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¸Œæœ›ã€‘
{user_preference}

ã€åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã€‘
{', '.join(available_categories)}

ã€ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã€‘
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¸Œæœ›ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
2. é–¢é€£æ€§ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚‚å«ã‚ã¦ã€æœ€å¤§3ã¤ã¾ã§é¸æŠå¯èƒ½
3. å®Œå…¨ä¸€è‡´ãŒãªãã¦ã‚‚ã€æ„å‘³çš„ã«è¿‘ã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
4. åºƒç¾©ã®è§£é‡ˆã‚‚å«ã‚ã¦æŸ”è»Ÿã«ãƒãƒƒãƒ”ãƒ³ã‚°

ã€ç‰¹åˆ¥ãªãƒãƒƒãƒ”ãƒ³ã‚°ä¾‹ã€‘
å¸Œæœ›: "ç¾å®¹ç³»" â†’ Howto & Style, People & Blogs (ç¾å®¹é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã¯é€šå¸¸ã“ã®åˆ†é¡)
å¸Œæœ›: "ã‚²ãƒ¼ãƒ å®Ÿæ³" â†’ ã‚²ãƒ¼ãƒ 
å¸Œæœ›: "ã‚°ãƒ«ãƒ¡" â†’ æ–™ç†, Howto & Style
å¸Œæœ›: "ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³" â†’ Howto & Style, People & Blogs
å¸Œæœ›: "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼" â†’ ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼
å¸Œæœ›: "ã‚¨ãƒ³ã‚¿ãƒ¡" â†’ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ, éŸ³æ¥½ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ

ã€æ³¨æ„äº‹é …ã€‘
- æ—¥æœ¬ã®YouTubeã‚«ãƒ†ã‚´ãƒªã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€ç¾å®¹ç³»ãƒãƒ£ãƒ³ãƒãƒ«ã¯ã€ŒHowto & Styleã€ã«åˆ†é¡ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„
- ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ç³»ã¯ã€ŒPeople & Blogsã€ã«å«ã¾ã‚Œã‚‹
- ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆç³»ã¯è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªã«åˆ†æ•£

çµæœã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ä¸è¦ï¼‰ï¼š
"""
            
            response = self.model.generate_content(prompt)
            response_text = response.text.strip()
            logger.info(f"ğŸ¤– Geminiå¿œç­”: '{response_text}'")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒªã‚¹ãƒˆã«å¤‰æ›
            selected_categories = []
            for category in response_text.split(','):
                category = category.strip()
                if category in available_categories:
                    selected_categories.append(category)
                    logger.info(f"âœ… ãƒãƒƒãƒ: '{category}'")
                else:
                    logger.warning(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªä¸ä¸€è‡´: '{category}' (åˆ©ç”¨å¯èƒ½: {available_categories})")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã®å¼·åŒ–
            if not selected_categories:
                logger.warning(f"âš ï¸ Geminiãƒãƒƒãƒ”ãƒ³ã‚°å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–‹å§‹")
                
                # 1. ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ã‚ˆã‚‹æ‰‹å‹•ãƒãƒƒãƒ”ãƒ³ã‚°
                user_lower = user_preference.lower()
                manual_mappings = {
                    'ç¾å®¹': ['Howto & Style', 'People & Blogs'],
                    'ã‚³ã‚¹ãƒ¡': ['Howto & Style', 'People & Blogs'],
                    'ãƒ¡ã‚¤ã‚¯': ['Howto & Style'],
                    'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³': ['Howto & Style', 'People & Blogs'],
                    'ã‚¹ã‚­ãƒ³ã‚±ã‚¢': ['Howto & Style'],
                    'ã‚°ãƒ«ãƒ¡': ['æ–™ç†', 'Howto & Style'],
                    'æ–™ç†': ['æ–™ç†'],
                    'ã‚²ãƒ¼ãƒ ': ['ã‚²ãƒ¼ãƒ '],
                    'ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹': ['People & Blogs', 'ã‚¹ãƒãƒ¼ãƒ„ãƒ»ã‚¢ã‚¦ãƒˆãƒ‰ã‚¢'],
                    'ãƒ“ã‚¸ãƒã‚¹': ['People & Blogs'],
                    'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼': ['People & Blogs'],
                    'ã‚¨ãƒ³ã‚¿ãƒ¡': ['ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ', 'éŸ³æ¥½ãƒ»ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ']
                }
                
                for keyword, mapped_cats in manual_mappings.items():
                    if keyword in user_lower:
                        for mapped_cat in mapped_cats:
                            if mapped_cat in available_categories:
                                selected_categories.append(mapped_cat)
                        break
                
                # 2. éƒ¨åˆ†ãƒãƒƒãƒã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not selected_categories:
                    for cat in available_categories:
                        if any(keyword in cat.lower() for keyword in user_lower.split()):
                            selected_categories.append(cat)
                            break
                
                # 3. æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - é–¢é€£æ€§ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚’è¿”ã™
                if not selected_categories:
                    # ç¾å®¹ç³»ã®å ´åˆã¯ä»£æ›¿ã‚«ãƒ†ã‚´ãƒªã‚’ææ¡ˆ
                    if 'ç¾å®¹' in user_lower or 'ã‚³ã‚¹ãƒ¡' in user_lower or 'ãƒ¡ã‚¤ã‚¯' in user_lower:
                        fallback_cats = ['Howto & Style', 'People & Blogs']
                        for cat in fallback_cats:
                            if cat in available_categories:
                                selected_categories.append(cat)
                    
                    # ã¾ã ä½•ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ã‚«ãƒ†ã‚´ãƒªå¯¾è±¡ã«ã™ã‚‹
                    if not selected_categories:
                        logger.warning(f"âš ï¸ å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: '{user_preference}' -> å…¨ã‚«ãƒ†ã‚´ãƒªå¯¾è±¡")
                        return []
            
            final_categories = selected_categories[:3]  # æœ€å¤§3ã¤ã¾ã§
            logger.info(f"ğŸ¯ æœ€çµ‚ãƒãƒƒãƒ”ãƒ³ã‚°çµæœ: {final_categories}")
            return final_categories
            
        except Exception as e:
            logger.error(f"Geminiã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            # ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            user_lower = user_preference.lower()
            fallback_categories = []
            
            # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
            if 'ç¾å®¹' in user_lower or 'ã‚³ã‚¹ãƒ¡' in user_lower:
                fallback_categories = ['Howto & Style', 'People & Blogs']
            elif 'ã‚²ãƒ¼ãƒ ' in user_lower:
                fallback_categories = ['ã‚²ãƒ¼ãƒ ']
            elif 'æ–™ç†' in user_lower or 'ã‚°ãƒ«ãƒ¡' in user_lower:
                fallback_categories = ['æ–™ç†']
            
            # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            final_fallback = [cat for cat in fallback_categories if cat in available_categories]
            logger.info(f"ğŸ”„ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {final_fallback}")
            return final_fallback[:3]